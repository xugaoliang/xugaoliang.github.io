---
typora-root-url: ../../../
---

# 【译】PVEs:用于结构化状态表征的无监督学习的位置速度编码器

**摘要**:我们提出了一种无监督学习的位置速度编码器(PVEs)，将图像编码成与任务相关的物体的位置和速度。PVEs将单个图像编码为低维位置状态，并根据位置的有限差异计算速度状态。与自编码器相比，位置速度编码器不是通过图像重建来训练的，而是通过使位置速度表征与物理世界交互的先验一致来训练的。我们将PVEs应用于多个像素模拟控制任务中，取得了良好的初步效果。

## I. 简介
虽然位置和速度是机器人状态表征的基本组成部分，但机器人不能直接感知这些属性。相反，他们需要从感官输入中提取与任务相关的位置和速度信息。为了使机器人成为万能机器人，它们必须能够从经验中学习这些表征形式。原则上深度学习允许学习位置-速度表征，但大多数现有的方法主要依赖于标记数据。

在这篇论文中，我们研究了机器人如何在没有监督的情况下学习位置速度表征。我们使用机器人特有的与物理世界交互的先验知识(也称为机器人先验)来解决这个问题，以学习从高维感官观察到低维状态表征的编码。我们的贡献是将状态表征分为速度状态和位置状态，并将机器人关于位置和速度的先验知识以模型约束和学习目标的形式结合起来。

我们的方法，位置速度编码器(PVE)，实现了一个硬模型约束，从有限的位置状态的差异估计速度状态。此约束确定了状态表示的这两部分之间的关系。此外，PVEs 还包括测量与机器人先验一致性的软目标。这些目标在学习过程中得到优化，确定编码的信息以及多个状态样本之间的关系。这两种成分一起工作，以学习编码成结构化的状态表征，其中包括位置状态(描述来自单个观察的信息)和速度状态(描述信息如何随时间变化)。


![图1](/assets/images/计算机视觉/PVE/fig1.png)

图1：PVEs 将观测编码为低维位置状态。从一系列这样的位置状态，用他们估计速度。PVEs通过优化位置和速度与机器人先验的一致性来学习编码。

图1显示了将观察值(蓝色矩形)映射到位置状态(蓝色圆点)的位置编码器。速度状态——位置状态的时间导数由位置的有限差分近似得到。这种结构化的状态空间使我们能够以学习目标的形式形成新的机器人先验，特别是关于位置和速度的先验。

PVEs 学习将观察结果编码成没有状态标签的状态，并且不学习解码器。相反，他们通过使位置状态及其导数与不同的机器人先验一致来学习编码。每个先验的不一致性用损失函数来度量。PVEs 通过使用梯度下降法最小化这些损失的加权和来学习。梯度可以被想象成状态空间中的力，它将状态样本拉在一起(当它们应该相似时)或将它们分开(当它们应该不同时)。反向传播将这些力转换为编码器中的参数变化(参见图1中的粉红色和紫色箭头)。

在我们的初步实验中，我们将位置速度编码器应用于像素模拟控制任务。我们表明,PVEs 能够发现任务的拓扑结构和维数,他们可以从不同的相机角度,学习相同的表征形式,他们捕获真实场景中物体的位置和速度信息,并基于学习position-velocity状态在强化学习中产生精确的控制。

## II. 相关工作

本文扩展了 Jonschkowski 和 Brock 对机器人先验学习状态表征的研究，介绍了机器人先验的思想及其在表征学习目标中的实现。我们对位置-速度状态的扩展是受 Scholz 等人基于模型的强化学习中基于物理先验的启发，他们提出了在给定位置-速度表征的情况下学习物理上合理的动力学模型。在这里，我们把他们的方法反过来问:我们如何从感觉输入中学习位置-速度的表征，而不指定哪些位置和速度与任务相关?

我们提出的答案是位置速度编码器，它通过两种不同的方式整合先验知识，这符合 Mitchell 的归纳偏差的分类到限制偏差和偏好偏差[13，第64页]。限制偏差限制了在学习过程中所考虑的假设空间，比如我们在PVE模型中通过位置的有限差异来估计速度的约束(而不是学习从一系列的观察中提取速度信息)。偏好偏差表达了对某些假设的偏好，比如我们训练 PVEs 的损失函数，它衡量了与机器人先验的不一致性。

在视觉表征学习文献中，限制偏差的其他例子包括卷积网络[10]、空间 transformer 网络[5],和空间 softmax[11],都包含了把视觉输入作为神经网络的体系结构约束的先验,以及 backprop 卡尔曼滤波器[3]和端到端可学习的直方图过滤器[8],都包含了贝叶斯过滤算法的结构作为递归状态估计。SE3-nets[2]实现了刚体变换的假设。虽然这些方法可以规范学习，但非监督学习(也)需要适当的偏好偏差。

内部表征的偏好偏差通常通过基于表征的其他可学习函数间接表达。例如，其他人通过学习图像重建[21]、预测未来状态[19,21]或其他辅助任务[4,12]来训练表征。一个强大但未被充分研究的替代方法是直接表达对学习表征的偏好偏差，这种方法可以用来学习符号 grounding（基础、底色） [6]或执行无标签监督学习[18]。直接偏好偏差也是度量学习的重点，例如，使用相当普遍的三重损失[17]来学习面部表征，但是以直接偏好偏差的形式来构建更有信息的机器人先验有很大的潜力，正如我们在这项工作中所做的。

## III. 位置速度编码器

位置速度编码器(PVEs)学习将原始观测数据映射到由位置部分和速度部分组成的结构化状态空间。PVEs 是在没有状态标签的情况下训练的，他们不需要学习辅助功能，如重建观察或预测状态转换。PVEs 通过结合两个关键思想来实现这一目标:

* 1) PVEs将当前观测结果编码为位置状态，并根据位置的有限差异(更多细节见第III-A节)来估计速度状态。
* 2) 通过优化与机器人先验的位置、速度和加速度的一致性来训练 PVEs(更多细节见 第 III-B & III-C 节)。

### A. 模型

PVE 模型由卷积网络和数值速度估计两部分组成。卷积网络$\phi$编码视觉观察$o_t$到一个低维位置状态$s_t^{(p)}$,上标$(p)$代表位置。

$$
s_t^{(p)} = \phi(o_t)
$$

模型从最后两个位置状态$s_t^{(p)}$和$s_{t-1}^{(p)}$的差异来估计速度状态$s_t^{(v)}$:

$$
s_t^{(v)} = \alpha(s_{t}^{(p)}-s_{t-1}^{(p)})
$$

其中$\alpha$是包含$\frac{1}{\mathrm{timestep}}$的超参数，并缩放速度状态。重要的是，速度状态要有相对于位置状态的适当比例，以便在组合状态$s_t$中创建一个合理的度量，我们通过叠加位置状态和速度状态来构造这个度量。

$$
s_t = \begin{bmatrix}
    s_t^{(p)}\\
    s_t^{(v)}
\end{bmatrix}
$$

我们还可以使用有限的差异来估计加速度(或急拉、颠簸等)。我们没有在状态中包含这些导数，因为我们假设机器人通过它的动作来控制加速度。但是我们确实在一些损失函数中使用了加速度状态。我们计算加速度状态$s_t^{(a)}$的方式与计算速度状态相同，但我们忽略了缩放，因为我们不会在合并状态空间中使用加速度:

$$
s_t^{(a)}=s_t^{(v)}-s_{t-1}^{(v)}
$$

### B. 机器人先验和学习目标

编码器$\phi$被训练,使合并后的状态空间符合一组机器人先验,我们将在本节描述。这些先验使用结构化的状态空间，并且特定于位置、速度和加速度。与这些先验以损失函数的形式定义，学习过程中最小化损失函数。下面列出的机器人先验应该被理解为对这个问题的探索，而不是最终的答案。

  1. **变化**:相关事物的位置变化。当机器人探索它的任务并操纵它的环境时，与任务相关的物体(包括它自己)的位置将会改变——否则机器人就没有多少东西可以学习。如果我们假设在机器人的经验中，相关物体的位置是变化的，那么这些位置的内部表征也必须是变化的;位置状态的随机对不应该是相似的。因此，我们通过最小化随机位置状态对之间的期望相似性来优化与变化先验的一致性，

$$
L_{\mathrm{variation}} = E[e^{-{||s_a^{(p)}-s_b^{(p)}||}}]
$$

我们用$e^{-\mathrm{distance}}$作为相似度的度量如果距离是0，那值就是1，随着位置状态之间距离的增加，那就是0，这正是我们想要的。

2. **缓慢度**:位置变化缓慢。物理对象不会瞬移;它们不会在每一秒之间任意改变位置。为了使内部位置状态与缓慢度先验一致，我们最小化连续位置状态之间的期望平方距离,

$$
L_{\mathrm{slowness}} = E[||s_t^{(p)}-s_{t-1}^{(p)}||^2]
$$

由于位置的变化与位置的变化率(或速度)直接相关，我们也可以用速度状态写出相同的损失

$$
L_{\mathrm{slowness}} = E\left[||\frac{s_t^{(v)}}{\alpha}||^2\right]
$$

其中$\alpha$是之前定义的缩放超参数。这种重新表述暗示了对缓慢度的另一种解释，即速度较低。

3. **惯性**:速度变化缓慢。由于物理对象具有惯性，它们会抵抗速度的变化(无论是方向还是大小)。如果我们假设克服这个阻力的力是有限的，那么速度的变化应该很小。注意惯性先验是如何与应用于速度的缓慢度先验相对应的。

$$
L_{\mathrm{inertia}} = E\left[||s_t^{(v)}-s_{t-1}^{(v)}||^2\right] =  E\left[||s_t^{(a)}||^2\right]
$$

惯性先验的这个公式着重于由于损失函数中的平方而引起的大的速度变化。或者，我们可以定义基于绝对变化的损失函数。

$$
L_{\mathrm{inertia(abs)}} =  E\left[||s_t^{(a)}||\right]
$$

速度的微小变化对第二个损失的影响比第一个损失大。我们发现，将这两种损失合并起来会比使用其中任何一种得到更好的结果。

4. **守恒**:速度量级变化缓慢。这个先验来自能量守恒定律，该定律指出，一个封闭系统的总能量保持不变。由于机器人对环境施加力，我们没有一个封闭的系统。此外，我们不能估计，如动能不知道物体的质量，更不用说势能储存在弹簧等。但是，我们仍然想要执行相同的思想，即保持绝对的能量量，或者在我们的例子中，在连续的时间步中保持类似的“移动”。

$$
L_{\mathrm{conservation}} = E\left[(||s_t^{(v)}||-||s_{t-1}^{(v)}||)^2\right]
$$

5. **可控性**:可控的事情是相关的。机器人能控制的物体可能与它的任务有关。如果机器人通过施力来行动，那么可控的事物可能是那些加速度与机器人动作相关的事物。因此，我们可以为每个动作维$i$定义一个损失函数来优化动作维i和状态维$i$中的加速度之间的协方差

$$
L_{\mathrm{controlability(i)}} = e^{-Cov(a_t,i,s_{t+1}^{(a)},i)} = e^{-E[(a_{t,i}-E[a_{t,i}])(s_{t+1,i}^{(a)}-E[s_{t+1,i}^{(a)}])]}
$$

请注意，我们只在一个任务中使用了这个损失——杯中球——因为上面的先验是不够的。这项工作的结果还只是初步的。这项任务的完整解决方案和对其他可控性公式的深入研究是未来工作的一部分。


### C. 训练过程

我们使用梯度下降法，通过最小化上述损失函数的加权和来训练PVEs。本节将详细解释训练过程。

1. 数据采集:首先，机器人通过探测环境来采集数据。由于我们使用的是强化学习设置，所以数据由观察、动作和奖励的序列组成。提出的损失函数大多只使用观察值，可控性损失也使用动作，但我们目前的损失没有一个使用奖励信号。
2. 损失计算:我们以小批量的方式对收集到的数据进行迭代，这些数据由一小组短序列组成。对于每个小批量，我们通过用统计平均值代替期望来计算损失函数。
3. 损失组合:我们把这些损失加权在一起。找到合适的权重是很重要的，因为它们可以平衡学习过程中每个优先级的执行情况。我们根据经验性来调整它们以确定这些权值，直到编码器参数中的梯度具有与所有先验相似的大小。未来的工作应该尝试将这个权重调整过程自动化，可能会以自动化的方式应用相同的启发式
4. 参数更新:对于每个小批，我们使用符号自动微分[1]计算联合损失的梯度与编码器参数，并使用Adam优化[9]执行更新。我们迭代这个过程直到收敛。
5. 速度缩放过程:在训练 PVEs 时，我们遵循的过程一开始关注位置，后来才考虑速度。这个过程是通过改变速度缩放参数$\alpha$来实现。在第一阶段,我们用$\alpha=0$训练，直到收敛。在第二阶段中,我们从0到其最终值线性增加$\alpha$并训练直到收敛。在第一阶段，只有前两个先验，变化和缓慢度，是活跃的。令人惊讶的是，这两个是强大的对手，可以展开位置状态空间的拓扑结构。第二阶段主要是对状态空间进行平滑处理，使速度可以通过有限差分得到精确的估计
6. 超参数:我们在实验中使用了以下超参数。卷积网络有3个卷积层，分别有16、32和64个信道，卷积核大小为5x5和步长为 2，随后是3个全连接层，大小分别为128、128和5(对于5维位置状态)。除最后一层外，每一层都有一个ReLu非线性[14]。小型批处理大小为32个序列，每个序列包含10个步骤。最大速度缩放比$\alpha$是10。不同损失的权重如表I所示


![表1](/assets/images/计算机视觉/PVE/tab1.png)

表1：每个任务损失权重（a和b相同）。

![图2](/assets/images/计算机视觉/PVE/fig2.png)

图2：来自像素输入的3个控制任务

## IV. 实验及结果
我们将PVEs应用于一系列由原始像素输入的模拟控制任务中(如图2所示)，所有的任务都使用 MuJoCo 模拟器[20]。对于每个任务，我们通过随机采样不同位置和速度的起始配置，并应用随机策略，收集了包含1000个20步的短轨迹的训练数据

A. 任务
1. 倒立摆:倒立摆是一根木棍，一端固定在一个旋转接头上。我们的目标是通过在关节处施加力使它向上摆动并保持平衡。然而，电机不够强大，无法一下子把钟摆拉起来。相反，钟摆必须来回摆动，以产生足够(但不是太多)的动量。
2. 车摆:车摆任务是倒立摆任务的延伸。由于这根杆子是与一个被动连接的小车相连的，它只能通过正确地加速小车来摆动起来，这需要精确的控制。
3. 杯中球:这个任务包括一个杯子和一个用绳子连在杯子底部的球。目标是移动杯子，使球落在杯子里。在我们的任务版本中，杯子和球只能在平面内移动

B. 学到的位置速度表征

对于每个任务，我们现在将查看所学习的状态表征。我们通过投射它们的主成分来形象化5维的位置状态。

![图3](/assets/images/计算机视觉/PVE/fig3.png)

图3：对于倒立摆，PVEs学习一个允许精确速度估计的圆形位置表征。(a)和(b)中的每个点都是单个观察的编码。颜色表示通过观察所获得的奖励(红色=高，蓝色=低)。(b)中的黑点表示(c)中的观测序列的编码，黑线表示估计的速度。补充的视频:http://youtu.be/ipGe7Lph0Lw 显示了学习过程，http://youtu.be/u0bQwz89h1I 演示了所学的PVE。

1. 倒立摆:PVE学习的状态表征如图3a所示，我们可以看到将测试集观测编码到位置状态空间中。每个点是一个图像的位置编码。颜色表示在该实例中获得的奖励数量。

图中显示了一些有趣的结果。首先，与类似奖励相对应的观测结果被紧密地编码在位置空间中。第二，位置状态形成一个圆，这是有意义的，因为倒立摆在一个圆中移动。第三，前两个之后的所有主成分都接近于零。这意味着循环编码位于五维空间中的一个平面上——PVE发现这个任务是二维的.

接下来，我们来看看在学习空间中估计的速度。在图3b中，我们使用图3c中所示的单个观察序列的编码来覆盖由reward着色的编码训练数据。位置状态用黑点标记，速度状态向量用直线表示。在观察序列中，钟摆从左侧摆到顶部，然后摆到右侧。类似地，编码的位置通过高奖励区域(红色)从一个中等奖励区域移动到另一边的中等奖励区域。在这个运动过程中，速度估计值与位置空间中的圆相切，且噪声最小，这对于控制摆应该是有用的(参见图3中的视频链接)。


![图4](/assets/images/计算机视觉/PVE/fig4.png)

图4： 对于车摆，PVEs 从不同的观测中学习等效状态表征。补充视频:移动相机学习过程 http://youtu.be/RKlciWWuJfc 和静态相机 http://youtu.be/MYxrA1Bw6MU ，学会了PVE与移动摄像机 http://youtu.be/67QZRsLNTAE

2. 车摆:这里，我们在两个不同的观察上比较 PVEs: 1)使用一个移动的摄像机，它通过侧面旋转跟随小车，2)使用一个静态的摄像机，它覆盖了小车移动的整个区域。图4显示了两个透视图的学习位置表征。

这个实验演示了 PVEs 如何从看起来非常不同的观察(图4e，图4f)中学习等价的内部表征(比较图4a和图4b)。对于这两种观测，状态样本形成一个管，其长度对应于车的位置，而圆形部分表示极点的位置。在这里，PVE使用了五个维度中的三个，从而发现了给定任务的三维本质。

从移动摄像机的观察序列(图4e)可以看出，小车向左移动，杆子在右侧落下。PVE表示这条轨迹(图4c)，它从高回报的红色区域移动到反映杆的运动的蓝色区域，并向右移动，对应于小车的侧移。从静态摄像机的观察序列(图4f)可以看到，当小车向右移动时，杆子向上摆动。因此，编码后的轨迹(图4d)进入红色区域和右侧(左右在这两种表征之间交换)。

3. 杯中球: 这项任务的结果是初步的。由于杯子的运动，这项任务具有挑战性，这与我们的一些机器人的先验不一致。杯子被限制在一个小区域内，由机器人控制，允许快速移动和改变方向。杯子可以从它的位置范围的一端移动到另一端，只需要几个时间步骤。因此，缓慢度先验在这里不成立(除非我们以更高的频率采样观测)。此外，机器人可以在杯子上施加大的力，导致大的加速度和颠簸的动作，这与我们之前对速度变化的许多预测不一致。因此，PVEs 难以对 cup 进行编码，我们将在下一节对其进行量化。

![图5](/assets/images/计算机视觉/PVE/fig5.png)

图5：学到的杯中球位置-速度表征。补充的视频:http://youtu.be/3fLaSL8d4TY 显示了学习过程，http://youtu.be/lIhEGv5kLFo 演示了学到的PVE

为了解决这个问题，我们加入了可控性先验，即机器人控制的事物被编码到状态中。这改进了结果状态表征(参见图5)。虽然状态表征的语义不像前面的任务那样清晰，但是表征使用了四维，这对于一个平面上的两个对象是有意义的。另外，进球状态(杯中的球)与其他状态是明显分开的。正如我们将在下面的章节中看到的，关于cup的信息仍然是非常嘈杂的，这可能是为什么基于PVEs的强化学习不能达到与其他任务相同的性能的原因。这一结果使得杯中球任务成为下一步通过修改和添加机器人先验扩展PVEs的一个很好的候选者。

C. 回归到真实位置和速度

为了测量学习位置-速度表征的质量，我们从学习位置-速度状态回归到相关物体的真实位置和速度。在这里，我们用Adam训练了一个全连接的神经网络，它有3个隐藏的ReLu层，每层256个单元，共200步。我们标准化了真实的位置和速度，并根据所学习的位置和速度状态进行监督学习，以获得最小平均误差的真实特征。经过1000次20步的观察训练，我们测试了100乘以20的测试样本。产生的测试错误如表2所示。


![表2](/assets/images/计算机视觉/PVE/tab2.png)

当我们比较这些误差时，我们发现倒立摆任务的误差是最小的，这是有道理的，因为在这个任务中可能观察到的范围很小，训练数据很好地覆盖了它。对于车摆，位置误差仍然很小，但估计速度的误差较大，因为从有限差分计算速度时，位置状态的噪声增大。此外，当我们从移动相机设置到静态相机设置时，错误会加倍。从这个差异，我们可以预测，控制在第一个设置应该更容易。最后，对于杯中球，由于前面讨论的原因，误差再次变得更大。杯子速度的估计尤其具有挑战性。

注意，我们执行了这个回归测试，以度量这些属性在状态中的编码情况。我们不使用状态标签来训练表征，也不使用它们来学习控制。在下一节中，我们将通过基于这些表征的强化学习性能来衡量学习表征的效用。

D. 强化学习

在本实验中，我们使用基于PVEs学习编码的神经拟合 Q-迭代(NFQ，[15])来学习对这些任务的控制。作为基线，我们在这个初步的工作中使用了带有随机初始化编码的未经训练的PVEs(我们将在以后的工作中与其他方法进行彻底的比较)。对于这个策略，我们使用了一个完全连接的神经网络，它有两个隐含层，每层包含250个sigmoid单元。在每个训练阶段结束后，我们对其进行两次训练，每次训练30 episodes。我们将奖励调整为非正，并取消折扣。我们在多个时间步骤中重复动作(倒立摆和车摆任务4次，杯中球任务6次)。


![图6](/assets/images/计算机视觉/PVE/fig6.png)

图6：基于 PVEs 学习的状态表征，增强不同任务的学习性能。线条表示50次试验的平均值，较深的阴影表示标准误差，较浅的阴影表示从最小值到最大值的范围

最终的学习曲线如图6所示。蓝色曲线显示带有随机编码的基线，不允许学习这三个任务中的任何一个。绿色和红色的曲线显示了基于 PVEs 的强化学习，这些pve在一批20步的1000条轨迹上被训练。对于倒立摆和车摆任务，绿色曲线仅经过50和300个周期就达到最佳性能。红色曲线显示了基于静态摄像机视角的性能并没有达到最佳性能，这可能是由于前一节讨论的更多的噪声状态估计。在这一点上，还不清楚这个问题是来自输入的低分辨率，还是来自杆和车的位置在这些观测中耦合更强，这使得学习状态编码更加困难。最后，对于杯中球任务，学习控制一致地超过基线(如浅绿色最大阴影所示)，使用学习的表征更成功的控制是可能的。但是，由于噪声状态估计的存在，这一方法不能很好地解决问题。未来的工作可以从这里开始，并调查解决这个问题和更现实的机器人任务缺少哪些先验


## V. 结论与未来工作

我们提出了位置速度编码器(PVEs)，它能够学习由一个位置和一个速度构造的状态表征，没有监督，也不需要图像重建。pve的关键是约束模型以正确的方式从位置估计速度，并通过优化与机器人先验(具体到位置、速度和加速度)的一致性来训练位置编码器。我们已经展示了如何将状态空间组织成位置和速度，从而为制定有用的约束和学习目标提供了新的机会。在未来的研究中，我们将致力于在状态空间中添加更多的结构，修改和扩展机器人先验列表，并将这些方法与端到端强化学习相结合


---
**参考**：
1. 论文：Rico Jonschkowski, Roland Hafner, Jonathan Scholz, Martin Riedmiller [PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations](https://arxiv.org/abs/1705.09805)
