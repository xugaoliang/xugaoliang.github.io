rbf 刘老师笔记

https://note.youdao.com/share/?id=ed6749079175de95ce57cdd4e8c38db2&type=note#/



抓住了非线性的无限映射



![C7BD3FE160E78549349EB875F59490BD](../../../../../Library/Containers/com.tencent.qq/Data/Library/Caches/Images/C7BD3FE160E78549349EB875F59490BD.png)

工程上用的多的是VGGnet  resNet,googlenet

googlenet中有个inception+短路机制



机器学习，第一应用，第二算法

深度学习，第一算法，第二应用



去均值：让均值变到0

归一化：方差大导致收敛速度变慢，图像0-255，不需要归一化



关于激活函数：http://www.cnblogs.com/neopenx/p/4453161.html

卷积动态图：http://cs231n.github.io/assets/conv-demo/index.html



手动提取，Hand-crafted



【链接】模式识别之特征提取算法
https://blog.csdn.net/xiongchao99/article/details/78776629



sigmoid不会跟在卷积层，一般在FC层，原因不清楚



Pooling作用（down sampling ）：

  (1) 简化运算 

 (2)增大receptive field。通过下采样，之后，再计算，同样一块区域，包含的信息是图像更大范围的信息



增大kernel,也能增大receptive field，但计算量也增大了

pyramid 金字塔，很重要的概念，传统算法和深度学习都提到，论文引用第一。解决scale 尺度不变性，大人，小人都能识别出来



死神经元不依赖于用的函数，而是依赖于层数



低频特征，高频特征



随机裁剪：用crop,不用scale,scale效果差



DNN,又叫MLP,多层感知机



LeNet：输入大小无限制，5*5 conv，2 * 2pool,5 * 5conv, 2 * 2 pool,fc,fc,Gauss(rbf)+softmax

AlexNet：

ZF Net:

GoogleNet:基于VGGNet,用Inception

VGGNet：* 重要（一定要会）

ResNet:基于VGG,用identity shortcut

desnet：基于resnet

inception resnet :基于googlenet,resnet开发

后五个都要会



googleNet :通过减少通道数来大幅降低维度通过减小卷积核尺寸可以减少参数；另外，还从通告的维度，通过减少通道数目大幅降低参数，从而使得整个网络加深以及速度加快得到了实现



track技巧：inception机制、identity shortcut机制

一个epoch一般刷3k-6k次



gensim

standfor nlp word2vec



目标检测：传统ACF很好，

目标检测模型汇总：https://github.com/xiayangdi/awesome-object-detection/



spp
RPN

faster,慢，但漏检可能性小 ，9个框
ssd，4-6个框
yolo,快，漏检的多，不稳定，2个框

	1.图像分割技术
	
	2.损失函数合在一起
	
	3.小框回归

无人驾驶：

	1.强化学习
	
	2.蒙特卡洛搜索
	
	3.深度学习



tanh特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果显示出来，但有是，在特征相差比较复杂或是相差不是特别大时，需要更细微的分类判断的时候，sigmoid效果就好了。  还有一个东西要注意，sigmoid 和 tanh作为激活函数的话，一定要注意一定要对 input 进行归一话，否则激活后的值都会进入平坦区，使隐层的输出全部趋同，但是 ReLU 并不需要输入归一化来防止它们达到饱和。



构建稀疏矩阵，也就是稀疏性，这个特性可以去除数据中的冗余，最大可能保留数据的特征，也就是大多数为0的稀疏矩阵来表示。其实这个特性主要是对于Relu，它就是取的max(0,x)，因为神经网络是不断反复计算，实际上变成了它在尝试不断试探如何用一个大多数为0的矩阵来尝试表达数据特征，结果因为稀疏特性的存在，反而这种方法变得运算得又快效果又好了。所以我们可以看到目前大部分的卷积神经网络中，基本上都是采用了ReLU 函数。



常用的激活函数  激活函数应该具有的性质：  

（1）非线性。线性激活层对于深层神经网络没有作用，因为其作用以后仍然是输入的各种线性变换。。

（2）连续可微。梯度下降法的要求。  

（3）范围最好不饱和，当有饱和的区间段时，若系统优化进入到该段，梯度近似为0，网络的学习就会停止。  

（4）单调性，当激活函数是单调时，单层神经网络的误差函数是凸的，好优化。

（5）在原点处近似线性，这样当权值初始化为接近0的随机值时，网络可以学习的较快，不用可以调节网络的初始值。



池化层：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征

pooling操作是特征图缩小，有可能影响网络的准确度，因此可以通过增加特征图的深度来弥补

通过池化来降低卷积层输出的特征向量，同时改善结果（不易出现过拟合）。



神经网络的参数的**随机初始化**的目的是使对称失效。否则的话，所有对称结点的权重都一致，也就无法区分并学习了。



Dropout是神经网络中解决过拟合问题的一种常见方法。

由于神经网络的非线性，Dropout的理论证明尚属空白，这里只有一些直观解释。

1.dropout掉不同的隐藏神经元就类似在训练不同的网络，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。这实际上就是bagging的思想。

2.因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这会迫使网络去学习更加鲁棒的特征。换句话说，假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。



脑神经元接受信号更精确的激活模型对比Sigmoid系主要变化有三点：

①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性

Softplus照顾到了新模型的前两点，却没有稀疏激活性。因而，校正函数max(0,x)成了近似符合该模型的最大赢家。



稀疏性大概有以下三方面的贡献：

1.1 信息解离

原始数据特征向量是相互关联的，一个小小的关键因子可能牵扰着一堆特征，有点像蝴蝶效应，牵一发而动全身。

如果能够解开特征间缠绕的复杂关系，转换为稀疏特征，那么特征就有了鲁棒性（去掉了无关的噪声）。

1.2 线性可分性

稀疏特征有更大可能线性可分，或者对非线性映射机制有更小的依赖。

1.3 稠密分布但是稀疏

稠密缠绕分布着的特征是信息最富集的特征，从潜在性角度，往往比局部少数点携带的特征成倍的有效。

而稀疏特征，正是从稠密缠绕区解离出来的，潜在价值巨大。

1.4 稀疏性激活函数的贡献的作用：

不同的输入可能包含着大小不同关键特征，使用大小可变的数据结构去做容器，则更加灵活。

假如神经元激活具有稀疏性，那么不同激活路径上：不同数量（选择性不激活）、不同功能（分布式激活），

两种可优化的结构生成的激活路径，可以更好地从有效的数据的维度上，学习到相对稀疏的特征，起到自动化解离效果。

 

在深度网络中，对非线性的依赖程度就可以缩一缩。另外，在上一部分提到，稀疏特征并不需要网络具有很强的处理线性不可分机制。综合以上两点，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。

诚然，稀疏性有很多优势。但是，过分的强制稀疏处理，会减少模型的有效容量。即特征屏蔽太多，导致模型无法学习到有效特征。论文中对稀疏性的引入度做了实验，理想稀疏性（强制置0）比率是70%~85%。超过85%，网络就容量就成了问题，导致错误率极高。

对比大脑工作的95%稀疏性来看，现有的计算神经网络和生物神经网络还是有很大差距的。



ReLu的使用，使得网络可以自行引入稀疏性。这一做法，等效于无监督学习的预训练。





图像处理：自动驾驶、安防，工程检测（质检），图像检测识别，图像检索、图像美容、图像融合

数据挖掘、推荐

nlp:





图像pca降维会不会变？？？

graph embedding是个啥东东？？

pytorch扫盲

adam，SGD





文本分类，数据类别不均衡，多的去重，去重方法：simhash 、overlap、编辑距离



