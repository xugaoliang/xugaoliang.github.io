# 【译】使用深度学习进行答案选择：一个研究及开放性任务

* 作者：Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, Bowen Zhou
* 论文：《APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK》
* 地址：https://arxiv.org/abs/1508.01585


## 摘要

我们应用一个通用的深度学习框架来解决非事实性的问答任务。我们的方法不依赖于任何语言工具，可以应用于不同的语言或领域。介绍并比较了各种体系结构。我们创建并发布了一个QA语料库，并在保险领域中设置了一个新的QA任务。实验结果表明，与基准方法相比，该方法具有更好的性能，各种技术也得到了进一步的改进。对于这项极具挑战性的任务，测试集的 top-1 准确率可达65.3%，具有很大的实际应用潜力。

**关键词（Index Terms）** —— Answer Selection, Question Answering, Convolutional Neural Network (CNN), Deep Learning, Spoken Question Answering System

## 1. 介绍

基于自然语言理解的口语对话系统是近年来人工智能复兴的一个热门话题。这些颇具影响力的系统中，有很多都包括问答模块，比如苹果的 Siri、IBM 的沃森(Watson)和亚马逊的 Echo。在本文中，我们来解决口语QA系统中的问答模块。我们从文本匹配和选择的角度来处理QA。IBM 的 Watson 系统是传统问答(QA)方法的一个经典例子。在这项工作中，我们使用一个深度学习框架来完成答案选择，这是QA任务中的一个关键步骤。因此，从答案匹配和选择的角度研究问答问题。给定一个问题 $q$ 和该问题的一个答案候选池$\{a_1,a_2,\cdots,a_s\}$($s$为池大小)，目标是找到最佳候选答案$a_k,1\le k \le s$。如果选择的答案 $a_k$ 在问题 $q$ 的 ground truth set 中(一个问题可以有多个正确答案)，则认为问题 $q$ 被正确回答，否则就是错误回答。从定义上看，QA 问题可以看作是一个二元分类问题。对于每一个问题，每一个候选答案，它可能是恰当的，也可能不是。为了找到最好的一对问答，我们需要一个度量标准来度量每个QA对的匹配程度，从而选择度量值最高的QA对。

上述定义是一般性的。唯一的假设是，每个问题都有一个候选答案池。在实践中，可以使用诸如谷歌搜索之类的通用搜索引擎或诸如 Apache Lucene之类的信息检索软件库轻松构建池。

我们通过从互联网上收集问答对创建了一个数据集。所有这些问答对都属于保险领域。保险领域QA语料库的建设是由该领域浓厚的科学和商业利益驱动的。我们发布了这个语料库（https://github.com/shuzi/insuranceQA.git）来创建一个开放的QA任务，使其他研究人员能够利用它，并支持不同方法之间的公平比较。语料库由四个部分组成:train,development,test1 和 test2。表1给出了数据统计。本文的所有实验都是基于该语料库进行的。据我们所知，这是第一次发布保险领域QA任务。

||Questions|Answers|Question Word Count
|-|-|-|-
|Train | 12887 | 18540 | 92095
|Dev | 1000 | 1454 | 7158
|Test1 | 1800 | 2616 | 12893
|Test2 | 1800 | 2593 | 12905

表1：语料库统计:前两列是问答数量；注意，有些问题可能有多个答案因此答案数量大于问题数量；第三栏是问题总字数。答案总数为 24981，整篇答案包含 2386749 个字。

我们的QA任务需要为 development,test1 和 test2 数据集中的每个问题指定一个候选答案池。发布的语料库共包含 24981 个独特的答案。可以使用整个答案空间作为候选池，因此每个问题必须与24981个答案候选人进行比较。然而，由于计算耗时，这是不切实际的。在本文中，我们将池大小设置为500，这样既实用又具有挑战性。我们将 ground truth 的答案放入池中，并从答案空间随机抽取 negative 答案，直到池的大小达到500。

本文描述的技术与发布的数据集和基准测试任务是针对潜在的应用，如在线客户服务。因此，它不应该处理需要推理的问答任务，例如明天是星期二吗?(答案取决于今天是不是星期一。)本文的其余部分组织如下:第2节描述了用于该工作的不同架构;第3节提供了实验设置细节;实验结果和讨论见第4节;第5节包含了相关的工作，最后我们在第6节中得出结论。

## 2. 模型描述

在本节中，我们将描述所提议的深度学习框架以及基于该框架的许多变体。然而，这些不同系统的主要思想是相同的:学习给定问题及其候选答案的分布式向量表征，然后使用相似性度量来度量匹配程度。我们首先开发了两个基线系统进行比较。

### 2.1 基线系统

第一个基线系统是词袋模型。第一步是通过 word2vec (Tomas Mikolov) 训练一个词嵌入。这个词嵌入为问题及其候选答案中的每个 token 的词向量。从这些数据中，基线系统为问题及其所有候选答案生成词向量的 idf 加权和。这将为问题和每个候选答案生成一个向量表征。最后一步是计算每个问题/候选对之间的余弦相似性。返回余弦相似度最高的一对作为答案。第二个基线模型是信息检索(IR)基线模型。采用最先进的加权依赖模型(WD)（Michael Bendersky 等人 《Learning concept importance using a weighted
dependence model》及《Parameterized concept weighting in verbose queries》）。WD模型使用基于词汇和基于词汇近似值的排序特征的加权组合来为每个候选答案打分。示例特征为候选答案中包含问题中的有序 bigrams 的计数和不同窗口大小的无序 bigrams 计数，以及简单的 unigram 计数。其基本思想是，当计算问题中重要的 bigrams 或 unigrams 的频率时，它们应该得到更高的权重。因此，特征权重是根据它们定义的问题的重要性分配的，其中重要因素是作为模型训练过程的一部分学习的。表2的第1行和第2行(第一列为索引)是基线系统结果。

### 2.2 基于 CNN 的系统

本文提出了一种基于卷积神经网络(CNN)的QA框架。正如《Deep learning》(Yoshua Bengio 等，2015)的第11章所总结的，CNN利用了三个可以帮助改进机器学习系统的重要思想:**稀疏交互**、**参数共享**和**等变表示**。稀疏交互与传统的神经网络形成对比，传统的神经网络中，每个输出都与每个输入交互。在CNN中，过滤器大小(或内核大小)通常比输入大小小得多。因此，输出只与输入的窄窗口交互。参数共享是指在卷积运算中对滤波参数进行重用，而传统神经网络权矩阵中的元素只使用一次来计算输出。等变表示与 $k$-MaxPooling 的思想有关，它通常与 CNN 相结合。在本文中，我们总是令 $k = 1$。所以CNN的每个滤波器都代表一些特征，经过卷积运算，1-MaxPooling 值代表输入包含特征的最高程度。这个特征在输入中的位置与卷积无关。此属性对于许多NLP应用程序非常有用。下面是一个示例来演示我们的CNN实现。

$$
\tag{1}

\begin{pmatrix} 
w_{11} & w_{21} & w_{31} & w_{41} \\\\
w_{12} & w_{22} & w_{32} & w_{42} \\\\
w_{13} & w_{23} & w_{33} & w_{43} 
\end{pmatrix}

\odot

\begin{pmatrix} 
f_{11} & f_{21} \\\\
f_{12} & f_{22} \\\\
f_{13} & f_{23}
\end{pmatrix}
$$

左边的矩阵 $W$ 是输入语句。每个单词由一个三维词嵌入向量表示，输入长度为4。右边的矩阵 $F$ 表示滤波器。2-gram 滤波器的尺寸为 $3 \times 2$。输入$W$和滤波器$F$的卷积输出是一个3维向量$O$，假设已经做了0 padding，那么只进行一个窄卷积。

$$
\tag{2}
o_1= w_{11}f_{11} + w_{12}f_{12} + w_{13}f_{13} + w_{21}f_{21} + w_{22}f_{22} + w_{23}f_{23} \\\\
o_2= w_{21}f_{11} + w_{22}f_{12} + w_{23}f_{13} + w_{31}f_{21} + w_{32}f_{22} + w_{33}f_{23} \\\\
o_3= w_{31}f_{11} + w_{32}f_{12} + w_{33}f_{13} + w_{41}f_{21} + w_{42}f_{22} + w_{43}f_{23} \\\\
$$

在1-MaxPooling之后，filter $F$保留3个值中的最大值，表示 filter $F$与输入$W$匹配的最高程度。

### 2.3 训练和损失函数

稍后将描述不同的体系结构。然而，所有这些不同的架构都共享相同的训练和测试机制。在本文中，我们最小化了类似于#tagspace: Semantic embeddings from hashtags（Jason Weston 等） 和 Convolutional neural network architectures for matching natural language sentences（Baotian Hu 等） 的排序损失。在训练过程中，每个训练问题Q都有一个正例的答案$A^{+}$(the ground truth)。训练实例是通过将这个$A^{+}$与从整个答案空间中抽取的一个负例答案$A^{-}$(一个错误的答案)配对来构建的。深度学习框架为问题和两个候选项生成向量表征:$V_Q$,$V_{A^+}$,$V_{A^-}$。计算余弦相似度 $\cos(V_Q,V_{A^+})$和$\cos(V_Q,V_{A^-})$，并将两者之间的距离与一个 margin 进行比较:$\cos(V_Q,V_{A^+})-\cos(V_Q,V_{A^-})<m$,$m$是 margin。当这个条件满足时，这意味着嵌入的向量空间把正答案排在负答案下面，或者说没有把正答案排在负答案上面，如果$\cos(V_Q,V_{A^+})-\cos(V_Q,V_{A^-}) \ge m$，则不更新参数，直到边距小于m时才采样一个新的反例(为了减少运行时间，我们在本文中设置了50次最大值)。由此定义关键的损耗函数为:

$$
\tag{3}
L = \max \{0,m-\cos(V_Q,V_{A^+})+\cos(V_Q,V_{A^-})\}
$$

为了测试，我们计算问题$Q$和池中每个答案候选$V_{candidate}$(大小为500)之间的$cos(V_Q, V_{candidate})$。选择余弦相似度最大的候选答案。

### 2.4 体系结构

![图1](/assets/images/QACNN/fig1.png)
图1：体系结构1。Q表示问题；A表示答案；P表示 1-MaxPooling；T表示tanh层；HL表示隐藏层，并且HL总是包含tanh作为激活函数

![图2](/assets/images/QACNN/fig2.png)
图2：体系结构II。QA意味着相应的层的权重是被Q和A共享的

![图3](/assets/images/QACNN/fig3.png)
图3：体系结构III。HL表示隐藏层，在$\mathrm{CNN}_{QA}$后面添加了另一个$\mathrm{HL}_Q$和$\mathrm{HL}_A$

![图4](/assets/images/QACNN/fig4.png)
图4：体系结构IV。在$\mathrm{CNN}_{QA}$后面添加了另一个共享隐藏层$\mathrm{HL}_{QA}$

![图5](/assets/images/QACNN/fig5.png)
图5：体系结构V。两个共享的$\mathrm{CNN}_{QA}$

![图6](/assets/images/QACNN/fig6.png)
图6：体系结构VI。两个共享的$\mathrm{CNN}_{QA}$，两个代价函数

Idx | Dev | Test1 | Test2 | Description
-|-|-|-|-
1 | 31.9 | 32.1 | 32.2 | Baseline: Bag-of-words
2 | 52.7 | 55.1 | 50.8 | Baseline: metzler-bendersky IR model
3 | 44.2 | 41.7 | 39.5 | Architecture I: HLQ(200) HLA(200) CNNQ(1000) CNNA(1000) 1-MaxPooling Tanh
4 | 58.2 | 57.8 | 53.6 | Architecture II: HLQA(200) CNNQA(1000)1-MaxPooling Tanh
5 | 36.1 | 33.6 | 32.7 | Architecture III: HLQA(200) CNNQA(1000) HLQ(1000) HLA(1000) 1-MaxPooling Tanh
6 | 51.4 | 50.5 | 46.1 | Architecture IV: HLQA(200) CNNQA(1000) HLQA(1000) 1-MaxPooling Tanh
7 | 47.0 | 46.7 | 43.0 | Architecture IV: HLQA(200) CNNQA(1000) HLQA(500) 1-MaxPooling Tanh
8 | 60.6 | 59.2 | 55.1 | Architecture II: HLQA(200) CNNQA(2000) 1-MaxPooling Tanh
9 | 61.5 | 61.3 | 57.8 | Architecture II: HLQA(200) CNNQA(3000) 1-MaxPooling Tanh
10 | **61.8** | **62.8** | **59.2** | Architecture II: HLQA(200) CNNQA(4000) 1-MaxPooling Tanh (best result in this table)
11 | 59.7 | 59.3 | 55.6 | Architecture V: HLQA(200) CNNQA(1000) CNNQA(1000) 1-MaxPooling Tanh
12 | 59.9 | 60.6 | 55.9 | Architecture VI: HLQA(200) CNNQA(1000) CNNQA(1000) 1-MaxPooling Tanh (2COST)
13 | 59.9 | 58.7 | 53.8 | Architecture II: HLQA(200) Augmented-CNNQA(1000) 1-MaxPooling Tanh
14 | 60.0 | 60.3 | 54.3 | Architecture II: HLQA(200) Augmented-CNNQA(2000) 1-MaxPooling Tanh
15 | 61.7 | 62.2 | 56.3 | Architecture II: HLQA(200) Augmented-CNNQA(3000) 1-MaxPooling Tanh
表2：实验结果。HL(200)表示隐含层大小为200;CNN(1000)表示使用了1000个过滤器;Dev、Test1和Test2的 top-1 precision 被公布。


在本小节中，我们将为这个QA任务演示几个建议的体系结构。图1显示了体系结构I。Q是作为输入问题提供给第一个隐层$\mathrm{HL}_Q$的输入。隐层(HL)定义为$z = \tanh(W x+B)$。$W$是权重矩阵;$B$是偏置向量;$x$是输入;$z$是激活函数$tanh$的输出。然后输出流到CNN层$\mathrm{CNN}_Q$，用于提取问题的特征。P是 MaxPooling 层(在本文中我们总是使用1-MaxPooling)， T是$tanh$层。与问题类似，对答案 A 进行$\mathrm{HL}_A$处理，然后通过$\mathrm{CNN}_A$提取特征。1-MaxPooling 层 P 和 $tanh$层 T 将在最后一步发挥作用。结果是向量表示的问题和答案。最终的输出是这些向量之间的余弦相似度。表2的第3行是我得到的体系结构。

图2是体系结构II。与体系结构 I相比，主要的不同之处在于，问答双方的HL和CNN权重相同。表2的第4行是体系结构 II的结果。

我们还考虑了CNN之后接隐藏层的结构。图3是体系结构III，其中在CNN之后的问题端添加另一个$\mathrm{HL}_Q$，在CNN之后的回答端添加另一个$\mathrm{HL}_A$。表2的第5行是体系结构III的结果。体系结构IV(如图4所示)与此类似，只是问题和答案的第二个HL具有相同的$\mathrm{HL}_{QA}$权重。表2的第6行和第7行是体系结构IV的结果。

图5是体系结构V，其中部署了两层$\mathrm{CNN}_{QA}$。在第2.2节中，我们展示了卷积输出是一个向量(在那个例子中是3-dim)。这只适用于只有一个过滤器的 CNNs。通过应用多个过滤器，结果是一个矩阵。如果在2.2节的示例中使用了4个过滤器，则输出如下矩阵：

$$
\tag{4}
\begin{pmatrix} 
o_{11} & o_{21} & o_{31} \\\\
o_{12} & o_{22} & o_{32} \\\\
o_{13} & o_{23} & o_{33} \\\\
o_{14} & o_{24} & o_{34} 
\end{pmatrix}
$$

每一行表示一个过滤器的输出，每一列表示输入的 bigram。这个矩阵是下一个$\mathrm{CNN}_{QA}$层的输入。对于第二层，每个 bigram 实际上是一个“单词”，前一个过滤器为该 bigram 输出的是它的词嵌入。表2的第11行是架构V的结果。图6中的体系结构VI类似于体系结构V，只是我们使用了分层监督。每个$\mathrm{CNN}_{QA}$层之后都有一个 1-MaxPooling 和一个 tanh 层，这样就可以计算成本函数并进行反向传播。架构VI的结果在表2的第12行。

我们尝试了另外三种技术来改进图2中的体系结构II。首先，CNN的过滤器数量增加了，见表2第8、9、10行。其次，卷积运算被扩展到包含 skip-bigrams。考虑2.2节中的例子，对于Eq.1中的输入和一个滤波器，增广卷积运算不仅会产生Eq.2，还会产生以下不连续卷积:

$$
\tag{5}
o_4= w_{11}f_{11} + w_{12}f_{12} + w_{13}f_{13} + w_{31}f_{21} + w_{32}f_{22} + w_{33}f_{23} \\\\
o_5= w_{21}f_{11} + w_{22}f_{12} + w_{23}f_{13} + w_{41}f_{21} + w_{42}f_{22} + w_{43}f_{23} 
$$

仍然使用1-MaxPooling来获得$[o_1, o_2, o_3, o_4, o_5]$中最大的值，这样这个过滤器就会自动适应以匹配一个 bigram 或 skip-bigram 特征。表2的第13、14和15行显示了结果。第三，研究相似性度量。到目前为止，我们一直在使用余弦相似度，这是广泛采用的向量空间模型。然而，余弦相似度是这个任务的最佳选择吗?表3为相似性度量研究结果。一些指标包括超参数，并进行了各种超参数的实验。我们提出了两种新的度量标准(GESD和AESD)，证明了优越的性能。

Dev | Test1 | Test2 | Description
-|-|-|-
58.2 | 57.8 | 53.6 | cosine: $k(x, y) = \frac{xy^\top}{\|x\|\|y\|}$
58.5 | 57.1 | 53.3 | polynomial: $k(x, y) = (\gamma xy^\top + c)^d$,$\gamma=0.5$,$d=2$,$c=1$
56.8 | 54.6 | 52.6 | polynomial: $k(x, y) = (\gamma xy^\top + c)^d$,$\gamma=1.0$,$d=2$,$c=1$
55.0 | 53.6 | 48.2 | polynomial: $k(x, y) = (\gamma xy^\top + c)^d$,$\gamma=1.5$,$d=2$,$c=1$
57.1 | 53.7 | 51.5 | polynomial: $k(x, y) = (\gamma xy^\top + c)^d$,$\gamma=0.5$,$d=3$,$c=1$
55.3 | 52.4 | 48.7 | polynomial: $k(x, y) = (\gamma xy^\top + c)^d$,$\gamma=1.0$,$d=3$,$c=1$
52.5 | 51.0 | 47.2 | polynomial: $k(x, y) = (\gamma xy^\top + c)^d$,$\gamma=1.5$,$d=3$,$c=1$
61.3 | 59.9 | 57.0 | sigmoid: $k(x, y) = \tanh(\gamma xy^\top + c)$,$\gamma=0.5$,$c=1$
61.6 | 60.2 | 57.1 |  sigmoid: $k(x, y) = \tanh(\gamma xy^\top + c)$,$\gamma=1.0$,$c=1$
60.2 | 60.2 | 55.7 |  sigmoid: $k(x, y) = \tanh(\gamma xy^\top + c)$,$\gamma=1.5$,$c=1$
60.0 | 60.3 | 54.7 | RBF: $k(x, y) = \exp(−\gamma\|x-y\|^2)$,$\gamma=0.5$
60.2 | 57.0 | 54.4 |  RBF: $k(x, y) = \exp(−\gamma\|x-y\|^2)$,$\gamma=1.0$
58.4 | 57.3 | 53.8 |  RBF: $k(x, y) = \exp(−\gamma\|x-y\|^2)$,$\gamma=1.5$
60.8 | 60.3 | 57.0 | euclidean: $k(x, y) = \frac{1}{1+\|x-y\|}$
42.2 | 42.5 | 38.2 | exponential: $k(x, y) = \exp(−\gamma\|x − y\|_1)$,$\gamma = 0.5$
41.4 | 39.5 | 36.0 |  exponential: $k(x, y) = \exp(−\gamma\|x − y\|_1)$,$\gamma = 1.0$
48.2 | 45.1 | 41.6 |  exponential: $k(x, y) = \exp(−\gamma\|x − y\|_1)$,$\gamma = 1.5$
51.0 | 49.5 | 46.4 | manhattan: $k(x, y) = \frac{1}{1+\|x-y\|_1}$
62.5 | 61.4 | 59.0 | GESD: $k(x, y) = \frac{1}{1+\|x-y\|} \cdot \frac{1}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=0.5$,$c=1$
62.9 | 62.1 | 59.3 | GESD: $k(x, y) = \frac{1}{1+\|x-y\|} \cdot \frac{1}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$,$c=1$
62.6 | 62.1 | 59.2 | GESD: $k(x, y) = \frac{1}{1+\|x-y\|} \cdot \frac{1}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.5$,$c=1$
63.1 | 61.9 | 58.2 | AESD: $k(x, y) = \frac{0.5}{1+\|x-y\|} + \frac{0.5}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=0.5$,$c=1$
63.4 | 61.7 | 58.7 | AESD: $k(x, y) = \frac{0.5}{1+\|x-y\|} + \frac{0.5}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$,$c=1$
62.8 | 62.0 | 57.7 | AESD: $k(x, y) = \frac{0.5}{1+\|x-y\|} + \frac{0.5}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.5$,$c=1$
-|-|-|-----
63.5 | 62.5 | 60.2 | GESD: $k(x, y) = \frac{1}{1+\|x-y\|} \cdot \frac{1}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$, 2000 filters
64.3 | 65.1| 61.0 | GESD: $k(x, y) = \frac{1}{1+\|x-y\|} \cdot \frac{1}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$, 3000 filters
**65.4** | **65.3** | 61.0 | GESD: $k(x, y) = \frac{1}{1+\|x-y\|} \cdot \frac{1}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$, 4000 filters
64.5 | 62.7 | 60.1 | AESD: $k(x, y) = \frac{0.5}{1+\|x-y\|} + \frac{0.5}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$,2000 filters
64.3 | 63.3 | **62.2** | AESD: $k(x, y) = \frac{0.5}{1+\|x-y\|} + \frac{0.5}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$,3000 filters
63.9 | 64.5 | 61.1 |  AESD: $k(x, y) = \frac{0.5}{1+\|x-y\|} + \frac{0.5}{1+\exp (-\gamma(xy^\top+c))}$,$\gamma=1.0$,4000 filters

表3：实验结果各有相似之处。以上部分的结果均基于1000个filter的体系结构 II(与表2中的第4行对应)，下端的结果基于体系结构 II，使用了更多filter用于提议的度量。$k(x, y)$是向量$x$和$y$的相似度。$\|x\|$是$L_2$范数，$\|x\|_1$是$L_1$范数。$xy^\top$代表$x$和$y$的内积。我们总是在计算相似度之前归一化问题和答案向量。每一列中最大的数字用粗体表示。

## 3. 实验设置

本文的深度学习框架是使用Java从零开始构建的。为了提高速度，我们采用了HOGWILD方法（Benjamin Recht 等“Hogwild: A lock-free approach to parallelizing stochastic gradient descent”）。每个线程处理一个训练实例，并更新神经网络的权值。任何线程都没有锁定。词嵌入(100维)由word2vec训练并用于初始化。词嵌入也是参数，并且针对QA任务进行了优化。随机梯度下降法是一种优化策略，并在损失函数中加入l2范数。本文l2 -范数的权值为0.0001，学习率为0.01，margin m为0.009。这些超参数的选取是基于以往对这些数据进行深度学习的经验，在合理范围内并不十分敏感。

这项工作所使用的计算资源是巨大的。我们严重占用一个Power 7集群，它由75台机器组成。每台机器有32个物理内核，每个内核支持2-4个超线程。由于没有锁定，HOGWILD方法将带来一些随机性。即使使用锁定，线程调度程序也会在运行之间更改示例的顺序，所以仍然存在随机性。因此，对于表2(除了第1行第2行)和表3中的每一行，都在dev set上进行了10次实验，并选择dev分数最好的运行来计算测试分数。

## 4. 结果与讨论

本节对实验结果进行了详细的分析。从表2和表3可以得出以下结论:(1)基线1只使用了词的嵌入，基线2基于传统的基于词的特征。我们提出的方法可以达到更好的精度，这说明了深度学习方法的优越性;(2)对Q和A使用单独的隐藏层(HL)或CNN层相比于共享HL或CNN层(表2、3和4行,行5和6)性能有所下降。这是合理的,因为一个共享的网络层,Q和A向量对应元素经过CNN滤波器保证得到表征相同的卷积结果，在单独处理Q和A向量时就没有这样的约束并且优化器需要学习拟合双倍的参数。因此，优化器面临更大的困难;(3)在CNN后添加HL降低了性能(表2，第4行vs. 6行和第7行)，这证明CNN已经捕获了用于QA匹配的有用特性，将这些特性映射到另一个空间毫无意义;(4)增加CNN的过滤器数量，可以捕捉到更多的特征，得到显著改善(表2，第4行vs. 8,9,10);(5)两层CNN可以表示更高层次的抽象，输入范围更广。因此，通过使用两个CNN层进行更深入的操作可以提高准确性(表2，第4行vs. 11);(6)深度网络的有效学习往往是一项艰巨的任务。分层管理可以缓解这个问题(表2，第11行与第12行);(7)将 bigram 和 skip-bigram 特性结合起来，可以在Test1上获得收益，但在Test2上没有(表2，第4行vs. 13，第8行vs. 14，第9行vs. 15);(8)表3表明，在模型容量相同的情况下，相似度度量起着重要的作用，而广泛使用的余弦相似度并不是该任务的最佳选择。表3中的相似度可分为三类:基于L1-norm的度量，即Q与A的语义距离与各坐标轴的距离之和;基于l2-norm的度量，即Q与A的直线语义距离;基于内积的度量度量Q与A之间的夹角，我们提出了提出了结合l2-norm和内积的两个新度量,用乘法(欧几里德和 Sigmoid 点积的GESD几何平均值)和加法(欧几里德和 Sigmoid 点积的AESD算术平均值)。所提出的两个指标是所有比较指标中最好的。最后，在表3的底部可以清楚地看到，使用更多的过滤器，所提出的度量可以获得更好的性能。


## 5. 相关工作

深度学习技术在机器学习任务中得到了广泛的应用，其性能往往优于传统的学习方法。其中许多应用都专注于与分类相关的任务，例如图像识别[9]、语音[10][11][12]和机器翻译[13][14]。本文是在前人利用深度学习进行NLP任务研究的基础上，Gao等人提出了一种基于CNN的网络，该网络将源-目标文档对映射到嵌入向量，使源文档与其对应的感兴趣的目标之间的距离最小化。Lu和Li[16]提出了一种基于CNN的深度网络用于短文本匹配任务;Hu等人的[7]也使用了几个基于CNN的网络进行句子匹配;Kalchbrenner等人使用CNN进行情绪预测和问题分类;Kim[18]在情绪分析中使用CNN;Zeng 等人[19]使用CNN进行关系分类;Socher等人[20][21]使用递归网络进行释义检测和解析;Iyyer等人[22]提出了一种递归的事实性问答网络;Weston等人[6]使用CNN进行 hashtag 预测;Yu等人[23]使用CNN进行答案选择;Yin和Schutze[24]使用bi-CNN进行释义识别。我们的工作遵循了之前许多工作的精神，我们利用CNN将自然语言的句子映射成嵌入向量，这样就可以计算出相似度。然而，本文对以往工作中没有涉及的各种体系结构进行了广泛的实验。此外，我们还探索了不同的相似性度量、基于 skip-bigram 的卷积和分层监督，这些在以前的工作中都没有被提出。

## 6. 总结

本文采用深度学习框架，从选择答案的角度对口语问答系统进行研究。该框架不依赖于任何语言工具，可以很容易地适应不同的语言或领域。我们的工作有力地证明了基于深度学习的QA是一个令人鼓舞的研究方向。(1)在保险领域创建一个新的QA任务，并发布一个新的语料库，使不同的方法能够公平比较;(2)针对QA任务提出了一个具有多种变体的通用深度学习框架，并进行了对比实验;(3)利用带来改进的新技术:分层监督的多层CNN，不连续卷积的增强CNN, l2-norm 与内积信息相结合的新相似性度量;(4)本文的最佳得分非常有期待:对于这个具有挑战性的任务(从500个大小的库中选择一个答案)，测试语料库的最高一项准确率可达65.3%;(5)对于想要继续这一任务的研究者，本文提供了有价值的指导:应该采用共享层结构;不需要在CNN之后添加隐藏层;两级CNN分层训练能提高准确率;不连续卷积有时会有帮助;相似度指标起着至关重要的作用，提出的相似度指标是首选指标，最后提高滤波器的数量带来了改进。

## 7. 参考

[1] David A. Ferrucci, Eric W. Brown, Jennifer ChuCarroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty, “Building watson: An overview of the deepqa project,” AI Magazine, vol. 31, no. 3, pp. 59–79, 2010.

[2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean, “Distributed representations of words and phrases and their compositionality,” in Advances in Neural Information Processing Systems 26, pp. 3111–3119. Curran Associates, Inc., 2013.

[3] Michael Bendersky, Donald Metzler, and W. Bruce Croft, “Learning concept importance using a weighted dependence model,” in Proceedings of the Third ACM International Conference on Web Search and Data Mining, New York, NY, USA, 2010, WSDM ’10, pp. 31–40, ACM

[4] Michael Bendersky, Donald Metzler, and W. Bruce Croft, “Parameterized concept weighting in verbose queries,” in Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, New York, NY, USA, 2011, SIGIR ’11, pp. 605–614, ACM.

[5] Yoshua Bengio, Ian J. Goodfellow, and Aaron Courville, “Deep learning,” Book in preparation for MIT Press, 2015.

[6] Jason Weston, Sumit Chopra, and Keith Adams, “#tagspace: Semantic embeddings from hashtags,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014, pp. 1822–1827, Association for Computational Linguistics.

[7] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen, “Convolutional neural network architectures for matching natural language sentences,” in Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, Eds., pp. 2042–2050. Curran Associates, Inc., 2014.

[8] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu, “Hogwild: A lock-free approach to parallelizing stochastic gradient descent,” in Advances in Neural Information Processing Systems 24, J. ShaweTaylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, Eds., pp. 693–701. Curran Associates, Inc., 2011.

[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” in Intelligent Signal Processing. 2001, pp. 306– 351, IEEE Press.

[10] Holger Schwenk, “Continuous space language models,” Comput. Speech Lang., vol. 21, no. 3, pp. 492–518, July 2007.

[11] Geoffrey Hinton, Li Deng, Dong Yu, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath George Dahl, and Brian Kingsbury, “Deep neural networks for acoustic modeling in speech recognition,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, November 2012.

[12] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton, “Speech recognition with deep recurrent neural networks,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, 2013, pp. 6645– 6649.

[13] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul, “Fast and robust neural network joint models for statistical machine translation,” in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, Maryland, June 2014, pp. 1370–1380, Association for Computational Linguistics.

[14] Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney, “Translation modeling with bidirectional recurrent neural networks,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, October 2014, pp. 14–25, Association for Computational Linguistics.

[15] Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, and Li Deng, “Modeling interestingness with deep neural networks,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, October 2014, pp. 2–13, Association for Computational Linguistics. 

[16] Zhengdong Lu and Hang Li, “A deep architecture for matching short texts,” in Advances in Neural Information Processing Systems 26, C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, Eds., pp. 1367–1375. Curran Associates, Inc., 2013.

[17] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom, “A convolutional neural network for modelling sentences,” in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2014, pp. 655–665, Association for Computational Linguistics.

[18] Yoon Kim, “Convolutional neural networks for sentence classification,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, October 2014, pp. 1746–1751, Association for Computational Linguistics.

[19] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao, “Relation classification via convolutional deep neural network,” in Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, Dublin, Ireland, August 2014, pp. 2335–2344, Dublin City University and Association for Computational Linguistics.

[20] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning, “Parsing natural scenes and natural language with recursive neural networks,” in Proceedings of the 26th International Conference on Machine Learning (ICML), 2011.

[21] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning, “Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection,” in Advances in Neural Information Processing Systems 24. 2011.

[22] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III, “A neural network for factoid question answering over paragraphs,” in Empirical Methods in Natural Language Processing, 2014.

[23] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman, “Deep Learning for Answer Sentence Selection,” in NIPS Deep Learning Workshop, Dec. 2014.

[24] Wenpeng Yin and Hinrich Sch¨utze, “Convolutional neural network for paraphrase identification,” in Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, May–June 2015, pp. 901–911, Association for Computational Linguistics.


---
**参考**：
1. 论文：Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, Bowen Zhou. [Applying Deep Learning to Answer Selection: A Study and An Open Task](https://arxiv.org/abs/1508.01585)（2015）
