

pytorch

---
x += y
是原地操作，等价于
x[:] = x + y

---
大小为1的张量，可直接转换为 float,int
a = torch.tensor([3.5])
a,a.item(),float(a),int(a)

---
矩阵按元素相乘叫哈达玛积，数学符号：一个圆圈，里面一个点
---
矩阵乘以向量 torch.mv(A,x)
矩阵乘以矩阵 torch.mm(A,B)
L2范数 torch.norm(u)
L1范数 torch.abs(u).sum()
矩阵的弗罗贝尼乌斯范数（Frobenius norm）：矩阵元素平方和的平方根，可以理解为把矩阵拉成向量后求范数。 torch.norm()
---
亚导数：将导数扩展到不可微的函数，在不可微的地方，取值可以是左导数和右导数之间的任一数字
---
学习率不能太小，也不能太大
批量大小不能太小，也不能太大。太小，不适合并行最大利用资源，太大，内存消耗增加，浪费计算
---
batchsize 越小，其实越好，因为随机梯度带来了噪音，越小则梯度方向越偏

---
L2损失，距离真实值越近，梯度越小，越远，梯度就越大
L1损失，距离真实值的远近，不影响梯度大小，都是+1或-1，比较稳定，但在0点处不可导，优化末期变得不稳定
Huber Loss,结合了L1,L2, 距离真实值远，用L1,距离真实值近，用L2。优化末期更稳定
---
与其说是关系类别不平衡，不如说应该关心是否存在一些类，这些类没有足够的样本
---
测试数据集只能用一次，结果是什么就是什么，不能再用第二次。
---
数据集太小，可以用K折交叉验证
---
估计模型容量
1. 不同种类的算法之间难以比较
2. 同一种类的模型，有两个主要因素
    1. 参数个数
    2. 参数值的选择范围
---
数据复杂度
* 多个重要因素
    * 样本个数
    * 每个样本的元素个数
    * 时间、空间结构
    * 多样性
---
使用有噪声的数据等价于 Tikhonov 正则。（不是固定噪音，而是随机噪音）
丢弃法：在层之间加入噪音

正则项只在训练中使用，影响权重的更新，推理时是不使用的

---
对GPU来说，选元素为0，和直接乘以一个mask，用mask更快
---
让数值变得更加稳定
1. 目标：让梯度值在合理的范围内，如[1e-6,1e3]
2. 将乘法变加法：如ResNet,LSTM
3. 归一化：如梯度归一化，梯度裁剪
4. 合理的权重初始和激活函数
---
整个深度学习的进展都是为了让数值更稳定
---
好的数据清洗带来的价值，往往比模型调参价值大很多
---
卷积
* 平移不变性
* 局部性


---
todo list

系统地学一下 numpy 