1. 为什么需要对数值类型的特征做归一化

    答：为了消除量纲影响，统一标准，统一区间

2. 特征归一化的方法

    1. 线性函数归一化（min-max scaling)
    2. 零均值归一化（Z-ScoreNormalization)

3. 特征归一化后的好处
    1. 消除量纲影响
    2. 同样学习速率能加快收敛速度（窃以为大范围的数据归一化后才有这效果）

4. 所有模型都需要特征归一化吗？

    需要梯度下降求解的模型通常需要归一化，包括线性回归，逻辑回归，支持向量机，神经网络等。决策树模型不适用，因为决策树进行节点分裂时用的信息增益比与特征是否归一化无关

5. 类别性特征（Categorical Feature）如何转化为数值型特征

    1. 序号编码（Ordinal Encoding)，针对有大小关系类别。比如低中高三档成绩可以分别编码为1，2，3。保留大小关系
    2. 独热编码（One-hot Encoding），针对不具有大小关系的类别。比如4种血型。
    3. 二进制编码（Binary Encoding)，其实就是序号编码转化为二进制表示。比如四种血型用001，010，011，100表示。即得到01特征向量，又比独热编码节省空间

6. 独热编码中特征类别太多怎么办？
    1. 用稀疏向量表示以节省空间
    2. 配合特征选择来降低维度
7. 维度太多有会带来哪些问题？
    1. 计算量大。比如K近邻算法中，高维空间下两点距离很难有效衡量
    2. 参数过多，容易过拟合。比如逻辑回归
    3. 通常只有部分维度对分类、预测有帮助

8. 还有哪些编码方法？

    Helmert Contrast、 Sum Contrast、 Polynomial Contrast、 Backward Difference Contrast

9. 文本表示模型都有哪些？
    1. 词袋模型（Bag of Words）
    2. TF-IDF（Term Frequency-Inverse Document Frequency）
    3. 主题模型（Topic Model）
    4. 词嵌入模型（Word Embeddings）

10. 什么是Word2Vec?

    Word2Vec是谷歌2013年提出的词嵌入模型之一，实际上是一种浅层的神经网络模型，它有两种网络结构，分别是 CBOW（Continues Bag of Words)和 Skip-gram

11. CBOW的原理

    输入词的上下文，预测输出词
    w(t-2),w(t-1),w(t+1),w(t+2)独热编码，经过一个N*K的矩阵变换为K维，然后求和，再经过一个K*N的矩阵变换为N维度，softmax后得到预测词汇的最大概率。由于softmax激活需要遍历所有单词，计算缓慢，所以产生了两种改进方法Hierarchical Softmax 和 Negative Sampling

12. 图像分类中数据量不足怎么办？

    a. 数据量不足带来的问题主要是过拟合。解决过拟合的方法有：
    1. 基于模型的
        1. 简化模型（非线性模型简化为线性模型）
        2. 添加约束项缩小假设空间（L1,L2正则）
        3. 继承学习
        4. Dropout
    2. 基于数据的数据扩充
        1. 旋转，平移，缩放，裁剪，填充，左右翻转（改变视角）
        2. 对像素添加噪声扰动，如椒盐噪声、高斯白噪声
        3. 颜色变换
        4. 改变亮度、清晰度、对比度、锐度等

    b. 先对图像进行特征提取，然后再图像的特征空间内进行变换，利用一些通用的数据扩充和上采样技术

    c. 利用已有训练好的通用模型微调

13. 准确率（Accuracy）的局限性

    当不同类别的样本比例非常不均衡时，虽然模型整体分类准确率高，但不代表对需要的类别的分类准确率也高。可以使用更为有效的平均准确率（每个类别的准确率的算术平均）

14. 如何综合评估一个排序模型的好坏？
    
    不仅要看模型在不同Top N 下的 precision@N 和 Recall@N 而且最好绘制出模型的P-R(Precision-Recall)曲线。
    P-R曲线横轴召回率，纵轴精确率。
    只用某个点对应的精确率和召回率是不能全面衡量模型的性能的，只有通过P-R曲线的整体表现才能够对模型进行更为全面的评估

    此外，F1 score和 ROC 曲线也能综合反映一个排序模型的性能。

15. 均方根误差有什么意外？

    模型可能在95%的区间上误差都很小，但RMSE却很高。因为个别偏差程度非常大的离群点，即使数量很少，也会让RMSE指标变得很差。

    解决方案：
    1. 如果离群点是噪声，数据预处理时过滤掉
    2. 如果离群点不是噪声，需要进一步提高模型预测能力，将离群点产生的机制建模进去
    3. 找一个更合适的指标评估模型，比如鲁棒性更好的平均绝对百分比误差 MAPE(Mean Absolute Percent Error)，MAPE = sum((yi-yi_)/yi)/n

16. 什么是ROC曲线

    ROC( receiver Operating Characterisitic Curve),受试者工作特征曲线。是评估二值分类器最重要的指标之一。横坐标是FPR(False Positive Rate)，纵坐标是TPR(True Positive Rate).
    FPR = FP/N, TPR = TP/P。 （P:真实的正样本数，N:真实的负样本数，TP:P个正样本中被分类预测为正的个数，FP是N个负样本中被分类预测为正的个数）

17. 什么是AUC

    Aera Under Curve, 指ROC曲线下的面积大小。该值能够量化地反应基于ROC曲线衡量出的模型性能。
    因为ROC一般在y=x上方，所以AUC一般在0.5-1之间。


18. ROC曲线和P-R曲线的特点

    正负样本分布发生变化时，ROC形状基本保持不变，P-R曲线形状一般会发生剧烈变化。ROC曲线能够降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。所以ROC曲线适合场景更多。但注意，选择哪种曲线因实际问题而异，如果研究者希望更多地看模型在特定数据集上的表现，P-R曲线则能够更直观的反应其性能

19. 余弦距离

    余弦相似度取值范围[-1,1]，1减去余弦相似度即为余弦距离。取值范围为[0,2]

20. 为什么有些场景中使用余弦相似度而不是欧式距离？

    余弦关注的是向量角度关系，而不关心他们的绝对大小。一对文本长度差距很大，但内容相近，用词频或词向量做特征，欧氏距离会很大，但余弦相似度可能很小。余弦相似度在高纬时依然保持“相同为1，正交为0，相反为-1”的性质，欧式距离数值受维度影响，范围不固定，且含义比较模糊

    欧式距离体现数值上的绝对差异，余弦距离体现方向上的相对差异

21. 余弦距离是否是一个严格定义的距离

    不是，距离需要满足正定性，对称性，三角不等式。余弦距离不满足三角不等式，

    反例，A(1,0),B(1,1),C(0,1),
    dist(A,B) = 1-sqrt(2)/2
    dist(B,C) = 1-sqrt(2)/2
    dist(A,C) = 1
    dist(A,B)+dist(B,C) = 2-sqrt(2) < 1=dist(A,C)

    KL距离也不是真正的距离，不满足对称和三角不等式

22. 在对模型进行过充分的离线评估后，为什么还要进行在线A/B测试？

    1. 离线评估无法完全消除模型过拟合影响，不能替代线上评估结果
    2. 离线评估无法完全还原线上工程环境
    3. 线上系统的某些商业指标在离线评估中无法计算，如推荐系统带来的点击率，留存时长等

23. 如何进行线上A/B测试？

    主要手段是进行用户分桶，将用户分成实验组和对照组，实验组用新模型，对照组用旧模型。分桶时要注意样本独立性和采样无偏性，确保同一个用户每次只能分到同一个桶中。


24. 模型评估的验证方法有哪些？

    1. Holdout检验：随机划分训练集和验证集。缺点：在验证集上计算出的最后评估指标与原始分组有很大关系。为了消除随机性，引入“交叉验证”思想
    2. 交叉验证：将全部样本划分成K个大小相等的样本子集，遍历K个子集，每次当前子集作为验证集，其余为训练集，进行训练和评估。把K次评估指标均值作为最终评估指标，一般K经常取10.
    留一验证：样本总数较多时开销大
    留p验证：n个元素选p个有C(n,p)种可能，开销更大，很少用
    3. 自助法：基于自助采样法的检验方法，用于样本规模较小时。总数n的样本，有放回采样n次，得到大小也为n的训练集，没取到的样本作为验证集。

25. 自助法采样过程中，对n个样本进行n次自助抽样，当n区域无穷大时，最终有多少数据未被选择过？

    未被抽到的概率（1-1/n),n次未被抽中概率（1-1/n)^n。 n趋于无穷时，结果为1/e 越等于 0.368

26. 超参数有哪些调优方法？

    1. 网格搜索

        最简单，应用最广泛。如果采用较大搜索范围，较小步长，有大概率找到全局最优值，但十分消耗计算资源和时间，特别是需要调优的超参数较多时。一般先大范围大步长寻找可能的位置，再缩小范围和步长找更精确的最优值。但由于目标函数一般是非凸的，可能错过全局最优

    2. 随机搜索

        在搜索范围随机选点，如果样本点集足够大，随机采样也大概率会找到全局最优值或近似值。一般比网格搜索快，但和快速版的网格搜索一样，无法保证结果

    3. 贝叶斯优化

        思路是利用了前一个点的信息。方法是根据先验分布，假设一个搜集函数，用心采样点测试目标函数，利用这个信息更新目标函数的先验分布，最后，算法测试由后延分布给出的全局最值最可能出现的位置的点。但一旦找到局部最优，会在该区域不断采用，陷入局部最优。为了弥补缺陷，会在探索和利用之间找一个平衡点。“探索”就是在还未采样的区域获取采样点，“利用”则是根据后验分布在最可能出现全局最值的全区域进行采样

    google使用的一套超参数调优算法：Google vizier

27. 降低“过拟合”的方法

    1. 获得更多的训练数据，更多样本能够让模型学习更多有效特征，减少噪声影响。增加数据困难，可以用一定规则扩充训练数据，比如图像中平移，旋转，缩放。更近一步地，用生成式对抗网络合成大量的新训练数据
    2. 降低模型复杂度，如神经网络中减少层数，神经元个数，决策树中减低数深度，进行剪枝等
    3. 正则化方法
    4. 集成学习方法，多模型集成在一起，降低单一模型过拟合风险
    (我认为还有Dropout也算)

28. 降低“欠拟合”的方法
    1. 添加新特征，挖掘上下文特征，ID类特征，组合特征等新特征
    2. 增加模型复杂度
    3. 减小正则化系数 

29. 线性可分的两组点，在SVM分类超平面上的投影是线性不可分的

30. 线性回归与逻辑回归的异同
    相同点：
    逻辑回归可以看做对于“y=1|x"这一事件的对数几率的线性回归，即log(p/(1-p)) = θ.T*x。
    都是用极大似然估计对训练样本建模，线性回归的最小二乘法实际是y服从正态分布假设下，极大似然估计的一个化简。逻辑回归是对数似然函数
    二者都用梯度下降法求解

    不同点：
    逻辑回归可以看做是广义线性模型在因变量y服从二元分布时的一个特殊情况，最小二乘法求解线性回归时，因变量y服从正态分布

31. PCA与LDA降维的区别

    PCA是无监督的，寻找样本点投影后方差最大的维度。
    LDA则是有监督的，类间距离尽可能大，类内方差尽可能小

    音频中提取人的语音信号，过滤掉固定频率背景噪音，用PCA.如果是为了每个人的语音信号具有区分性则需要用LDA

32. 非监督学习包含两大类学习方法：数据聚类和特征变量关联

33. K均值算法的优缺点是什么？

    缺点：受初值和离群点影响每次结果不稳定，结果是通常是局部最优，无法很好地解决数据簇分布差别较大的情况（比如一类是另一类样本数量的100倍）不太适用于离散分类
    1. 需要人工确定初始K值
    2. 只能收敛到局部最优，受初始值影响很大
    3. 易受噪点影响
    4. 样本点只能被划分到单一类中

    优点：高效

    调优方法：
    1. 数据归一化和离群点处理。均值方差大的维度对将结果有决定性影响，离群点对均值产生大的影响，导致中心偏移，所以必须预处理
    2. 合理选择K值，尝试不同的K,画出损失折线，在拐点处的K最佳（手肘法），Gap Statistic 法
    3. 采用核函数

34. 针对K均值缺点的改进方法：

    1. k-means++,选择相互离得远的点为初值点
    2. 高纬度海量数据集，用 ISODATA（迭代自组织数据分析法）算法确定K值。比K均值增加了2个操作，分裂操作和合并操作，分别增加和减少聚类中心数

35. 高斯混合模型（GMM）用EM算法求，极大似然很复杂且非凸

36. 自组织映射神经网络