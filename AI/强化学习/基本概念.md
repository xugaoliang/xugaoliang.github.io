# 基本概念

* 智能体（Agent）
* 环境（Environment）
* 状态（tate）
* 观察（Observation）
* 动作（Action）
* 策略（Policy）
  * $\pi(a|s)$ 概率密度函数
* 奖励（Reward）
  * $R_t,R_{t+1},\cdots$
* 状态转移（state transition）
  * $p(s'|s,a) = \mathbb{P}(S'=s|S=s,A=a)$
* 轨迹（trajectory）
  * $S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\cdots$
* 回报（Return）：未来奖励的累计
  * $G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \cdots $
* 折扣回报（Discounted Return）
  * $G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma ^ 2R_{t+3} + \cdots $
* 状态值函数（State-Value Function）
  * $v_\pi(s) \doteq \mathbb{E}_\pi [G_t|S_t=s]$
* 最优状态值函数（Optimal State-Value Function）
  * $v_*(s)  \doteq \argmax_{\pi} v_\pi (s)$
* 动作值函数（Action-Value Function）
  * $q_\pi(s,a) \doteq \mathbb{E}_\pi [G_t|S_t=s,A_t=a]$
* 最优动作值函数（Optimal Action-Value Function）
  * $q_*(s,a) \doteq \argmax_\pi q_\pi (s,a)$

## 问题

* 强化学习中随机性来源有2种：
  * actions 具有随机性
  * state transitions 具有随机性
