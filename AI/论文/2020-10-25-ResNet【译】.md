---
title: "ResNet【译】"
date: 2020-10-25 20:00:00 +0800
categories: [AI,paper]
tags: [论文,图像,深度学习] 
math: true
---

论文：Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf). arXiv:1512.03385v1, 2015

？？23，9，37，13
？18，30
？42，43
？21，41

## 摘要

更深层次的神经网络更难训练。我们提出了一种残差学习框架，以减轻网络的训练，这些网络实质上比以前使用的更深。我们明确地将这些层重新表述为**参照层的输入学习残差函数**，而不是学习没有参照的函数。我们提供的综合经验证据表明，这些残差网络更容易优化，并能从显著增加的深度获得精度。在 ImageNet 数据集上，我们评估了深度高达152层的残差网络——比VGG网络[41]深8倍，但仍然具有较低的复杂性。这些残差网络的集合在 ImageNet 测试集上的误差达到3.57%，该结果在 ILSVRC 2015 分类任务中获得了第一名。我们还对 CIFAR-10 进行了 100 层和 1000 层的分析。

representations 的深度是许多视觉识别任务的 central importance。仅仅由于我们的超深 representations，我们在 COCO 目标检测数据集获得了 28% 的相对改进。深度残差网络是我们提交给 ILSVRC 和 COCO 2015 竞赛的基础，在本次竞赛中，我们在 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割的任务中获得了第一名。

## 1.介绍

深度卷积神经网络[22，21]为图像分类带来了一系列突破[21,50,40]。深度网络自然地以端到端的多层方式将低/中/高层特征[50]和分类器集成在一起，并且特征的“层次”可以通过堆叠层的数量(深度)来丰富。最近的证据[41,44]表明，网络深度至关重要，在具有挑战性的 ImageNet 数据集[36]上的领先结果[41,44,13,16]都利用了“非常深”的[41]模型，深度从16[41]到30[16]。许多其他 non-trivial 的视觉识别任务[8,12,7,32,27]也从非常深的模型中受益匪浅。

在深度重要性的驱动下，一个问题出现了:学习更好的网络就像叠加更多层次一样容易吗?回答这个问题的一个障碍是臭名昭著的梯度消失/爆炸问题[1,9]，它从一开始就阻碍了收敛。然而，这个问题已经通过归一化的初始化[23,9,37,13]和中间归一化层[16]得到了很大程度的解决，这使得具有数十层的网络在具有反向传播[22]的随机梯度下降(SGD)下开始收敛。

当更深层次的网络能够开始收敛时，一个退化（degradation）问题就暴露出来了:随着网络深度的增加，accuracy 会达到饱和(这可能并不令人惊讶)，然后迅速退化。出乎意料的是，这种退化并不是由过拟合引起的，在适当深度的模型上增加更多的层会导致更高的训练误差，这在[11,42]中有报道，我们的实验也充分验证了这一点。图1给出了一个典型示例。

![图1](/assets/img/ai/resnet/fig1.png)

训练 accuracy 的降低表明并不是所有的系统都同样容易优化。让我们考虑一个较浅的体系结构，以及在其上添加更多层的较深的结构。通过构造更深层次的模型，有一个解决方案:添加的层是恒等映射，其他层从学习的浅层次模型复制。这个构造的解的存在表明一个更深的模型应该不会产生比它的浅层结构更高的训练误差。但实验表明，我们现有的求解器无法找到与构造的解决方案同等好的或更好的解决方案(或者无法在可行时间内做到这一点)。

在本文中，我们通过引入一个深度残差学习框架来解决退化问题。我们不是希望每几个堆叠层直接匹配一个所需的底层映射，而是显式地让这些层匹配一个残差映射。形式上，将期望的基础映射表示为$\mathcal{H}(\mathbf{x})$，我们让堆叠的非线性层拟合另一个映射$\mathcal{F}(\mathbf{x}):= \mathcal{H}(\mathbf{x})−\mathbf{x}$。原始映射被重新修改为$\mathcal{F}(\mathbf{x})+\mathbf{x}$。我们假设优化残差映射比优化原始的、无参照的映射更容易。在极端情况下，如果一个恒等映射是最优的，那么将残差推到零比用一堆非线性层来拟合恒等映射要容易得多。

![图2](/assets/img/ai/resnet/fig2.png)

公式$\mathcal{F}(\mathbf{x})+\mathbf{x}$可以通过带“shortcut connections”的前馈神经网络实现(图2)。Shortcut connections[2,34,49]是那些跳过一个或多个层的连接。在我们的例子中，Shortcut connections 简单地执行恒等映射，并且它们的输出被添加到堆叠层的输出中(图2)。恒等 shortcut 连接既不添加额外的参数，也不增加计算复杂度。整个网络仍然可以通过带有反向传播的 SGD 进行端到端的训练，并且可以使用公共库(例如 Caffe[19])轻松实现，而无需修改求解器。

我们在ImageNet[36]上进行了全面的实验来显示退化问题并评估我们的方法。我们的结果表明:1)我们的极深残差网络很容易优化，但是对应的“plain”网(仅仅是堆叠层)在深度增加时表现出更高的训练误差;2)我们的深度残差网络可以很容易地从深度的大幅增加中获得精度的提高，产生的结果比以前的网络要好得多。

在CIFAR-10集合[20]上也显示了类似的现象，这表明优化的困难和我们方法的效果并不只是针对于一个特定的数据集。我们在这个数据集上用超过100层展示了成功训练的模型，并探索了超过1000层的模型。

在ImageNet分类数据集[36]上，利用极深残差网路得到了很好的结果。我们的152层残差网络是ImageNet所呈现的最深的网络，同时仍然比VGG网络[41]复杂度低。
我们在ImageNet测试集上的总体效果top-5的误差为 3.57%，在ILSVRC 2015分类竞赛中获得了第一名。极深的表征在其他识别任务上也有很好的泛化性能，并带领我们在ILSVRC & COCO 2015年的比赛中，在ImageNet检测、ImageNet定位、COCO检测、COCO分割中进一步获得第一名。这有力的证据表明，残差学习原则是通用的，我们期望它适用于其他视觉和非视觉问题。

## 2. 相关工作

**残差表征**。在图像识别中，VLAD[18]是根据字典对残差向量进行编码的表征，Fisher 向量[30]可以表示为 VLAD 的概率版本[18]。它们都是对图像重新计算和分类的强大的浅层表征[4,48]。对于向量量化，编码残差向量[17]比编码原始向量更有效。

在低级视觉和计算机图形学中，为了求解偏微分方程(PDEs)，广泛应用的 Multigrid 方法[3]将系统重新化为多尺度上的子问题，其中每个子问题对应于较粗尺度和较细尺度上的残差解。Multigrid 的一种替代方法是分层基础预适应[45,46]\(hierarchical basis preconditioning\)，它依赖于表示两尺度间残差向量的变量。已经证明[3,45,46]，这些求解器比不知道解的残差性质的标准求解器收敛得快得多。这些方法表明，一个良好的重构或预处理可以简化优化。

**Shortcut 连接**。导致 shortcut 连接的实践和理论[2,34,49]已经研究了很长时间。训练多层感知器(MLPs)的早期实践是增加一个从网络输入连接到输出的线性层[34,49]。在[44,24]中，一些有趣的层直接连接到辅助分类器，用于考虑梯度消失/爆炸。文献[39,38,31,47]提出了通过 shortcut 连接实现层响应置中(centering layer responses)、梯度和传播误差的方法。在[44]中，“inception”层由一个 shortcut 分支和几个较深的分支组成。

与我们的工作同期的，“highway networks”[42,43]呈现了具有门控功能[15]的 shortcut 连接。这些门是依赖于数据的，并且有参数，与我们的无参数恒等shortcut相反。当门控的 shortcut “关闭”(接近于零)时，highway networks 的层代表无残差功能。相反，我们的公式总是学习残差函数;我们的恒等 shortcut 从来没有关闭过，所有的信息总是通过，还有额外的残差函数需要学习。此外，highway networks 还没有显示出深度增加(例如，超过100层)会提高精度。

## 3. 深度残差学习

### 3.1. 残差学习

让我们把$\mathcal{H}(\mathbf{x})$看作一个底层映射，来拟合几个堆叠层(不一定是整个网络)，而$\mathbf{x}$表示这些层中的第一层的输入。如果假设多个非线性层可以渐近地近似复杂函数，那么同样可以假设它们可以渐进地近似残差函数，即$\mathcal{H}(\mathbf{x})-\mathbf{x}$(假设输入和输出具有相同的维度)。因此，我们明确地让这些层近似一个残差函数$\mathcal{F}(x):=\mathcal{H}(\mathbf{x})-\mathbf{x}$.而不是期望堆叠层来近似$\mathcal{H}(\mathbf{x})$，原始函数因此变成$\mathcal{F}(\mathbf{x})+\mathbf{x}$。虽然两种形式都应该能够渐近地近似期望的函数(猜测)，但学习的容易程度可能不同。

这种重构的动机是退化问题的反直觉现象(图1，左)。正如我们在介绍中所讨论的，如果添加的层可以被构造成恒等映射，那么一个更深的模型的训练误差应该不会大于它的较浅的对应版本。退化问题表明，求解器在使用多个非线性层逼近恒等映射时会遇到困难。利用残差学习重构，如果恒等映射是最优的，求解器可以简单地将多个非线性层的权值趋近于零来逼近恒等映射。

在实际情况下，恒等映射不太可能是最优的，但我们的重构可能有助于预设（precondition）问题。如果最优函数更接近恒等映射而不是零映射，那么求解器应该更容易找到恒等映射的扰动，而不是学习新的函数。我们通过实验表明(图7)，学习后的残差函数一般具有较小的响应，这表明恒等映射提供了合理的预处理（preconditioning）。

### 3.2. Shortcuts 的恒等映射

我们采用残差学习方法对每几个堆叠层。构建模块如图2所示。形式上，本文考虑的构造块定义为:

$$
\tag{1}
\mathbf{y}=\mathcal{F}(\mathbf{x},\{W_i\})+\mathbf{x}.
$$

这里$\mathbf{x}$和$\mathbf{y}$是所考虑的层的输入和输出向量。函数$\mathcal{F}(\mathbf{x},\{W_i\})$表示待学习的残差映射。在图2中是两个层的例子，$\mathcal{F}=W_2\sigma(W_1\mathbf{x})$，其中$\sigma$表示 ReLU[29],为了简化，省略了偏差。操作$\mathcal{F}+\mathbf{x}$是通过 shortcut 连接和按元素相加来执行的。我们在加法后面执行第二个非线性(即$\sigma(\mathbf{y})$，见图2)。

等式(1)中的 shortcut 连接既不引入参数，也不引入计算复杂度。这不仅在实践中很有吸引力，而且在我们比较 plain 网络和残差网络时也很重要。我们可以公平地对同时具有相同数量的参数、深度、宽度和计算成本(除了可忽略的元素加法之外)的 plain/residual 网络进行比较。

在等式(1)中$\mathbf{x}$和$\mathcal{F}$的维度必须相等。如果不是这样(例如，当改变输入/输出通道时)，我们可以通过 shortcut 连接执行一个线性投影$W_s$来匹配尺寸:

$$
\tag{2}
\mathbf{y} = \mathcal{F}(\mathbf{x},\{W_i\})+W_s\mathbf{x}
$$

我们也可以在等式(1)中使用一个方阵$W_s$。但是我们将通过实验证明，恒等映射对于解决退化问题是足够的，并且是经济的，因此$W_s$只在匹配维度时使用。

残差函数$\mathcal{F}$的形式是灵活的。本文中涉及的函数$\mathcal{F}$有两层或三层(图5)，但也可能有更多层。但如果F只有一层，则等式(1)类似于线性层:$\mathbf{y}=W_1\mathbf{x}+\mathbf{x}$，对此我们没有观察到优点。

我们还注意到，尽管为了简单起见，上面的表示法是关于全连接层的，但它们也适用于卷积层。函数$\mathcal{F}(\mathbf{x},\{W_i\})$可以代表多个卷积层。按元素相加是在两个特征图上进行的，一个通道接着一个通道。

### 3.3. 网络结构

我们测试了各种 plain/residual 网络，发现了一致的现象。为了提供讨论的实例，我们如下描述了ImageNet的两个模型。

![图3](/assets/img/ai/resnet/fig3.png)

**Plain 网络**。我们的 plain 基线(图3，中间)主要是受到VGG网络[41]\(图3，左边\)的理念的启发。卷积层大多有3×3滤波器，并遵循两个简单的设计规则:(i)对于相同的输出特征图 size，层的滤波器数量相同;(ii)当特征图 size 减半时，将滤波器的数量增加一倍，以保持每层的时间复杂度。我们通过步长为2的卷积层直接执行向下采样。网络以全局平均池化层和带 softmax 的 1000-way 全连接层结束。图3(中间)中权重层的总数为34。

值得注意的是，我们的模型比VGG网络[41]有更少的过滤器和更低的复杂性(图3，左)。我们的34层基线有36亿次 FLOPs(乘法和加法)，这只是VGG-19(196亿次FLOPs)的18%。

**Residual 网络**。在上述 plain 网络的基础上，我们插入 shortcut 连接(图3，右)，使网络变成对应的残差版本。恒等 shortcuts (等式(1))可以直接使用,当输入和输出是相同的维度(图3中实线shortcuts)时。当维度增加(图3中虚线 shortcuts),我们考虑两个选择:(A) shortcut 仍然执行恒等映射,用额外的零填充增加的维度。此选项不引入任何额外参数;(B)使用等式(2)中的投影 shortcut 匹配维度(通过1×1卷积完成)。对于这两个选项，当 shortcuts 穿过两种 size 的 feature map 时，它们的步长都为2。

### 3.4. 实现

我们对ImageNet的实现遵循了[21,41]中的实践。为了 scale 增强[41]，图像以其较短的边随机采样[256,480]来调整大小。

从一幅图像或其水平翻转中随机采样224×224裁切，减去每个像素的平均值[21]。使用[21]中的标准颜色增强。我们在每次卷积后激活前，采用批归一化(BN) [16]，按照[16]。我们按[13]初始化权重，并从头开始训练所有的 plain/residual 网络。我们使用SGD，mini-batch size 为256。学习率从0.1开始，当误差稳定时，学习率被除以10，并且模型训练达到$60 \times 10^4$次迭代。我们使用了0.0001的权重衰减和0.9的动量。我们没有使用dropout[14]，遵循[16]中的惯例。

在试验中，为了比较研究，我们采用标准的 10-crop 试验[21]。为了得到最好的结果，我们采用[41,13]中所述的全卷积形式，在多个尺度上对分数进行平均(对图像进行缩放，使较短的边位于{224,256,384,480,640})。

## 4. 实验

### 4.1. ImageNet 分类

我们在 ImageNet 2012 分类数据集[36]上评估了我们的方法，该数据集包含1000个类。对128万张训练图像进行训练，对50k张验证图像进行评估。我们还可以获得由测试服务器报告的100k测试图像的最终结果。我们评估了 top-1 和 top-5 的错误率。

![表1](/assets/img/ai/resnet/tab1.png)

**Plain 网络**。我们首先评估18层和34层的 plain 网络。34层 plain 网络如图3(中)所示。18层的 plain 网络也是类似的形式。有关详细架构，请参见表1。

![图4](/assets/img/ai/resnet/fig4.png)

![表2](/assets/img/ai/resnet/tab2.png)

表2的结果表明，较深的34层 plain 网络验证误差高于较浅的18层 plain 网络验证误差。为了揭示原因，在图4(左)中，我们比较了他们在训练过程中的训练/验证错误。我们观察到了退化问题——尽管18层 plain 网络的解空间是34层 plain 网络解空间的子空间，但在整个训练过程中，34层 plain 网络的训练误差较高。

我们认为这种优化困难不太可能是由于梯度消失造成的。这些网络使用BN[16]进行训练，保证前向传播的信号具有非零方差。我们还验证了反向传播的梯度具有BN的健康的 norms。所以前向信号和反向信号都不会消失。事实上，34层 plain 网仍然能够达到相当的 accuracy (表3)，说明求解器在一定程度上是有效的。我们推测，deep plain 网络可能有指数级的低收敛率，这影响训练误差的减少（`我们用更多的训练迭代(3倍)进行了实验，仍然观察到了退化问题，这表明这个问题不能通过简单地使用更多的迭代来解决。`）。造成这种优化困难的原因将是未来研究的重点。

**Residual 网络**。接下来我们评估18层和34层的残差网络(ResNets)。基线架构与上面的 plain 网络相同，只是在图3(右)中为每对3×3过滤器添加了一个 shortcut 连接。在第一个比较中(表2和图4右)，我们对所有的 shortcut 方式和零填充来增加维度都使用了恒等映射(选项A)。因此，与 plain 相比，它们没有额外的参数。

从表2和图4中我们可以得到三个主要的观察结果。首先，情况与残差学习相反——34层的 ResNet 比18层的 ResNet 好(好2.8%)。更重要的是，34层的ResNet显示出相当低的训练误差，并可推广到验证数据。这表明在此设置中退化问题得到了很好的解决，并且我们设法通过增加深度来获得 accuracy 增益。

其次，与普通的 ResNet 相比，34层 ResNet 将 top-1 误差降低了3.5%(表2)，这是由于成功地降低了训练误差(图4右 vs 左)。通过比较验证了残差学习在极深系统上的有效性。

最后，我们还注意到18层的 plain/residual 网络具有相当的精度(表2)，但是18层的ResNet收敛速度更快(图4左右)。当网络“不是太深”(这里是18层)时，当前的SGD求解器仍然能够找到 plain 网络的好解决方案。在这种情况下，ResNet通过在早期提供更快的收敛速度来简化优化。

![表3](/assets/img/ai/resnet/tab3.png)

**恒等 vs. 投影 Shortcuts**。我们已经证明了无参数、恒等 shortcuts 有助于训练。接下来我们研究投影 shortcuts (等式(2))。在表3中，我们比较了三个选项:(A)零填充 shortcuts 用于增加维度，并且所有 shortcuts 都是无参数的(与表2和图4右相同);(B)投影 shortcuts 用于增加维度，其他 shortcuts 为恒等;(C)所有 shortcuts 都是投影。

表3显示了所有三个选项都比对应的 plain 好得多。B比A稍微好一点，我们认为这是因为A中的零填充维度确实没有残差学习。C比B稍微好一点，我们将此归因于许多(13条)投影 shortcuts 引入的额外参数。但是A/B/C之间的小差异表明投影 shortcuts 对于解决退化问题不是必需的。因此，在本文的其余部分，我们不使用选项C，以减少内存/时间复杂度和模型大小。恒等 shortcuts 对于不增加下面介绍的 bottleneck 架构的复杂性特别重要。

![图5](/assets/img/ai/resnet/fig5.png)

**更深的 Bottleneck 架构**。接下来，我们将描述 ImageNet 的更深层次的网络。考虑到我们所能承受的训练运行时间问题，我们对构建块进行了修改，作为 bottleneck 设计。(`较深的非瓶颈型ResNets(例如，图5左)也可以通过增加深度获得精度(如CIFAR-10所示)，但不如瓶颈型ResNets经济。所以使用瓶颈设计主要是出于实际考虑。我们进一步注意到，在瓶颈设计中也出现了普通网的退化问题。`)对于每一个残差函数$\mathcal{F}$，我们使用3层堆叠而不是2层(图5)。这三层是1×1、3×3和1×1卷积，其中1×1层负责减少和增加(恢复)维度，把较小的输入/输出维度留给3×3层的瓶颈层。图5给出了一个例子，其中两种设计具有相似的时间复杂度。

无参数恒等 shortcuts 对于瓶颈架构尤其重要。如果用投影代替图5(右)中的恒等shortcut，可以看出，由于 shortcut 连接到两个高维端点，时间复杂度和模型大小都增加了一倍。因此，恒等 shortcut 为瓶颈设计带来了更有效的模型。

**50层 ResNet**: 我们将34层网络中的每个2层块替换为这个3层瓶颈块，从而形成一个50层的ResNet(表1)。我们使用选项B来增加维度。这种模式有38亿次 FLOPs。

**101层和152层ResNet**:我们使用更多的3层块来构建101层和152层ResNet(表1)。值得注意的是，虽然深度显著增加，但152层ResNet(113亿 FLOPs)的复杂度仍然低于VGG-16/19网络(153/196 亿次 FLOPs)。

![表4](/assets/img/ai/resnet/tab4.png)

50/101/152层的ResNets比34层的更为精确(表3和4)。我们没有观察到退化问题，因此，从显著增加的深度中获得显著的精度。对所有的评价指标(表3和表4)来说，深度的好处都是显而易见的。

![表5](/assets/img/ai/resnet/tab5.png)

**和 SOTA 方法比较**:在表4中，我们与之前的最佳单模型结果进行了比较。我们的基准34层ResNets已经取得了非常有竞争力的准确性。我们的152层ResNet的单模型 top-5 验证误差为4.49%。这个单模型的结果优于之前所有的集成结果(表5)。我们将6个不同深度的模型组合成一个集成(提交时只有两个152层的模型)。这导致测试集中出现 3.57% 的top-5错误(表5)。该作品获得2015年 ILSVRC 第一名。

### 4.2. CIFAR-10 和 分析

我们对 CIFAR-10 数据集[20]进行了更多的研究，该数据集由10个类中50k的训练图像和10k的测试图像组成。我们展示了在训练集上训练并在测试集上评估的实验。我们关注的是极深网络的行为，而不是推动最先进的结果，所以我们有意使用以下简单的架构。

plain/residual 结构遵循图3(中间/右边)的形式。网络输入为32×32幅图像，减去每像素的均值。第一层为3×3卷积。然后在大小为{32,16,8}的 feature map 上使用一个3×3卷积的6n层堆栈，每个feature map size对应2n层。过滤器的数目分别为{16,32,64}。子采样是由步长为2的卷积构成的。该网络以全局平均池化、10-way 全连接层和softmax 结束。总共有6n+2个堆叠的权重层。下表总结了该架构:

![6n+2](/assets/img/ai/resnet/6n+2.png)

使用 shortcut 连接时，连接到3×3层的 pairs 上(共3n个 shortcuts 连接)。在此数据集中，我们在所有情况下使用恒等 shortcut 方式(即选项A)，因此我们的残差模型与 plain 模型具有完全相同的深度、宽度和参数数量。

我们采用权值衰减为0.0001，动量为0.9，采用[13]的权重初始化和BN[16],但没有 dropout。这些模型在两个gpu上用128个小批量进行训练。我们从0.1学习率开始，在32k和48k迭代时将其除以10，在64k迭代时终止训练，这是在45k/5k训练/验证分割时确定的。我们采用[24]中的简单数据增强进行训练:每边填充4个像素，然后从填充的图像或其水平翻转中随机采样一个32×32的剪裁。在测试时，我们只对原始的32×32图像的单个视图进行评估。

![图6](/assets/img/ai/resnet/fig6.png)

我们比较 n ={3, 5, 7, 9}，得到20,32,44,56层网络。图6(左)显示了 plain 网络的行为。深的plain 网络的深度越深，训练误差越大。这种现象与ImageNet(图4，左)和MNIST([42])类似，说明这种优化困难是一个基本问题。

图6(中间)显示了 ResNets 的行为。同样与ImageNet的情况类似(图4，右)，我们的 ResNets 克服了优化的困难，并且随着深度的增加，精度得到了极大的提高。

![表6](/assets/img/ai/resnet/tab6.png)

我们进一步探索n = 18，得到110层ResNet。在这种情况下，我们发现初始学习率为0.1有点太大，无法开始收敛(`初始学习率为0.1，几个epoch后开始收敛(<90%误差)，仍然达到相似的精度。`)。所以我们使用0.01来预热训练，直到训练误差低于80%(大约400次迭代)，然后回到0.1并继续训练。学习计划的其余部分和前面一样。这个110层的网络收敛得很好(图6，中间)。它的参数比其他深的和瘦的网络(如FitNet[35]和Highway[42])更少(表6)，但仍是最先进的结果之一(6.43%，表6)。

![图7](/assets/img/ai/resnet/fig7.png)

**Layer Responses 分析**。图7为layer responses 的标准差(std)。responses 是每3×3层的输出，在BN之后和其他非线性(ReLU/加法)之前。对于ResNets，这一分析揭示了残差函数的 response 强度。从图7可以看出，ResNets的 responses 通常小于对应的 plain 版本。这些结果支持了我们的基本动机(第3.1节)，即残差函数可能通常比非残差函数更接近于零。我们还注意到，更深的ResNet具有较小的 responses 强度，如图7中 ResNet-20、56和110之间的比较所证明的那样。当有更多的层时，单个的ResNets层倾向于较少地修改信号。

**探索超过1000层**。我们积极地探索超过1000层的深层模型。我们设 n=200，得到一个1202层的网络。我们的方法没有优化难度，这个 $10^3$ 层的网络可以实现训练误差<0.1%(图6，右)。它的测试误差仍然相当好(7.93%，表6)。

但在如此深的模型中，仍存在一些悬而未决的问题。这个2层网络的测试结果比我们110层网络的测试结果差，尽管两者都有相似的训练错误。我们认为这是因为过拟合。对于这个小数据集，1202层网络可能会不必要地大(19.4M)。使用强正则化方法如maxout[10]或dropout[14]在该数据集上获得最佳结果([10,25,24,35])。在本文中，我们没有使用maxout/dropout，只是简单地通过设计对深和瘦架构进行正则化，而没有分散对优化难点的关注。但结合更强的正则化可能会得到更精确的结果，我们将在以后进行研究。

### 4.3. 基于 PASCAL 和 MS COCO 的目标检测

![表7](/assets/img/ai/resnet/tab7.png)

![表8](/assets/img/ai/resnet/tab8.png)

该方法对其它识别任务具有良好的泛化性能。表7和表8显示了在 PASCAL VOC 2007 和 2012[5] 和 COCO[26]上的目标检测基线结果。我们采用 Faster R-CNN[32]作为检测方法。在这里，我们感兴趣的是用 ResNet-101替换 VGG-16[41]的改进。使用两种模型的检测实现(见附录)是相同的，所以只能归因于更好的网络。最引人注目的是，在具有挑战性的 COCO 数据集上，我们发现 COCO 的标准度量值增长了 6.0% (mAP@[.5,.95])。这是一个28%的相对进步。这种增益完全是由于学得的表征。

基于deep residual nets，我们在2015年 ILSVRC 和 COCO 的比赛中获得了ImageNet检测、ImageNet定位、COCO检测、COCO分割等多个项目的第一名。细节在附录中。

## 参考

1. Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen- cies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.
2. C. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995.
3. W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam, 2000.
4. K. Chatfield, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods.
In BMVC, 2011.
5. M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis- serman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, pages 303–338, 2010.
6. S. Gidaris and N. Komodakis. Object detection via a multi-region & semantic segmentation-aware cnn model. In ICCV, 2015.
7. R. Girshick. Fast R-CNN. In ICCV, 2015.
8. R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier- archies for accurate object detection and semantic segmentation. In CVPR, 2014.
9. X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.
10. I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. arXiv:1302.4389, 2013.
11. K. He and J. Sun. Convolutional neural networks at constrained time cost. In CVPR, 2015.
12. K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014.
13. K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.
14. G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.
15. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
16. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.
17. H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. TPAMI, 33, 2011.
18. H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into compact codes.
TPAMI, 2012.
19. Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014.
20. A. Krizhevsky. Learning multiple layers of features from tiny im- ages. Tech Report, 2009.
21. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
22. Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to hand- written zip code recognition. Neural computation, 1989.
23. Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller. Efficient backprop.
In Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.
24. C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu.
Deeply- supervised nets. arXiv:1409.5185, 2014.
25. M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400, 2013.
26. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV. 2014.
27. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
28. G. Montúfar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014.
29. V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.
30. F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR, 2007.
31. T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012.
32. S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.
33. S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015.
34. B. D. Ripley. Pattern recognition and neural networks. Cambridge university press, 1996.
35. A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.
36. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.
37. A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.
arXiv:1312.6120, 2013.
38. N. N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical report, 1998.
39. N. N. Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pages 207–226. Springer, 1998.
40. P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le- Cun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.
41. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
42. R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.
arXiv:1505.00387, 2015.
43. R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. 1507.06228, 2015.
44. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu- tions. In CVPR, 2015.
45. R. Szeliski. Fast surface interpolation using hierarchical basis func- tions. TPAMI, 1990.
46. R. Szeliski. Locally adapted hierarchical basis preconditioning. In SIGGRAPH, 2006.
47. T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas- tic gradient towards second-order methods–backpropagation learn- ing with transformations in nonlinearities. In Neural Information Processing, 2013.
48. A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms, 2008.
49. W. Venables and B. Ripley. Modern applied statistics with s-plus.1999.
50. M. D. Zeiler and R. Fergus. Visualizing and understanding convolu- tional neural networks. In ECCV, 2014.

## A. 目标检测基线

在这一节中，我们介绍了我们基于基线 Faster R-CNN[32]系统的检测方法。模型由 ImageNet 分类模型初始化，然后在目标检测数据上进行微调。在2015年的 ILSVRC 和 COCO 检测比赛中，我们使用了ResNet-50/101。

不像在[32]中使用的 VGG-16，我们的 ResNet 没有隐藏的 fc 层。我们采用“Networks on Conv feature maps”(NoC) [33]来解决这个问题。我们使用那些在图像上的步幅不大于16像素的层(即，conv1, conv2_x, conv3_x，和conv4_x, ResNet-101中共有91个conv层;表1)计算全图像共享卷积特征图;我们认为这些层类似于 VGG-16 中的13个卷积层，通过这样做，ResNet 和 VGG-16 都拥有相同总步幅(16像素)的卷积特征图。这些层由一个区域建议网络(region proposal network,RPN，产生300个 proposals) [32]和一个 Fast R-CNN 检测网络[7]共享。在 conv5_1之前进行 RoI pooling[7]。在这个 RoI 池化特性上，每个区域都采用 conv5_x 和 up 的所有层，起到 VGG-16 的 fc 层的作用。最后的分类层被两个 sibling 层(分类和 box 回归[7])所取代。

对于BN层的使用，在经过预训练后，我们在ImageNet训练集上对每一层的BN统计量(均值和方差)进行统计，然后在微调过程中对BN层进行固定，用于目标检测。因此，BN层变成具有恒定偏移量和伸缩性的线性激活，并且BN统计信息不会通过微调更新。我们固定BN层主要是为了在 Faster R-CNN 训练中减少内存消耗。

**PASCAL VOC**

接下来[7,32]，对于 PASCAL VOC 2007 测试集，我们使用 VOC 2007 中的5k trainval 图像和 VOC 2012中的 16k trainval图像进行训练(“07+12”)。对于 PASCAL VOC 2012 测试集，我们使用 VOC 2007 中的 10k trainval+test 图像和 VOC 2012 中的 16k trainval 图像进行训练(“07++12”)。训练 Faster R-CNN 的超参数与[32]相同。表7显示了结果。ResNet-101 比 VGG-16 改进了 3% 的 mAP。这完全是因为ResNet学习了改进的特性。

**MS COCO**

MS COCO的数据集[26]涉及80个目标类别。我们评估了 PASCAL VOC 度量(mAP@IoU=0.5)和标准 COCO 度量(mAP@IoU=.5:.05:.95)。我们使用训练集上80k的图像进行训练，使用 val 集上40k的图像进行评估。我们的 COCO 检测系统和 PASCAL VOC 的检测系统是相似的。我们用 8-GPU 单一处理器训练 COCO 模型，因此RPN step 有8张图片的 mini-batch size (即每GPU 1张)，而 Fast R-CNN step 有16张图片的 mini-batch size。RPN step 和 Fast R-CNN step 分别训练240k和80k迭代，学习率分别为0.001和0.0001。

表8显示了 MS COCO 验证集的结果。ResNet-101的 mAP@[.5,.95] 增加了6%。相比于 VGG-16，这是一个28%的相对改进，这完全归功于更好的网络学到的特征。值得注意的是，mAP@[.5,.95]的绝对增长(6.0%)几乎和 mAP@.5(6.9%)一样大。这表明更深层次的网络可以同时提高 recognition 和 localization。

## B. 目标检测改进

为了完整起见，我们报告了为竞赛所做的改进。这些改进是基于深度特性的，因此应该从残差学习中受益。

**MS COCO**

![表9](/assets/img/ai/resnet/tab9.png)

**Box _refinement_(细化)**。我们的 box 细化部分遵循了[6]中迭代定位。在 Faster R-CNN 中，最终的输出是一个回归框，它不同于它的提议框。因此，为了进行推理，我们从回归框中 pool 一个新的特征，并获得一个新的分类分数和一个新的回归框。我们把这300个新的预测与原来的300个预测结合起来。使用IoU阈值0.3[8]对预测框的 union set 应用非最大值抑制(Non-maximum suppressio，NMS)，然后 box voting[6]。框细化提高了 mAP 大约2个点(表9)。

**全局上下文**。我们在 Fast R-CNN 步骤中结合全局上下文。给定全图像卷积特征图，we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s bounding box as the RoI。这个 pooled 特征被馈入 post-RoI 层以获得全局上下文特征。这个全局特征与原始的每个区域特征连接在一起，然后是 sibling 分类和 box 回归层。这种新结构是端到端训练的。全局上下文提升 mAP@.5大约1个点(表9)。

**Multi-scale 测试**。上面的结果都是[32]中通过 single-scale 训练/测试得到的，其中图像的短边为 s=600 像素。在[12,7]中通过从特征金字塔中选择一个尺度来开发多尺度训练/测试，在[33]中使用 maxout layers。在我们当前的实现中，我们在[33]之后进行了 multi-scale 测试;由于时间有限，我们没有进行 multi-scale 的训练。此外，我们只对 Fast R-CNN step 进行了 multi-scale 测试(但还没有对 RPN step进行 multi-scale 测试)。通过训练模型，我们计算图像金字塔上的卷积特征映射，其中图像的短边$s\in \{200,400,600,800,1000\}$。

我们从[33]之后的金字塔中选择两个相邻的尺度。在这两种尺度[33]的feature map上进行 RoI pooling 和 subsequent layers，按照[33]用 maxout 合并。Multi-scale 测试将 mAP 提高了超过2个点(表9)。

**使用验证数据**。接下来，我们使用 80k+40k 训练集进行训练，使用20k test-dev 集进行评估。test-dev 集没有公开可用的 ground truth，结果由评估服务器报告。在这个设置下，结果是 mAP@.5 of 55.7%和 mAP@[.5,.95] of 34.9%(表9)。这是我们的单模型结果。

**集成**。在 Faster R-CNN 中，系统被设计用于学习 region proposals 和对象分类，因此一个集成可以用于提高这两个任务。我们使用集成来提出区域，建议的 union set 由每区域分类器的集成来处理。表9显示了我们基于3个网络集成的结果。test-dev集的 mAP 分别为 59.0% 和 37.4%，该结果在 COCO 2015 的检测任务中获得了第一名。

**PASCAL VOC**

![表10](/assets/img/ai/resnet/tab10.png)

![表11](/assets/img/ai/resnet/tab11.png)

我们基于上述模型重新访问 PASCAL VOC 数据集。使用 COCO 数据集上的单一模型(55.7% mAP@.5在表9)中，我们在 PASCAL VOC 集合上对这个模型进行了微调。此外，还采用了框细化、上下文和 multi-scale 测试等改进方法。通过这样做，我们在 PASCAL VOC 2007(表10)和 PASCAL VOC 2012(表11)上分别实现了85.6%和83.8%的 mAP。在 PASCAL VOC 2012 的结果比先前的最先进的结果[6]高 10 点。

**ImageNet检测**

![表12](/assets/img/ai/resnet/tab12.png)

ImageNet检测(DET)任务涉及200个目标类别。accuracy 由 mAP@.5 来评估。我们针对 ImageNet DET 的目标检测算法与表9中针对 MS COCO 的目标检测算法相同。网络在1000个类的ImageNet分类集上进行预训练，并在 DET 数据上进行微调。我们将验证集按照[8]分为两个部分(val1/val2)。我们使用DET训练集和val1集对检测模型进行微调。val2集用于验证。我们没有使用其他ILSVRC 2015数据。使用ResNet-101的单一模型对DET测试集的 mAP 为58.8%，三种模型的集成对DET测试集的 mAP 为62.1%(表12)。该结果在ILSVRC 2015 ImageNet检测任务中获得第一名，以8.5分(绝对)的优势超过第二名。

## C. ImageNet Localization

ImageNet 定位(LOC)任务[36]需要对对象进行分类和定位目标。按照[40,41]，我们假设首先使用图像级分类器来预测图像的类标签，定位算法只考虑基于预测的类来预测边框。我们采用“per-class regression”(PCR)策略[40,41]，为每一类学习一个边界框回归。我们先对网络进行了 ImageNet 分类的预训练，然后对其进行了微调以实现定位。我们在提供的1000类 ImageNet训练集上训练网络。

我们的定位算法是基于[32]的 RPN 框架，并做了一些修改。与[32]中的分类无关的方式不同，我们用于定位的 RPN 是按类的形式设计的。这个RPN以两个sibling 1×1卷积层结束，用于二进制分类(cls)和 box 回归(reg)，如[32]中所示。与[32]相比，cls 和 reg 层都是在每个类中。具体来说，cls层有一个 1000-d 的输出，每个维度都是用于预测是否存在一个对象类的二元logistic回归;reg层有一个1000×4-d的输出，由1000个类的盒 box 回归组成。按照[32]中，我们的边界框回归是参考每个位置上的多个平移不变的“锚定”框。

在 ImageNet 分类训练中(第3.4节)，我们随机抽取 224×224 裁切作为数据增强。我们使用一个 256 张图像的 mini-batch size 进行微调。为避免负样本占主导地位，每幅图像随机采样8个锚点，其中正锚点与负锚点的比例为1:1[32]。为了测试，将网络完全卷积地应用在图像上。

![表13](/assets/img/ai/resnet/tab13.png)

表13比较了定位结果。在[41]之后，我们首先使用 ground truth 类作为分类预测来执行“oracle”测试。VGG的论文[41]报告了使用 ground truth 类的 center-crop 误差为33.1%(表13)。在相同的设置条件下，我们的RPN方法使用ResNet-101网络，显著地将 center-crop 的误差降低到13.3%。这个比较演示了我们的框架的出色性能。使用密集(全卷积)和 multi-scale 测试，我们的 ResNet-101 使用 ground truth 类有11.7%的误差。使用 ResNet-101 预测类(top-5分类错误4.6%，表4)，top-5定位错误为14.4%。

以上结果仅基于 Faster R-CNN[32]中的 proposal network (RPN)。可以使用 Faster R-CNN 中的检测网络(Fast R-CNN[7])来改进结果。但我们注意到，在这个数据集上，一张图像通常包含一个单一的主导对象，并且建议的区域高度重叠，因此具有非常相似的 RoI-pooled 特征。因此，Fast R-CNN[7]以图像为中心的训练产生了小的变异样本，这可能不是随机训练所需要的。基于此，在我们当前的实验中，我们使用原始的 R-CNN[8]，它是 RoI-centric，来代替 Fast R-CNN。

我们的 R-CNN 实现如下。我们在训练图像上应用如上训练的每类RPN来预测ground truth类的边界框。这些预测框扮演了 class-dependent proposals 的角色。对于每一张训练图像，提取得分最高的200个 proposals 作为训练样本，训练一个 R-CNN 分类器。将图像区域从一个 proposal 中裁剪出来，扭曲到224×224像素，按照R-CNN[8]的方式输入分类网络。这个网络的输出由cls和reg的两个 sibling fc层组成，也是按类的形式。这个R-CNN网络在训练集上进行了微调，以 RoI-centric 方式将 mini-batch size 调整为256。为了进行测试，RPN为每个预测类生成得分最高的200个 proposals ，然后使用R-CNN网络来更新这些 proposals 的得分和框位。

![表14](/assets/img/ai/resnet/tab14.png)

该方法将 top-5定位错误减少到10.6%(表13)。这是我们在验证集上的单模型结果。使用用于分类和定位的网络集成，我们在测试集上实现了9.0%的 top-5定位误差。这个数字显著优于 ILSVRC 14 的结果(表14)，显示出误差相对减少了64%。该结果在2015年 ILSVRC ImageNet 定位任务中获得第一名。

