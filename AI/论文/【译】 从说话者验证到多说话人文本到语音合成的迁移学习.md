
# 从说话者验证到多说话人文本到语音合成的迁移学习

* 作者：Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu
* 论文：《Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis》
* 地址：https://arxiv.org/abs/1806.04558

## 摘要

我们描述了一个基于神经网络的文本到语音(text-to-speech,TTS)合成系统，该系统能够在不同的讲话者的声音中生成语音音频，包括那些在训练中看不到的。我们的系统包括三个独立训练组件:(1) speaker encoder network,训练一个 speaker verification（说话者验证）任务，使用来自成千上万的 speaker 的没有合成的嘈杂的独立数据集 ,引用目标 speaker 仅有的几秒的 speech生成固定维度嵌入向量;(2)基于 Tacotron 2的序列到序列合成网络，在 speaker 嵌入的条件下，从文本中生成 mel 谱图;(3)基于 auto-regressive WaveNet 的 vocoder network(语音合成网络) ，将 mel 谱图转换为时域 waveform (波形)样本。实验结果表明，该模型能够将训练有素的 speaker 编码器学习到的 the knowledge of speaker variability (说话者变化性知识)转化为多语音TTS任务，并能够合成训练过程中未被发现的语音。为了获得最佳的泛化性能，我们量化了在大型且多样化的 speaker 集上训练  speaker 编码器的重要性。最后，我们证明了随机采样的 speaker 嵌入可以用于合成不同于训练中使用的说话者声音的语音，这表明该模型获得了高质量的 speaker 表征。

## 1. 介绍

本工作的目标是建立一个TTS系统，该系统能够以数据高效的方式为各种 speakers 生成自然语音。我们特别解决了一个 zero-shot 学习设置，其中来自目标 speaker 的几秒钟未经转录的参考音频被用于合成该 speaker 声音中的新语音，而无需更新任何模型参数。这样的系统具有可访问性应用程序，比如恢复失声用户的自然沟通能力，因此无法提供许多新的训练示例。它们还可以支持新的应用程序，例如跨语言传输语音以实现更自然的语音到语音的转换，或者在低资源设置下从文本生成真实的语音。然而，同样重要的是要注意这项技术可能被滥用，例如未经他人同意就模仿他人的声音。为了解决与[1]等原则相一致的安全问题，我们验证了由提出的模型生成的声音可以很容易地与真实的声音区分开来。

合成自然语音需要在大量高质量的语音文本对上进行训练，并且支持许多演讲者，通常使用每个演讲者[8]数十分钟的训练数据。对许多 speaker 记录大量高质量的数据是不切实际的。我们的方法是通过独立训练一个识别 speaker 的嵌入网络来将 speaker 建模从语音合成中分离出来，该网络捕获 speaker 特征的空间，并根据第一个网络学习到的表征在一个较小的数据集上训练一个高质量的TTS模型。解耦网络使它们能够在独立数据上进行训练，从而减少了获取高质量 multispeaker 训练数据的需求。我们训练嵌入网络的 speaker verification 任务，以确定两个不同的话语是否出自同一说话者。与随后的TTS模型相比，该网络是针对来自大量speakers的包含回响和背景噪声的未转录语音进行训练的。

我们证明了该编码器和合成网络可以在不平衡和不相交的 speakers 集上进行训练，并且具有良好的泛化能力。我们在1.2K 个 speaker 上训练合成网络，并证明在更大的一组18K speaker上训练的编码器可以提高适应质量，并通过从嵌入前的采样进一步使合成完全新颖的 speaker 成为可能。

人们对TTS模型的端到端训练非常感兴趣，这种训练直接从文本-音频对开始，不需要依赖手工制作的中间表征[17,23]。Tacotron 2[15]采用 WaveNet [19]作为 vocoder (语音编码器，语音合成仪，声码器)，将注意力[3]的编解码器结构产生的声谱图进行反向转换，将 Tacotron 的[23]韵律与 WaveNet 的音质结合，获得接近人类语音的自然度。它只支持一个speaker。

Gibiansky等人[8]引入了Tacotron的 multispeaker 变体，该变体为每个训练 speaker 学到了 低维 speaker 嵌入。Deep Voice 3[13]提出了一个全卷积的编码器-解码器架构，该架构可支持LibriSpeech[12]上的2400多个 speaker 。

这些系统学习一组固定的 speaker 嵌入，因此只支持在训练期间看到的声音合成。相比之下，VoiceLoop[18]提出了一种基于固定大小的内存缓冲区的新架构，它可以从训练过程中看不到的声音生成语音。要想取得好成绩，需要新 speaker 进行数十分钟的 enrollment (登记，入伍)演讲和 transcripts（文字记录）。

最近的扩展已经启用了 few-shot speaker 适配，其中每个 speaker 只有几秒钟的讲话(没有文本)可以用来生成新的 speaker 的声音。[2]扩展了Deep Voice 3，将类似于[18]的 speaker 适配方法(其中模型参数(包括speaker嵌入)在少量的适配数据上进行微调)与 speaker 编码方法(使用神经网络直接从声谱图预测speaker嵌入)进行了比较。后一种方法具有更高的数据效率，只需一两句话，就可以使用少量的适应数据获得更高的自然度。它的计算效率也显著提高了，因为它不需要数百次的反向传播迭代。

Nachmani等人对VoiceLoop进行了类似的扩展，利用目标 speaker 编码网络来预测 speaker 的嵌入。该网络与合成网络联合训练，使用 contrastive triplet loss(对比三元损耗)，以确保来自同一speaker 的话语预测的嵌入比来自不同发言者的计算的嵌入更接近。此外，我们使用了一个周期一致性损失来确保合成的语音编码到类似于自适应话语的嵌入。

一个类似的谱图编码器网络，经过训练（无三元损失），被证明是能转移目标 prosody (韵律)到 synthesized speech (综合语音)[16]。在这篇论文中，我们证明了训练一个相似的编码器来区分不同的说话者，会导致说话者特性的可靠传递。我们的工作是最类似于 speaker 编码模型[2,10],我们除了利用网络在成千上万speakers的无文本音频的大型数据集上独立训练 speaker verification 任务,使用最先进的广义的端到端损失[22]。[10]在他们的模型中加入了一个类似的说话者识别表征，但是所有的组件都是联合训练的。与此相反，我们探讨了从一个预训练的 speaker verification 模型的迁移学习。

Doddipatla等人的[7]使用了类似的迁移学习配置，其中使用预训练的speaker分类计算出的speaker嵌入来调整TTS系统。本文利用不依赖于中间语言特征的端到端综合网络，以及不局限于封闭的speakers集的完全不同的 speaker 嵌入网络。此外，我们还分析了 speakers 的数量对 speakers 质量的影响，发现 zero-shot 传输需要对数千个 speakers 进行训练，比[7]使用的 speaker 要多得多。

## 2. Multispeaker speech synthesis model (多说话人语音合成模型)

我们的系统是由三个独立训练神经网络组成,如图1所示:(1)递归 speaker encoder,基于[22],它从语音信号计算固定维向量,(2)sequence-to-sequence合成器,基于[15],从字母和音素序列作为输入预测了梅尔谱图,以 speaker 嵌入向量为条件,和(3)一个自回归WaveNet[19] vocoder,将谱图转换为时域波形。

### 2.1 Speaker encoder

The speaker encoder is used to condition（决定，以...为条件） the synthesis network(合成网络) on a reference speech signal（参考语音信号） from the desired（渴望的，想要的） target speaker。对于良好的泛化来说，至关重要的是使用一种能够捕捉不同说话人特征的表示法，以及仅使用一种短暂的自适应信号来识别这些特征的能力，这种自适应信号与语音内容和背景噪声无关。使用针对文本无关的speaker verification 任务训练的 speaker-discriminative 模型来满足这些需求。

我们遵循了[22]，它提出了一个高度可扩展和准确的神经网络框架用于 speaker verification。该网络将从任意长度的语音计算得到的一系列log-mel谱图帧 映射到一个固定维的嵌入向量，称为$d-vector$[20,9]。对网络进行训练，优化广义端到端 speaker verification loss，使得来自同一说话人的话语嵌入具有较高的余弦相似度，而来自不同说话人的话语嵌入在嵌入空间中相距较远。训练数据集包括分割为1.6秒的语音音频示例和相关的说话人身份标签;没有使用转录文本。

输入40通道的log-mel谱图被传递到一个由3层LSTM层768个单元组成的网络，每个LSTM层后面是256维的投影。最后的嵌入是对最后一帧顶层的输出进行$L_2$-normalizing 创建的。在推理过程中，任意长度的话语被分割成800ms窗口，重叠50%。该网络在每个窗口上独立运行，并对输出进行平均和标准化以创建最终的话语嵌入。

虽然网络没有被直接优化来学习一个捕获与合成相关的speaker特征的表示法，但是我们发现，针对一个 speaker discrimination (识别)任务的训练导致了一个嵌入，它直接适用于将合成网络限定在speaker 身份上。

## 2.2 Synthesizer （合成器）

我们使用 attention Tacotron 2架构[15]来扩展递归序列到序列，以支持多个speakers，支持类似于[8]的方案。目标 speaker 的嵌入向量在每个时间步与 synthesizer encoder 输出连接。与[8]不同的是，我们发现简单地将嵌入传递到注意层(如图1所示)，可以在不同的 speaker 之间聚合。

我们比较了该模型的两种变体，一种使用 speaker encoder 计算嵌入，另一种基线优化训练集中每个 speaker 的固定嵌入，本质上是学习一个类似于[8,13]的 speaker 嵌入查找表。

合成器是在文本和目标语音对上训练的。在输入时，我们将文本映射到一系列音素，从而加快了单词和专有名词的收敛速度，改善了它们的发音。该网络采用迁移学习配置进行训练，使用预训练好的speaker编码器(其参数被冻结)从目标音频中提取嵌入的speaker，即训练时speaker参考信号与目标语音相同。在训练期间不使用明确的speaker 身份标签。

目标声谱特征是从50ms窗口计算得到的，计算步骤为12.5ms，通过一个80通道的mel-scale filterbank，然后进行 log 动态范围压缩。我们通过增加额外的$L_1$损失来扩展预测光谱图上的$L_2$损失。。在实践中，我们发现这种综合损失在有噪声的训练数据上更有鲁棒性。与[10]不同，我们没有引入基于speaker嵌入的额外损失项。

## 2.3 Neural vocoder

利用逐样本自回归 WaveNet [19]作为 vocoder，将合成网络发出的 mel谱图转换为时域波形。其结构与[15]中描述的相同，由30个 dilated 卷积层组成。网络不直接取决于 speaker 编码器的输出。合成器网络预测的mel声谱图捕获了各种声音高质量合成所需的所有相关细节，这使得 multispeaker vocoder 可以通过简单地在许多 speakers 的数据上训练来构建。

## 2.4 Inference and zero-shot speaker adaptation



---
**参考**：
1. 论文：Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu [Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis](https://arxiv.org/abs/1806.04558)


