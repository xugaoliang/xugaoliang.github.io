# 【译】降噪远程监督开放域问答

* 作者：Yankai Lin, Haozhe Ji, Zhiyuan Liu∗,Maosong Sun
* 论文：《Denoising Distantly Supervised Open-Domain Question Answering》
* 地址：http://nlp.csai.tsinghua.edu.cn/~lyk/publications/acl2018_qa.pdf
* 代码：https://github.com/thunlp/OpenQA
* Yankai Lin 网址：http://nlp.csai.tsinghua.edu.cn/~lyk/

## 摘要

远程监督开放域问题回答(DS-QA)旨在从未标记的文本集合中寻找答案。现有的DS-QA模型通常从大型语料库中检索相关段落，并应用阅读理解技术从最相关的段落中提取答案。他们忽略了其他段落中所包含的丰富信息。此外，远程监督数据不可避免地会伴随着错误的标注问题，这些噪声数据会大大降低DS-QA的性能。为了解决这些问题，我们提出了一种新颖的DS-QA模型，该模型使用段落选择器来过滤那些有噪声的段落，并使用段落阅读器从这些去噪的段落中提取正确的答案。在实际数据集上的实验结果表明，与所有基线相比，我们的模型能够从噪声数据中获取有用的信息，并在DS-QA上取得显著的改进。本文的源代码和数据可以通过 https://github.com/thunlp/OpenQA 获得。

## 1 介绍

![图1](/assets/images/nlp/DS-QA(DDSODQA)/fig1.png)

图1:模型的概述。对于“都柏林的首都是什么?”，我们的段落选择器选择两个段落$\mathrm{p}_1$和$\mathrm{p}_3$，这两个段落实际上对应于所有检索到的段落中的问题。然后我们的段落阅读器从所有选定的段落中提取正确的答案“Dublin”(红色)。最后，系统对提取的结果进行汇总，得到最终的结果。


阅读理解是近年来自然语言处理(NLP)研究的一个热点问题，它的目的是回答有关文档的问题。许多阅读理解系统(Chen 等， 2016;Dhingra 等，2017a; Cui 等，2017;Shen 等，2017; Wang 等,2017)提出并取得了很有前景的结果，因为他们的多层架构和注意力机制允许他们对问题进行推理。在某种程度上，阅读理解已经显示出最近的神经模型在阅读、处理和理解自然语言文本方面的能力。

尽管取得了成功，但现有的阅读理解系统依赖于预先确定的相关文本，而这些文本并不总是存在于真实的问答场景中。因此，阅读理解技术不能直接应用于开放领域的QA任务。近年来，研究人员试图用一个大规模的未标记语料库来回答开放域问题。Chen等(2017)提出了一种远程监督开放域问答(DS-QA)系统，该系统利用信息检索技术从维基百科中获取相关文本，然后应用阅读理解技术提取答案。

尽管DS-QA提出了一种有效的自动采集相关文本的策略，但它始终存在噪声问题。例如，对于“哪个国家的首都是都柏林?”(1)检索到的段落“都柏林是爱尔兰最大的城市……”实际上并没有回答这个问题;(2)检索到的段落“都柏林是爱尔兰的首都。此外，都柏林是爱尔兰著名的旅游城市之一和……”中的第二个“都柏林”并不是正确答案的 token。这些嘈杂的段落和 token 被认为是DS-QA中的有效实例。为了解决这个问题，Choi等人(2017)将DS-QA中的答案生成分为两个模块，包括在文档中选择一个目标段落，通过阅读理解从目标段落中提取正确答案。此外，Wang等(2018a)利用强化学习联合训练目标段落选择和答案提取。

这些方法只根据最相关的段落来提取答案，而忽略了这些段落所包含的大量丰富信息。事实上，正确的答案往往在多个段落中被提及，问题的不同方面可能在多个段落中得到回答。因此，Wang等人(2018b)提出进一步明确收集不同段落的证据，重新排列提取的答案。然而，re-ranking 方法仍然依赖于现有的DS-QA系统得到的答案，并不能从根本上解决DS-QA的噪声问题。

为了解决这些问题，我们提出了一种由粗到细的DS-QA降噪模型。如图1所示，我们的系统首先通过信息检索从一个大型语料库中根据问题检索段落。然后，为了利用所有的信息段落，我们采用一个快速的段落选择器来浏览所有检索到的段落并过滤掉那些嘈杂的段落。然后，我们使用一个严格的段落阅读器，对每个选定的段落进行仔细阅读，以提取答案。最后，我们将所有选定段落的导出结果聚合起来，以获得最终的答案。在我们的方法中，我们的段落选择器快速浏览和我们的段落阅读器精读使DS-QA能够降低嘈杂的段落噪声，并保持效率。

在Quasar-T、SearchQA 和 TriviaQA 等真实数据集上的实验结果表明，与所有基线方法相比，我们的系统通过聚合所有信息段落提取的答案，实现了显著的、一致的改进。特别地，我们证明了我们的模型可以通过选择几个信息段落来达到类似的性能，这大大加快了整个DS-QA系统的速度。我们将在Github上发布这项工作的所有源代码和数据集，以进行进一步的研究探索。

## 2 相关工作

问答是自然语言处理中最重要的任务之一。在QA方面已经投入了很多努力，尤其是在开放领域的QA方面。开放域QA最早是由(Green Jr 等人，1961)提出的。该任务旨在使用外部资源回答开放领域的问题，如文档集合(Voorhees 等人，1999)、网页(Kwok 等人，2001;Chen and Van Durme, 2017)，结构化知识图谱(Berant 等人，2013a; Bordes 等人，2015）或者自动提取关系三元组(Fader 等人， 2014)。

近年来，随着机器阅读理解技术的发展(Chen 等， 2016;Dhingra 等，2017a;Cui 等，2017;Shen 等，2017;Wang 等，2017）研究人员试图通过对普通文本进行阅读理解来回答开放领域的问题。Chen等(2017)提出了一种DS-QA系统，该系统从大规模语料库中检索问题的相关文本，然后使用阅读理解模型从这些文本中提取答案。然而，在DS-QA中检索到的文本往往存在噪声，影响了DS-QA的性能。因此，Choi 等.(2017)和 Wang 等. (2018a)试图通过将问答问题分解为段落选择和答案提取来解决DS-QA中的噪声问题，他们都只是在所有检索到的段落中选择最相关的段落来提取答案。它们丢失了那些被忽视的段落中所包含的大量丰富信息。因此，Wang等(2018b)提出了基于强度和覆盖率的重新排序方法，该方法可以将现有DS-QA系统从每个段落中提取的结果进行汇总，从而更好地确定答案。然而，该方法依赖于现有DS-QA模型的预提取结果，由于对所有检索到的段落都进行了不加区分的考虑，因此在远程监督数据中仍然存在噪声问题。与这些方法不同的是，我们的模型使用了一个段落选择器来过滤掉那些有噪声的段落，并保留那些有信息的段落，从而可以充分利用有噪声的DS-QA数据。

我们的工作也受到了 NLP 中由粗到细模型的启发。Cheng和Lapata(2016)和Choi等人(2017)提出了一个由粗到细的模型，首先选择基本的句子，然后分别对选择的句子进行文本摘要或阅读理解。Lin等(2016)利用选择性注意对所有句子的信息进行聚合，提取关系事实。Yang 等人.(2016)提出了一种分层注意力网络，该网络在文档分类中应用了单词和句子两个层次的注意。我们的模型也使用了一个由粗到细的模型来处理DS-QA中的噪声问题，它首先选择检索到的信息段落，然后从这些选择的段落中提取答案。

## 3 方法论

在本节中，我们将详细介绍我们的模型。我们的模型旨在从大规模的无标记语料库中提取给定问题的答案。首先利用信息检索技术从开放域语料库中检索与问题对应的段落，然后从这些段落中提取答案。

形式上，给定一个问题$q = (q^1,q^2,\cdots,q^{|q|})$，我们检索m个段落，定义为$P = \lbrace p_1,p_2,\cdots,p_m\rbrace $，其中$p_i = (p_i^1,p_i^2,\cdots,p_i^{|p_i|})$是第$i$个检索段落。在给定问题$q$和对应段落集合$P$时，我们的模型度量抽取的答案$a$的概率。如图1所示，我们的模型包含两部分:

1. **段落选择器**。给定问题$q$和检索到的段落$P$，段落选择器度量检索到的所有段落的概率分布$\mathrm{Pr}(p_i|q, P)$，该概率分布$Pr$用于选择真正包含问题$q$答案的段落。

2. **段落阅读器**。给定问题$q$和段落$p_i$，段落阅读器通过多层长短期记忆网络计算要提取的答案$a$的概率$\mathrm{Pr}(a|q, p_i)$。

总的来说，提取给定问题$q$的答案的概率$\mathrm{Pr}(a|q, P)$可以计算为:

$$
\tag{1}
\mathrm{Pr}(a|q,P)=\sum_{p_i \in P}\mathrm{Pr}(a|q,p_i)\mathrm{Pr}(p_i|q,P)
$$

### 3.1 段落选择器

由于DS-QA数据中不可避免地会出现错误的标注问题，因此在对所有检索到的段落信息进行挖掘时，需要过滤掉这些有噪声的段落。很明显，我们需要估计每一段的置信度。因此，我们使用段落选择器来度量所有段落中每个段落包含答案的概率。

**段落编码**。 首先将$p_i$段落中的每个单词$p_i^j$表示为一个词向量$\mathrm{p}_i^j$，然后将每个词向量输入神经网络，得到隐藏表征$\hat{\mathrm{p}}_i^j$。这里我们采用了两种类型的神经网络，包括:

1. 多层感知器（MLP)

$$
\tag{2}
\hat{\mathrm{p}}_i^j = \mathrm{MLP}(\mathrm{p}_i^j)
$$

2. 递归神经网络（RNN）

$$
\tag{3}
\lbrace \hat{\mathrm{p}}_i^1,\hat{\mathrm{p}}_i^2,\cdots,\hat{\mathrm{p}}_i^{|p_i|}\rbrace  = \mathrm{RNN}(\lbrace \mathrm{p}_i^1,\mathrm{p}_i^2,\cdots,\mathrm{p}_i^{|p_i|} \rbrace )
$$

其中 $\hat{\mathrm{p}}_i^j$ 被期望能够编码单词$p_i^j$及其周围单词的语义信息。对于RNN，我们选择单层（可能论文笔误，应该为多层）双向长短期记忆网络(LSTM)作为 RNN 单元，将各层的隐藏状态拼接起来，得到$\hat{\mathrm{p}}_i^j$ 

**问题编码**。与段落编码类似，我们也把问题中的每个词$q^i$表示为词向量$\mathrm{q}^i$，然后将他们传入MLP:

$$
\tag{4}
\hat{\mathrm{q}}_i^j=\mathrm{MLP}(\mathrm{q}_i^j)
$$

或者RNN：

$$
\tag{5}
\lbrace \hat{\mathrm{q}}^1,\hat{\mathrm{q}}^2,\cdots,\hat{\mathrm{q}}^{|q|}\rbrace  = \mathrm{RNN}(\lbrace \mathrm{q}^1,\mathrm{q}^2,\cdots,\mathrm{q}^{|q|}\rbrace )
$$

其中$\hat{\mathrm{q}}^j$是单词$q^j$的隐藏表征形式，期望对单词$q^j$的上下文信息进行编码。然后，我们对隐藏表征进行自注意力运算，得到问题$q$的最终表征$\mathrm{q}$:

$$
\tag{6}
\hat{\mathrm{q}} = \sum_j \alpha_j \hat{\mathrm{q}}^j
$$

其中$\alpha_j$编码了每个问题单词的重要性，计算方式为：

$$
\tag{7}
\alpha_i = \frac{\exp(\mathrm{wq}_i)}{\sum_j \exp(\mathrm{wq}_j)}
$$

其中$\mathrm{w}_b$是一个可学习的权重向量。

接下来，我们通过 max-pooling 层和 softmax 层计算每个段落的概率:

$$
\tag{8}
\mathrm{Pr}(p_i|q,P) = \mathrm{softmax}( \mathop{\max_j} (\hat{\mathrm{p}}_i^j \mathrm{Wq}) )
$$

其中$\mathrm{W}$是一个要学习的权重矩阵。

### 3.2 段落阅读器 

段落阅读器的目的是从段落$p_i$中提取答案。与段落阅读器(可能是论文笔误，应该为段落选择器)类似，我们首先通过多层双向LSTM将每个段落$p_i$编码为$\lbrace  \bar{\mathrm{p}}_i^1,\bar{\mathrm{p}}_i^2,\cdots,\bar{\mathrm{p}}_i^{|p_i|}, \rbrace$。我们还通过一个自注意力的多层双向LSTM得到了问题的嵌入$\bar{\mathrm{q}}$。

段落阅读器的目的是提取最可能是正确答案的 token 的范围。我们把它分成预测答案范围的开始和结束位置。因此，从给定段落$p_i$中提取问题$q$的答案$a$的概率可以计算为:

$$
\tag{9}
\mathrm{Pr}(a|q,p_i)=P_s(a_s)P_e(a_e)
$$

其中 $a_s$ 和 $a_e$ 表示段落中答案$a$的起始和结束位置，$P_s(a_s)$ 和 $P_e(a_e)$ 分别是 $a_s$和$a_e$为起始和结束单词的概率，计算方式为：

$$
\tag{10}
P_s(j) = \mathrm{softmax}(\bar{\mathrm{p}}_i^j\mathrm{W}_s\bar{\mathrm{q}})
$$

$$
\tag{11}
P_e(j) = \mathrm{softmax}(\bar{\mathrm{p}}_i^j\mathrm{W}_e\bar{\mathrm{q}})
$$

其中 $\mathrm{W}_s$ 和 $\mathrm{W}_e$ 是两个要学习的权重矩阵。 在 DS-QA 中，由于我们没有手动标记答案的位置，所以我们可能在一个段落中有多个与正确答案匹配的标记。设$\lbrace (a_s^1,a_e^1),(a_s^2,a_e^2),\cdots,(a_s^{|a|},a_e^{|a|})\rbrace$为$p_i$段落中与答案$a$匹配的 tokens 的开始和结束位置的集合。式(9)用两种方法进一步定义:

（1）**Max**。也就是说，我们假设段落中只有一个 token 表示正确的答案。这样，通过最大化所有候选 token 的概率来定义提取答案$a$的概率:

$$
\tag{12}
\mathrm{Pr}(a|q,p_i) = \mathop{\max_j} \mathop{\mathrm{Pr}_s}(a_s^j)\mathop{\mathrm{Pr}_e}(a_e^j)
$$


（2）**Sum**。 用这种方式，我们对所有与正确答案匹配的 tokens 都一视同仁。我们将答案抽取概率定义为:

$$
\tag{13}
\mathrm{Pr}(a|q,p_i) = \sum_j \mathop{\mathrm{Pr}_s}(a_s^j)\mathop{\mathrm{Pr}_e}(a_e^j)
$$

我们的段落阅读器模型是受到之前机器阅读理解模型的启发，即(Chen 等.， 2016)中描述的 Attentive Reader。事实上，其他的阅读理解模式也可以很容易地作为我们的段落阅读器。由于篇幅的限制，本文仅探讨了  Attentive Reader 的有效性。

### 3.3 学习与预测

对于学习目标，我们使用极大似然估计定义损失函数$L$:

$$
\tag{14}
L(\theta)=-\sum_{(\bar{a},q,P) \in T} \log \mathrm{Pr}(a|q,P)-\alpha R(P)
$$

其中$\theta$表示我们的模型的参数,$a$表示正确答案,$T$是整个训练集和$R(P)$是段落选择器的正则化项,以避免过度拟合。这里，$R(P)$定义为$\mathrm{Pr}(p_i |q, P)$与概率分布$\mathcal{X}$之间的KL散度，其中，如果该段落包含正确答案，$\mathcal{X}_i = \frac{1}{\mathcal{c}_P}$ ($\mathcal{c}_P$是$P$中包含正确答案的段落数量)，否则为0。具体地说，$R(P)$定义为:

$$
\tag{15}
R(P) = \sum_{p_i\in P} \mathcal{X_i} \log \frac{\mathcal{X_i} }{\mathrm{Pr}(p_i|q,P)}
$$

为了解决优化问题，我们采用 Adamax 将目标函数最小化，如(Kingma and Ba, 2015)所述。

在测试过程中,我们提取答案$\hat{a}$最高概率如下:

$$
\tag{16}
\begin{aligned}
\hat{a} &= \mathop{\arg\max_a} \mathrm{Pr}(a|q,P) \\\\
&=\mathop{\arg\max_a} \sum_{p_i\in P}  \mathrm{Pr}(a|q,p_i)  \mathrm{Pr}(p_i|q,P)
\end{aligned}
$$

在这里，段落选择器可以看作是快速浏览所有段落，这决定了每个段落包含答案的概率分布。因此，我们可以简单地将具有高概率的段落的预测结果聚合起来用于加速。

## 4 实验

我们在五个开放域问答数据集上对模型进行了评估。

**Quasar-T** （https://github.com/bdhingra/quasar ）(Dhingra 等，2017b)由4.3万个开放域的琐碎问题组成，这些问题的答案是从 ClueWeb09 数据源中提取出来的，而段落则是使用 LUCENE 从 ClueWeb09 数据源中为每个问题检索50个句子得到的。

**SearchQA** （https://github.com/nyu-dl/SearchQA ）(Dunn 等，2017)是一个大规模的开放域问答数据集，由从J! Archive上爬取的问答对组成，并从 Google Search API中为每个问题检索50个网页来获得段落。

**TriviaQA** （http://nlp.cs.washington.edu/triviaqa/ ） (Joshi 等，2017)包含了9.5万个问答对，由琐碎爱好者编写并独立收集证据文件得到。平均每个问题6个，并利用 Bing Web search API收集了50个与这些问题相关的网页。

**CuratedTREC** （https://github.com/brmson/dataset-factoid-curated/tree/master/trec ）(Voorhees 等，1999)基于来自 TREC QA 任务的基准，其中包含从 TREC1999、2000、2001和2002年的数据集中提取的2,180个问题。

**WebQuestions** （https://github.com/brmson/dataset-factoid-webquestions ）(Berant 等，2013b)是为回答来自 Freebase 知识库的问题而设计的，该知识库是通过在谷歌 Suggest API 中爬取问题来构建的，使用的段落是从英文维基百科中检索的。

对于 Quasar-T、SearchQA 和 TriviaQA 数据集，我们使用(Wang 等， 2018a)提供的检索段落。对于 CuratedTREC 和 WebQuestions 数据集，我们使用英语维基百科的2016-12-21 dump 作为我们的知识来源来回答问题，并在此基础上构建 Lucene 索引系统。然后，我们将每个输入问题作为一个查询来检索前50个段落。

这些数据集的统计数据如表1所示。

Dataset | #Train | #Dev | #Test
-|-|-|-
Quasar-T | 37,012 | 3,000 | 3,000
SearchQA | 99,811 | 13,893 | 27,247
TriviaQA | 87,291 | 11,274 | 10,790
CuratedTREC | 1,486 | - | 694
WebQuestions | 3,778 | - | 2,032

表1：数据集的统计数据

依照(Chen 等.， 2017)，我们采用 ExactMatch (EM)和 F1 两个指标来评估我们的模型。EM度量的是与 ground truth 答案中的一个完全匹配的预测的百分比，F1分数是一个度量指标，松散地度量预测与 ground truth 答案之间的平均重叠。

### 4.2 基线

为了比较，我们选择了几个公共模型作为基线，包括:(1) GA (Dhingra 等， 2017a)是一种具有门控注意力机制的多重跨段落跳跃的阅读理解模型;(2) BiDAF (Seo 等， 2017)，一个具有双向注意流网络的阅读理解模型。(3) AQA (Buck 等， 2017)，一个增强的系统，学习重写问题并聚合重写问题产生的答案;(4) $\mathrm{R}^3$ (Wang 等， 2018a)，利用排序器选择最自信段落的增强模型，训练阅读理解模型。

我们还将我们的模型与它的朴素版本进行了比较，后者对每个段落都一视同仁，并对段落的选择设置了一个统一的分布。我们将我们的模型命名为“Our+FULL”，它的初始版本为“Our+AVG”。

### 4.3 实验设置

在本文中，我们在验证集上调整我们的模型，并使用网格搜索来确定最优参数。我们选择LSTM的隐状态大小为 $n \in \lbrace 32,64,\textbf{128},\cdots,512\rbrace$, 文档和问题编码器的 LSTM 层数为$\lbrace 1,2,\textbf{3},4\rbrace$,正则化的权重 $\alpha$ 为$\lbrace 0.1,\textbf{0.5},1.0,2.0\rbrace$和 batch 大小为 $\lbrace 4,8,16,\textbf{32},64,128\rbrace$。最优参数用粗体突出显示。对于其他参数，由于它们对结果几乎没有影响，我们只需遵循(Chen 等, 2017)中使用的设置。

对于训练，我们的 Our+FULL 模型首先由使用 Our+AVG模型的预训练初始化，并将所有训练数据的迭代数设置为10。对于预先训练好的词向量，我们使用300维 GloVe（http://nlp.stanford.edu/data/glove.840B.300d.zip ） (Pennington 等， 2014)词向量，他们是从 840B Web 抓取数据中学到的。

### 4.4 不同段落选择器的效果

由于我们的模型包含了不同类型的神经网络，包括 MLP 和 RNN 作为我们的段落选择器，我们研究了不同的段落选择器对 Quasar-T 和 SearchQA 验证集的影响。


![表3](/assets/images/DS-QA(DDSODQA)/table3.png)

表3：不同段落选择器对 Quasar-T 和 SearchQA 验证集的影响。

如表3所示，我们的 RNN 段落选择器对 Quasar-T 和 SearchQA 都有显著的统计改进。注意，与 Our+AVG 相比，使用 MLP 段落选择器的 Our+FULL 在 Quasar-T 数据集中的性能更差。它表明 MLP 段落选择器不足以区分一个段落是否回答了问题。由于 RNN 段落选择器不断地改进所有的评估指标，我们在接下来的实验中使用它作为默认的段落选择器。

### 4.5 不同段落阅读器的效果


![表4](/assets/images/DS-QA(DDSODQA)/table4.png)

表4：不同段落阅读器对 Quasar-T 和 SearchQA 验证集的影响。Our+FULL中使用的段落选择器是RNN。

在这里，我们比较了不同类型段落阅读器的性能，结果如表4所示。

从表中，我们可以看到，在大多数情况下，所有带有Sum或Max段落阅读器的模型都具有类似的性能，但是在 SearchQA 数据集上，用 Max 阅读器的 Our+AVG 相比 用 Sum 阅读器的模型，增加了大约3%。这表明 Sum 阅读器更容易受到噪声数据的影响，因为它将与答案匹配的所有 tokens 视为 ground truth。在接下来的实验中，我们选择 Max 阅读器作为段落阅读器，因为它更稳定。

### 4.6 总体结果

![表2](/assets/images/DS-QA(DDSODQA)/table2.png)

表2：四个开放域QA测试数据集的实验结果:Quasar-T、SearchQA、TriviaQA、CuratedTREC 和 WebQuestions。TriviaQA、CuratedTREC 和 WebQuestions不提供开放领域设置下的排行榜。因此，在这个设置中没有公共基线，我们只报告$\mathrm{R}^3$基线的结果。TriviaQA数据集是在验证集上的结果。CuratedTREC 数据集是通过正则表达式匹配(REM)来计算的。



在这一部分中，我们将展示不同模型在5个DS-QA数据集上的性能，并提供一些进一步的分析。我们的模型的性能如表2所示。从结果可以看出:

（1）我们的模型，包括 Our+AVG 和 Our+FULL，与其他基线相比，在大多数数据集上取得了更好的结果。原因是我们的模型可以充分利用所有检索到的段落的信息来回答问题，而其他基线模型只考虑最相关的段落。它验证了我们的说法，即合并所有检索到的段落的丰富信息可以帮助我们更好地提取问题的答案。

（2）在所有数据集上，Our+FULL 模型都比 Our+AVG 模型表现得更好。这表明我们的段落选择器可以有效地过滤掉那些无意义的检索段落，从而缓解DS-QA中错误的标注问题。

（3）在 TriviaQA 数据集上，Our+AVG 模型的性能比$\mathrm{R}^3$模型差。通过观察 TriviaQA 数据集，我们发现在这个数据集中只有一两个检索到的段落包含正确的答案。因此，简单地使用所有检索到的段落平均地提取答案可能会带来很多噪音。相反，通过考虑每个检索段落的置信度，Our+FULL 模型仍然有一些改进。

（4）在 CuratedTREC 和 WebQuestions 数据集上，与$\mathrm{R}^3$模型相比，我们的模型只有轻微的改进。其原因是这两个数据集的大小很小，而且这些DS-QA系统的性能受到与用于预训练的数据集的差距的严重影响。

### 4.7 段落选择器性能分析

为了演示段落选择器在过滤那些有噪声的被检索段落方面的有效性，我们在本部分中将段落选择器与传统的信息检索(IR)（信息检索模型使用Lucene实现的BM25对段落进行排序）进行了比较。我们还将模型与一个名为 Our+INDEP 的新基线进行了比较，该基线独立地训练段落阅读器和段落选择器。为了训练段落选择器，我们把所有包含正确答案的段落都当作 ground-truth 并用公式14来学习。


![表5](/assets/images/DS-QA(DDSODQA)/table5.png)

表5：在段落选择中，我们的段落选择器与传统的信息检索模型进行了比较。在 WebQuestions 数据集中使用的 Our+AVG 和 Our+FULL 模型是使用 Quasart-T 数据集进行预训练的。

首先，我们展示了在选择信息段落方面的性能。由于远程监督数据并没有标注为ground-truth 的数据来告诉我们哪些段落实际上回答了这个问题，所以我们采用了 held-out 评估。它通过将所选段落与伪标签进行比较来评估我们的模型:如果一个段落包含与正确答案匹配的标记，那么我们将该段落视为 ground-truth。我们使用Hit@N表示正确段落在 top-N 中的比例作为评价指标。结果如表5所示。从表中我们可以看出:

（1）Our+INDEP 和 Out+FULL 在信息段落的选择上都明显优于传统的 IR 模型。这表明我们提出的段落选择器能够捕捉到问题和段落之间的语义关联。

（2）Our+FULL 与 Our+SINGLE 在选择有效段落方面的性能从 Hits@1 到 Hits@5 都类似。原因是我们评估段落选择的方法与 Our+SINGLE 中 ranker 的训练目标是一致的。

事实上，这种评估方法可能不足以区分不同段落选择器的性能。因此，我们进一步报告了 Our+FULL 和 Our+INDEP的整体答案提取性能。从表中，我们可以看到，虽然 Our+FULL 在段落选择方面与 OUT+SINGLE 有相似的性能，但是在提取答案方面表现得更好。它表明我们的段落选择器可以更好地确定与答案匹配的 tokens 实际上是通过与段落阅读器的联合训练来回答问题的。


### 4.8 不同段落数量的性能

在仔细阅读段落之前，我们的段落选择器可以被看作是快速浏览的步骤。为了显示我们的段落选择器可以在多大程度上加速DS-QA系统，我们将模型的性能与段落选择器(Our+FULL)或传统IR模型所选择的 top 段落进行了比较。


![图2](/assets/images/DS-QA(DDSODQA)/fig2.png)

图2：在Quasar-T(上)和SearchQA(下)数据集中使用不同数量的 top 段落的性能。

结果如图2所示。毫无疑问，随着段落数量的增加，Our+IR 和 Our+FULL 模型的性能将显著提高。从图中可以看出，无论是在 Quasar-T 还是 SearchQA 数据集中，Our+FULL 都只能使用检索到的段落的一半来进行答案提取，并且没有出现性能下降的情况，而 Our+IR 在减少段落数量的同时，性能却出现了显著的下降。实验结果表明，该模型可以通过段落选择器选择几个信息段落来提取答案，从而加快整个DS-QA系统的运行速度。

### 4.9 潜在改善

为了显示我们DS-QA系统的答案重排模型抽取的答案进行聚合的潜在改进，我们对验证集上的系统性能上限进行了统计分析。在这里，我们通过评估抽取答案top-k上的F1/EM得分将我们的模型与$\mathrm{R}^3$模型进行比较。我们的系统的top-k性能可以看作是我们的系统对提取的top-k答案重新排序的上限。


![表7](/assets/images/DS-QA(DDSODQA)/table7.png)

表7：通过重新排列答案，在DS-QA性能的潜在提升。性能基于Quasar-T和SearchQA验证数据集。

从表7中可以看出:(1)top 3/5 与 top1 的 DS-QA 性能(10-20%)存在明显差距。这说明我们的 DS-QA 模型远远没有达到较高的性能，仍然有很大的概率通过重新排序来改进。(2) Our+FULL 模型在 Quasar-T 和 SearchQA 数据集上的 top-1、top-3 和 top-5 都比 $\mathrm{R}^3$模型的性能好 5% 到 7%。结果表明，将所有信息段的信息聚合在一起，可以有效地增强 DS-QA 模型，使用答案重新排序更有潜力。

### 4.10 案例研究


![表6](/assets/images/DS-QA(DDSODQA)/table6.png)

表6：用我们的模型提取出给定问题的答案示例。粗体标记是每个段落中提取的答案。段落是根据我们的段落选择器输出的概率排序的。

表6显示了我们的模型的两个例子，说明我们的模型可以充分利用信息段落。从表格中我们发现:
（1）对于“谁导演了1946年的《生活多美好》?”，我们的模型从段落选择器排序的前两段中提取了答案“Frank Capra”。
（2）对于“哪个著名艺术家可以同时用左手和右手写字?”，我们的模型从第一段可以看出“Leonardo Da Vinci”是一位艺术家，从第二段可以看出他可以同时用左手和右手写字

## 5 结论及未来工作

本文提出了一种降噪远程监督开放域问答系统，该系统由一个段落选择器和一个段落阅读器组成，分别对选定的段落进行快速浏览和精读。我们的模型可以充分利用所有的信息段落，缓解DS-QA中的错误标注问题。在实验中，我们证明我们的模型显著且持续地优于最先进的DS-QA模型。特别地，我们演示了仅使用被选中段落的前几个时，模型的性能几乎不会受到影响。

在未来的研究中，我们将探索以下几个方向:
（1）增加一个答案重新排序的步骤，可以进一步完善我们的模型。我们将探讨如何有效地重新排序我们提取的答案，以进一步提高性能。
（2）事实知识、常识等背景知识可以有效帮助我们进行段落的选择和答案的提取。我们将把外部知识基础纳入我们的DS-QA模型，以提高其性能

**致谢**

本研究由国家自然科学基金委员会(NSFC No.61572273,61661146007和61572273)资助。本文还获得了微软亚洲研究院 FY17-RES-THEME-017 项目的部分资助。

**参考文献**

* Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013a. Semantic parsing on freebase from question-answer pairs. In Proceedings of EMNLP. pages 1533–1544.
* Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013b. Semantic parsing on Freebase from question-answer pairs. In Proceedings of EMNLP. pages 1533–1544.
* Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075 .
* Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, and Wei Wang. 2017. Ask the right questions: Active question reformulation with reinforcement learning. arXiv preprint arXiv:1705.07830 .
* Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A thorough examination of the cnn/daily mail reading  omprehension task. In Proceedings of ACL. pages 2358–2367.
* Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the ACL. pages 1870–1879.
* Tongfei Chen and Benjamin Van Durme. 2017. Discriminative information retrieval for question answering sentence selection. In Proceedings of EACL. pages 719–725.
* Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. In Proceedings of ACL. pages 484–494.
* Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. 2017. Coarse-to-fine question answering for long documents. In Proceedings of ACL. pages 209– 220.
* Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2017. Attention-overattention neural networks for reading comprehension. In Proceedings of ACL. pages 593–602.
* Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2017a. Gatedattention readers for text comprehension. In Proceedings of ACL. pages 1832–1846.
* Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. 2017b. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904 .
* Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179 .
* Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of SIGKDD. pages 1156–1165.
* Bert F Green Jr, Alice K Wolf, Carol Chomsky, and Kenneth Laughery. 1961. Baseball: an automatic question-answerer. In Proceedings of IRE-AIEEACM. pages 219–224.
* Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of ACL. pages 1601–1611.
* Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.
* Cody Kwok, Oren Etzioni, and Daniel S Weld. 2001. Scaling question answering to the web. TOIS pages 242–262.
* Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural relation extraction with selective attention over instances. In Proceedings of ACL. pages 2124–2133.
* Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP. pages 1532–1543.
* Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In Proceedings of ICLR.
* Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2017. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of SIGKDD. ACM, pages 1047–1055.
* Ellen M Voorhees et al. 1999. The trec-8 question answering track report. In Proceedings of TREC. pages 77–82.
* Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang. 2018a. R3: Reinforced ranker-reader for open-domain question answering. In Proceedings of AAAI.
* Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2018b. Evidence aggregation for answer re-ranking in open-domain question answering. In Proceedings of ICLR.
* Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In Proceedings of ACL. pages 189–198.
* Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In Proceedings of NAACL. pages 1480–1489.

