---
typora-root-url: ../../../
---

# 【译】带函数式时间表征学习的自注意力.md

* 作者：Da Xu, Chuanwei Ruan, Sushant Kumar, Evren Korpeoglu, Kannan Achan
* 论文：《Self-attention with Functional Time Representation Learning》
* 地址：https://arxiv.org/abs/1911.12864

---

## 个人总结

传统自注意力在考虑序列问题时，利用位置编码，但位置编码只能解决离散问题，对于连续时间无法解决。而且用位置编码的注意力机制也不能识别出时间上的远近信息。

该文就是为了解决连续时间的编码问题，将连续性的时间跨度信息编码到高维空间，同时保证在高维空间中，依然具有时间的平移不变性，即时间的起点可以不同，但表达的规律是相同的。同时将这个编码与自注意力机制结合使用。因为传统自注意力机制采用的是Q和K的内积计算权重。所以本文所采用的解决方案是构造核函数，根据核函数及一些定理要求设计出了几种时间映射函数，将时间跨度信息映射到高维空间。将时间编码和事件编码拼接进行后续自注意力。

根据 Bochner 定理，设计出3种映射函数
1. reparameterization trick 
2. parametric inverse CDF 
3. non-parametric inverse CDF 

根据 Mercer 定理,设计出1种映射函数

具体映射及参数可见内容中表1

对于Stack Overflow和Walmart.com数据集，Mercer 时间嵌入实现了最好的性能，而对于MovieLens数据集，Bochner Non-para优于其余方法。

---

## 摘要

在自然语言处理中，具有自注意力的序列建模已经取得了很好的效果。自注意力具有模型灵活性、计算复杂性和可解释性等优点，正逐渐成为事件序列模型的重要组成部分。然而，像大多数其他的序列模型一样，自注意力并不能解释事件之间的时间跨度，因此它捕捉的是序列信号而不是时间模式。在不依赖递归网络结构的情况下，自注意力通过位置标记来识别事件顺序。为了弥补时间无关和时间相关事件序列建模之间的差距，我们引入了一个嵌入时间跨度到高维空间的函数式特征映射（functional feature map）。通过构造相关的平移不变时间核函数(translation-invariant time kernel function)，揭示了经典函数式函数分析结果下特征映射的函数形式，即Bochner定理和Mercer定理。我们提出了几个模型来学习函数式时间表征以及与事件表征的交互。这些方法是在各种连续时间事件序列预测任务下对真实数据集进行评估的。实验结果表明，所提出的方法与基线模型相比，具有更好的性能，同时也能捕获有用的时间-事件交互。

# 1. 介绍

注意机制假设事件序列的输出只与部分序列输入相关，它正迅速成为各种机器学习任务(如神经翻译[1]、图像字幕生成[25]和语音识别[4])的基本工具。它通过连续捕获序列输入的重要权重来工作，通常作为附加组件用于基本模型，如递归神经网络(RNNs)和卷积神经网络(CNNs)[3]。最近，一种仅依赖注意力模块“self-attention”的seq-to-seq模型在神经翻译[20]中获得了最先进的性能。它从输入事件序列中检测注意力权重，并返回序列表征。在不依赖于递归网络结构的情况下，由于序列处理可以完全并行化，自注意力提供了很有吸引力的计算优势。原始的自注意力模块的关键是位置编码，它映射离散的位置索引 ${1,\cdots,l}$为$\mathbb{R}^d$中的向量，可以作为自由参数固定或联合优化。位置编码允许自注意力识别排序信息。然而，它也限制了模型的时间独立或离散时间的事件序列建模，其中排序位置的差异可以测量事件发生之间的距离。

在连续时间事件序列中，事件之间的时间跨度通常对其预测的相对重要性有重要影响。由于这些事件是非周期性发生的，因此在顺序模式和时间模式之间存在差距。例如，在用户在线行为分析中，驻留时间通常表示对web页面的感兴趣程度，而顺序信息只考虑过去浏览的顺序。此外，检测时间和事件上下文之间的交互是用户行为建模[12]中一个日益重要的主题。在网上购物中，交易通常表示长期利益，而浏览通常是短期的。因此，未来的推荐应该同时依赖于事件上下文和事件发生的时间戳。

为了有效地编码事件上下文并将其输入到自注意力模型中，离散事件通常被嵌入到一个连续的向量空间[2]中。经过训练后，其向量表征的内积往往反映了相似等关系。在一般的自注意力中，事件嵌入常常被添加到位置编码中，形成事件位置表征[20]。因此，考虑用一些将时间嵌入到向量空间中的函数映射来替换位置编码是很自然和直接的。

然而，不像位置编码那样只需要有限数量的索引表示，时间跨度是一个连续变量。嵌入时间的挑战有三个方面。首先，需要确定以时间跨度为输入的合适的函数形式。其次，函数形式必须正确地参数化，并且可以作为模型的一部分进行联合优化。最后，嵌入的时间表征应该遵循时间本身的函数性质。具体来说，相对时差比绝对时间戳在序列建模中的插补或外推作用要重要得多。因此，两个时间表征在嵌入空间中的相对位置应该能够反映它们的时间差异。本论文的贡献总结如下:

* 我们提出了平移不变的时间核，它激发了几种基于经典泛函分析理论的时间特征映射的函数形式，即Bochner (Bochner)定理[13]和Mercer (Mercer)定理[15]。与其他启发式时间矢量法相比，我们的提议具有坚实的理论依据和保证。
* 我们根据时间特征映射开发可行的时间嵌入，使它们正确参数化并与自注意力兼容。我们进一步讨论了对时间嵌入的解释，以及如何在自注意力的情况下对它们与事件表征的交互进行建模。
* 我们对所提出的方法进行定性和定量评估，并将其与多个数据集(其中两个是公共数据集)的各种事件预测任务中的几个基线方法进行比较。我们特别将其与位置编码的神经网络和自注意力进行了比较，以证明该方法在连续时间事件序列建模中的优越性。提供了几个案例研究来显示我们的模型捕获的时间-事件交互。

## 2. 相关工作

原始的self-attention使用dot-product attention[20]，定义为:

$$
\tag{1}
Attn(Q,K,V)=softmax(\frac{QK^\top}{\sqrt{d}})V
$$

其中Q表示 queries，K表示 keys ，V表示序列中事件的 values (表征)。自注意力机制依赖于位置编码来识别和捕获序列信息，其中，在所有序列中共享的每个位置的向量表征被添加或连接到相应的事件嵌入中。上述Q、K和V矩阵通常是事件位置组合表征的线性(或恒等)投影。注意力模式通过检测 query-key 的内积，并作为组合事件值的权重传播到输出。在不同的用例下，包括在线推荐[10]，开发了几种自注意力的变体，其中序列表征通常由事件嵌入的注意力加权和给出。

针对RNNs中连续时间输入的问题，提出了改进的门结构[27]的 time-lstm 模型。经典的时间点过程也允许使用事件间的时间间隔作为连续的随机变量来模拟连续观测[26]。提出了几种将 point process 与 RNNs 耦合以考虑时间信息的方法[23,22,14,6]。然而，在这些工作中，事件之间的时间间隔直接附加到隐藏的事件表征中，作为RNNs的输入。最近的一项工作提出了一个用时间编码的 time-aware RNN[12]。

在我们的工作中提出的 functional time embeddings 有很好的理论依据和解释。此外，通过用时间嵌入代替位置编码，我们继承了自注意力的优势，如计算效率和模型可解释性。虽然在本文中我们没有讨论如何使 function time 表征适应于其他设置，但所提出的方法可以看作是一种通用的时间嵌入技术。

## 前述

从一个区间(假设从原点开始)$T=[0,t_{\max}]$到$\mathbb{R}^d$的嵌入时间等于找到一个映射$\Phi:T\rightarrow \mathbb{R}^d$。时间嵌入可以添加或拼接到事件嵌入$Z \in \mathbb{R}^{d_E}$，其中$Z_i$给出事件$e_i$的向量表征，$i=1,\cdots,V$表示总共有$V$个事件。直觉是，在事件和时间表征的拼接上，进行两个时间依赖事件$(e_1,t_1)$和$(e_2,t_2)$的内积，即$[Z_1,\Phi(t_1)]'[Z_2,\Phi(t_2)]=\langle Z_1,Z_2 \rangle + \langle \Phi(t_1),\Phi(t_2) \rangle$。$\langle Z_1,Z_2 \rangle$表示事件之间的关系,我们希望$\langle \Phi(t_1),\Phi(t_2) \rangle$能捕获时间模式,特别是那些我们之前讨论的$t_1−t_2$的时间差。这表明制定时间模式与平移不变的内核$\mathcal{K}$，$\Phi$作为特征映射和$K$关联。

让内核为$\mathcal{K}:T\times T \rightarrow \mathbb{R}$,其中$\mathcal{K}(t_1,t_2) := \langle \Phi(t_1),\Phi(t_2)\rangle$ 和 $\mathcal{K}(t_1,t_2) = \psi(t_1-t_2),\forall t_1,t_2 \in T$,其中$\psi:[-t_{\max},t_{\max}] \rightarrow \mathbb{R}$.这里函数映射$\Phi$捕获核函数嵌入原始数据到一个高维空间,所以引入核函数时的想法是按照我们最初的目标。注意，核函数$\mathcal{K}$是半正定的(positive semidefinite,PSD)，因为我们用一个 Gram matrix 表示它。不失一般性,我们假设$\Phi$是连续的,这表明$K$是平移不变,PSD也是连续的。

所以学习时间的任务模式转换为内核学习问题与$\Phi$特征映射。同时,事件嵌入和时间之间的交互可以用其他映射$(Z,\Phi(t)) \rightarrow f(Z,\Phi(t))$识别，我们将在第6节讨论。通过把时间嵌入关联到核函数来学习,我们希望用一些函数形式确定$\Phi$兼容当前深度学习框架,这样通过bask-propagation计算仍然是可行的（感觉可能是作者笔误，应该是反向传播吧）。经典泛函分析理论提供了重要的洞见来识别候选函数$\Phi$的形式。我们首先阐述了 Bochner 定理和 Mercer 定理，并简要讨论了它们的含义。

**Theorem 1**（Bochner's Theorem）A continuous, translation-invariant kernel  $\mathcal{K}(x,y)=\psi(x-y)$ on $\mathbb{R}^d$ is positive definite if and only if there exists a non-negative measure（测度） on $\mathbb{R}$ such that $\psi$ is the Fourier transform of the measure.

Bochner定理的含义是，当适当缩放时，我们可以这样表达$\mathcal{K}$:

$$
\tag{2}
\mathcal{K}(t_1,t_2) = \psi(t_1,t_2) = \int_{\mathbb{R}} e^{iw(t_1-t_2)}p(w)dw = E_w[\xi_w(t_1)\xi_w(t_2)^*]
$$

其中$\xi_w(t)=e^{iwt}$因为内核$K$和概率测度$p(ω)$是实数,我们提取(2)式的实部得到：

$$
\tag{3}
\mathcal{K}(t_1,t_2) = E_w[cos(w(t_1-t_2))] = E_w[cos(wt_1)cos(wt_2)+sin(wt_1)sin(wt_2)]
$$

用这个核函数$K$的替代表达式，期望项可以近似为蒙特卡罗积分[17]。假设我们有$d$个样本$w_1,\cdots,w_d$来自$p(ω)$,估计我们的内核$\mathcal{K}(t_1,t_2)$可以由$\frac{1}{d}\sum_{i=1}^d cos(w_it_1)cos(w_it_2)+sin(w_it_1)sin(w_it_2)$构成。因此，Bochner定理通过下式将有限维特征映射到$\mathbb{R}^d$:

$$
t\rightarrow \Phi_d^\mathcal{B} (t) :=\sqrt{\frac{1}{d}} [cos(w_1t),sin(w_1t),\cdots,cos(w_dt),sin(w_dt)]
$$

使得 $\mathcal{K}(t_1,t_2)\approx \lim_{d \rightarrow \infty}\langle \Phi_d^\mathcal{B}(t_1),\Phi_d^\mathcal{B}(t_2)\rangle$

到目前为止,我们已经获得了一个特定的函数形式为$\Phi$,这实际上是一个随机投影到高维向量空间。随机变量由$p(w)$定义,其中每个坐标由三角函数转换。然而,目前尚不清楚如何从$w$的未知分布中采样。否则，根据(2)中的傅里叶变换，我们已经有$\mathcal{K}$了，而 Mercer 定理，则激发了一种确定性方法。

**Theorem 2**(Mercer's Theorem).Consider the function class  $L^2(\mathcal{X},\mathbb{P})$ where $\mathcal{X}$ is compact. Suppose that the kernel function $K$ is continuous with positive semidefinite and satisfy the condition $\int_{\mathcal{X}\times \mathcal{X}} \mathcal{K}^2(x,z)d\mathbb{P}(x)d\mathbb{P}(y) \le \infty$, then there exist a sequence of eigenfunctions $(\phi_i)_{i=1}^\infty$ that form an orthonormal basis of $L^2(\mathcal{X},\mathbb{P})$, and an associated set of non-negative eigenvalues $(c_i)_{i=1}^\infty$ such that:

$$
\mathcal{K}(x,z)=\sum_{i=1}^\infty c_i\phi_i(x)\phi_i(z)
$$

where the convergence of the infinite series holds absolutely and uniformly.

Mercer 定理为如何将实例从函数域$T$嵌入到无限序列空间$\mathcal{L }^2(\mathbb{N})$提供了直观的认识。具体来说，可以通过以下方式定义映射 $t\rightarrow\Phi^\mathcal{M}(t):=[\sqrt{c_1}\phi_1(t),\sqrt{c_2}\phi_2(t),\cdots]$,而 Mercer 定理保证了$\langle\Phi^\mathcal{M}(t_1),\Phi^\mathcal{M}(t_2)\rangle \rightarrow \mathcal{K}(t_1,t_2)$的收敛性.

后面的两个定理提供了重要的洞察力支撑 the functional forms of feature map $\Phi$。然而，它们仍然不适用。feature map 出于 Bochner 定理,更不用说从未知的$p(w)$抽样的不可行性,使用蒙特卡罗估计带来其他的不确定性,即一个像样的近似需要多少样本。对于Mercer定理中的feature map，首先，它是无限维的。其次，在不做额外的假设下，它不具备特定的 functional forms。上述挑战的解决方案将在接下来的两部分中进行讨论。

## 4. Bochner Time Embedding

### reparameterization trick 
为了有效地学习Bochner定理提出的 feature map，一个实用的解决方案是使用“reparameterization trick”[11]。Reparameterization技巧提供了想法,通过使用辅助变量$\epsilon$采样分布,$\epsilon$具有独立的边缘分布$p(\epsilon)$。

![表1](/assets/images/深度学习/带函数式时间表征学习的自注意力/tab1.png)

表1：提出的特征映射$\Phi=[\cdots,\phi_{2i}(t),\phi_{2i+1}(t),\cdots]$的函数形式，动机源于 Bochner 定理和 Mercer 定理,以及自由参数的解释和$w$的解释。

location-scale的家庭分布,如正态分布假设$w\sim N(\mu,\sigma)$,然后辅助随机变量$\epsilon \sim N(0,1)$,$w$可以重新参数化为$\mu+\sigma\epsilon$。现在采样$w$可以转换为采样$\epsilon$,自由分布参数$\mu,\sigma$可以作为整个学习模型的一部分进行优化。用高斯分布，Bochner定理提出的 feature map $\Phi_d^\mathcal{B}$ 可以被$\mu$和$\sigma$有效地参数化，$\mu,\sigma$也是函数$w_i(\mu,\sigma)$的输入，$w_i(\mu,\sigma)$把第$i$个样本从辅助分布转换到目标分布样本(表1),一个潜在的问题是, location-scale家庭在捕获傅里叶变换下时间模式的复杂性上可能不够丰富。实际上，以$f(x)\equiv e^{-ax^2}$的形式出现的高斯函数的傅里叶变换也是一个高斯函数。另一种方法是使用逆累积分布函数CDF变换(inverse cumulative distribution function CDF transformation.)。

### inverse CDF

让$F^{-1}$为一些概率分布的逆CDF(如果存在的话),然后对采样自均匀分布的$\epsilon$,我们可以使用$F^{-1}(\epsilon)$来生成所需分布的样本。这表明要用一些功能相近的方法参数化逆CDF函数为$F^{-1}\equiv g_\theta(\cdot)$,如神经网络或 包含 normalizing flow 和 RealNVP 的flow-based CDF 评估方法（更多讨论见附录)。事实上，如果样本是先 drawn 的(按照任意一种转换方法)，并且在训练期间保持固定，我们
可以考虑使用非参数变换。从辅助分布采样的$\{w_i\}_{i=1}^d$,
对于一些非参数化逆CDF $F^{-1}$ 让 $\widetilde{w}_i=F^{-1}(w_i),i=1,2,\cdot,d$。因为$w_i$是固定的,学习$F^{-1}$相当于直接优化转换后的样本$\{\widetilde{w}\}_{i=1}^d$为自由参数。

![表2](/assets/images/深度学习/带函数式时间表征学习的自注意力/tab2.png)

表2：勾勒出的Bochner和Mercer时间嵌入（$\Phi_d^{\mathcal{B}}(t)$和$\Phi_{w,d}^{\mathcal{M}}(t)$）的视觉插图入,$d=3$的特定的$t=t_i$。在右侧面板中，正弦和余弦波的尺度随着频率的增大而减小，这是傅立叶级数中常见的现象。

简而言之，Bochner's time feature maps 可以用重新参数化方法或参数/非参数逆CDF转换来实现。我们称之为Bochner时间编码。在表1中,我们总结了 functional forms 来进行 Bochner时间编码和提供了自由参数的解释以及$w$的含义。在表2的左边面板中提供了一个草图。最后，我们从相应的分布$p(w)$中抽取样本，给出了蒙特卡罗近似一致收敛于高概率的核函数$\mathcal{K}$的理论证明( the Monte Carlo approximation converges uniformly to the kernel function $\mathcal{K}$ with high probability. )。Claim 1中所述的上限为实现良好近似所需的样本数量提供了一些指导。

Claim 1. Let $p(w)$ be the corresponding probability measure stated in Bochner’s Theorem for kernel function $\mathcal{K}$. Suppose the feature map $\Phi$ is constructed as described above using samples $\{w_i\}_{i=1}^d$, we have

$$
\tag{5}
Pr(\sup_{t_1,t_2\in T}|\Phi_d^{\mathcal{B}}(t_1)'\Phi_d^{\mathcal{B}}(t_2)-\mathcal{K}(t_1,t_2)|\ge \epsilon) \le 4\sigma_p \sqrt{\frac{t_{\max}}{\epsilon}} exp(\frac{-d\epsilon^2}{32})
$$

where $\sigma_p^2$ is the second momentum with respect to $p(w)$.

证据在补充资料中提供。

因此，我们能用$\Omega(\frac{1}{\epsilon^2}\log\frac{\sigma_p^2 t_{\max}}{\epsilon})$来采样(at the order of hundreds if $\epsilon \thickapprox 0.1$) from $p(w)$ to have $\sup_{t_1,t_2 \in T} |\Phi_d^{\mathcal{B}}(t_1)'\Phi_d^{\mathcal{B}}(t_2) -\mathcal{K}(t_1,t_2) |<\epsilon$ with any probability.

### 5. Mercer 时间嵌入

Mercer's Theorem 解决了把时间跨度嵌入到序列空间的挑战,然而,$\Phi$的函数形式未知，空间又是无限维的。解决第一个问题,我们需要在$\mathcal{K}$的周期性质上做一个假设来满足 Proposition 1,即描述了functional mapping $\Phi(\cdot)$的一个相当简单的公式。

Proposition 1. 内核函数$\mathcal{K}$是连续的, PSD和平移不变 with $\mathcal{K}=\psi(t_1-t_2)$,假设$\psi$是频率为$w$的周期偶函数,即$\psi(t)=\psi(-t)$且$\psi(t+\frac{2k}{w})=\psi(t)$对所有$t\in [-\frac{1}{w},\frac{1}{w}]$和整数$k\in \mathbb{Z}$, $\mathcal{K}$的特征函数 given by the Fourier basis.

Proposition 1的证明见补充材料。

注意在我们的设置中，核$\mathcal{K}$不一定是周期性的。不过我们可以假定时间模式可以从一组有限的周期核$\mathcal{K}_w:T\times T \rightarrow \mathbb{R}, w \in \{w_1,\cdots,w_k\}$ 中 detected ,其中每个$\mathcal{K}_w$是连续的,平移不变的和PSD核进一步赋予一些频率$w$。换句话说，我们将未知的核函数$\mathcal{K}$投影到一组具有与$\mathcal{K}$相同属性的周期核上。

根据 Proposition 1,我们立即看到每个周期核$\mathcal{K}_{wi}$ 特征函数 stated in Mercer定理被给出：$\phi_{2j}(t)=1,\phi_{2j}(t)=cos(\frac{j\pi t}{w_i}),\phi_{2j+1}(t)=sin(\frac{j\pi t}{w_i})$,其中 $j=1,2,\cdots$, with $c_i,i=1,2,\cdots$给出相应的傅里叶系数。因此对每个$\mathcal{K}_w$我们有无限维的 Mercer 的 feature map:

$$
t \rightarrow \Phi_w^{\mathcal{M}}(t) = [\sqrt{c_1},\cdots,\sqrt{c_{2j}}cos(\frac{j\pi t}{w}),\sqrt{c_{2j+1}}sin(\frac{j\pi t}{w}),\cdots]
$$

where we omit(省略) the dependency（依赖） of all $c_j$ on $w$ for notation(符号) simplicity(简单性).

通过傅里叶级数表达$\mathcal{K}_w$的一个重要优点是,他们经常有漂亮的 truncation properties(截断特性),它允许我们使用没有失去太多信息的 truncated feature map。结果表明，在温和的条件下，傅里叶系数$c_j$指数衰减到零[21]，经典的近似理论保证了截断傅里叶级数[9]的一致收敛边界(讨论见附录)。因此,我们建议使用 truncated feature map $\Phi_{w,d}^\mathcal{M}(t)$,从而完成 Mercer 的时间嵌入:

$$
t \rightarrow \Phi_d^{\mathcal{M}} = [\Phi_{w_1,d}^{\mathcal{M}}(t),\cdots,\Phi_{w_k,d}^{\mathcal{M}}(t)]^\top
$$

因此，Mercer’s feature map embeds the periodic(周期) kernel function into the high-dimensional space spanned by truncated Fourier basis under certain frequency。对于未知的傅里叶系数$c_j$,很明显,学习核函数$\mathcal{K}_w$形式上相当于学习相应的系数。为了避免不必要的并发症，我们将$c_j$作为自由参数。

最后但并非最不重要,我们指出的频率集$\{w_1,\cdots,w_k\}$,指定每个周期核函数应该能够覆盖广泛的带宽为了捕捉各种信号,实现好的近似。它们可以作为自由参数进行固定或联合优化。在我们的实验中他们导致类似的 performances 如果正确地初始化,比如使用几何序列:$w_i = w_{\max} - (w_{\max}-w_{\min})^{i/k},i=1,\cdots,k$覆盖$[w_{\min},w_{\max}]$ with a focus on high-frequency regions。在表2的右面板中提供了草图的视觉说明。

## 6. 时间事件交互

学习时间事件相互作用是连续时间事件序列预测的关键。在有限维向量空间中嵌入时间跨度之后，我们能够直接使用时间和事件嵌入来建模交互。有必要首先将时间和事件表征投射到相同的空间中。对于一个事件序列 $\{(e_1,t_1),\cdots,(e_q,t_q)\}$,我们拼接事件和时间表征为$[Z,Z_T]$，其中$Z=[Z_1,\cdots,Z_q],Z_T = [\Phi(t_1),\cdots,\Phi(t_q)]$,把他们投影到 query,key 和 value 空间。例如，为了只考虑query空间中事件和时间表征的线性组合，我们可以简单地使用$Q=[Z,Z_t]W_0+b_0$。为了在层次上捕获非线性关系，我们可以考虑使用多层感知器(MLP)和激活函数，例如

$$
Q = ReLU([Z,Z_t]W_0+b_0)W_1+b_1
$$

其中$ReLU(\cdot)$是 rectified linear unit.还可以添加残差块，将有用的低层信息传播到最终输出。当预测下一个时间依赖事件$(e_{q+1},t_{q+1})$时，考虑到每个输入序列的事件和目标事件之间的时间滞后，我们让$\widetilde{t}_i=t_{q+1}-t_i,i=1,\cdots,q$ 使用$\Phi(\widetilde{t}_i)$作为时间表征，这样做不改变输入事件间的时间差，即：$\widetilde{t}_i - \widetilde{t}_j = t_i-t_j$,其中$i,j=1,\cdots,q$,现在注意力权重和预测变成了下一个发生时间的函数。

## 7. 实验和结果

我们评估了所提出的时间嵌入方法在不同领域的真实数据集上的性能。实验旨在定量评价四种时间嵌入方法的性能，并与基线模型进行比较。

### 7.1 数据集

* Stack Overflow 数据集记录用户的历史在一个问答中被授予徽章
的网站。任务是预测用户收到的下一个徽章，作为一个分类任务。
* MovieLens是一个公共数据集，包含用于基准推荐算法[7]的电影评级。任务是预测用户推荐的下一部电影。
* Walmart.com dataset 来自沃尔玛在美国的在线电子商务平台。它包含基于会话的搜索、视图、add-to-cart和事务信息，以及来自所选用户的每个操作的时间戳。任务是预测用于推荐的下一个视图项。所有数据集的详细信息在补充材料中提供。

**数据准备**

为了与基线进行公平的比较，对于MovieLens数据集，我们遵循[10]中提到的相同预处理步骤。对于至少评价了三部电影的用户，我们使用他们的第二个最后评价作为验证，使用他们最后评价的电影进行测试。对于 stack overflow 数据集，我们使用与[12]中描述的相同的过滤过程，并将用户的数据集随机分为训练(80%)、验证(10%)和测试(10%)。在Walmart.com的数据集中，我们过滤掉了与少于5个用户交互的活动和产品少于10个的用户。训练、验证和测试数据根据会话开始时间进行划分。

### 7.2 基线和模型配置

我们将提出的方法与LSTM、时间感知的RNN模型(TimeJoint)[12]和 stack overflow 数据集上的循环标记时间点过程模型(recurrent marked temporal point process model ,RMTPP)[6]进行了比较。我们指出后两种方法也利用了时间信息。对于以上三种模型，我们对相同的stack overflow 数据集重用了[12]中报告的优化模型配置和指标(分类正确率)。

对于MovieLens数据集上的推荐任务，我们选择了基于会话的RNN推荐模型(GRU4Rec)[8]、卷积序列嵌入方法(Caser)[19]和基于翻译的推荐模型(TransRec)[10]作为基线。这些位置感知顺序模型已经被证明可以在相同的MovieLens数据集[10]上实现前沿性能。我们还重用指标——top K命中率(Hit@K)和标准化折现累积增益(NDCG@K)，以及在[10]中报告的优化模型配置。

在Walmart.com的数据集上，除了GRU4Rec和TransRec，我们与基于注意力的RNN模型RNN+attn进行了比较。根据验证数据集上的Hit@10度量，将基线的超参数调优以获得最佳性能。结果见表3。

作为提出时间嵌入方法,我们尝试 Bochner 时间嵌入 用 reparameterization技巧使用正态分布( Bochner Normal),参数逆CDF变换(Bochner Inv CDF) with MLP,MLP+residual block,masked autoregressive flow(MAF)和non-volume preserving transformations(NVP)[5],无参逆CDF变换(Bochner Non-para),以及Mercer 时间嵌入。在消融研究中，我们将所有任务与原始位置编码自注意力(PosEnc)进行比较(表3)。Bochner和Mercer时间嵌入均使用$d=100$，时间嵌入维度的敏感性分析见附录。我们将Mercer时间嵌入的傅立叶基$k$的维数作为超参数，并根据验证Hit@10从{1,5,10,15,20,25,30}中进行选择。在报告表3中的结果时，我们标记了模型配置，从而为每个时间嵌入方法带来最佳的验证性能。其他配置和训练细节在附录中提供。

### 7.3 实验结果

我们在表3中观察到，与所有三个数据集上的基线模型相比，提议的用自注意力的时间嵌入更好。对于Stack Overflow和Walmart.com数据集，Mercer 时间嵌入实现了最好的性能，而对于MovieLens数据集，Bochner Non-para优于其余方法。结果表明 functional time representation 的有效性，与位置编码的比较表明时间嵌入更适合于连续时间事件序列的建模。另一方面，Bochner Normal和Bochner Inv CDF的方差似乎较高，这可能是训练过程中需要抽样步骤造成的。此外，Bochner Inv CDF在所有三个数据集的表现与Bochner Non-para相当。一般而言，我们观察到Bochner Non-para 时间嵌入和Mercer时间嵌入有较佳的表现。具体来说，通过调优的傅里叶基$k$, Mercer的方法在所有任务中的表现始终优于其他方法。嵌入的时间维度$d$控制$[w_{\min},w_{\max}]$的带宽被覆盖的如何,$k$控制在每个频率下傅里叶基的自由度。当$d$固定时，$k$越大，在一定频率下，时间核可能会出现过拟合问题，图1b中$k$的敏感性分析证实了这一点。

在图2中，我们将整个人群的平均注意力权重可视化为时间和用户动作或产品部门在Walmart.com数据集上的函数，以演示Mercer时间嵌入捕获的一些有用的时间模式。例如，图2a显示，当推荐下一个产品时，模型学会将更多的注意力放在最后搜索的产品上。类似地，图2b中的模式表明，该模型捕捉到的信号是，与电子产品和配件相比，客户通常会长期或反复关注婴儿用品。有趣的是,当关注权重,通过预测未来时间点作为输入(图2 c),我们看到我们的模型预测,用户几乎完全失去关注他们最近购买的产品(这是合理的),经过较长时间没有以前互动产品问题了。

**讨论** 通过采用最先进的CDF学习方法，Bochner Inv CDF实现了比 Movlielens 和 Walmart.com 数据集上的位置编码和其他基线更好的性能(图1a)。这表明用更高的模型复杂性来学习Bochner's Thm的$p(w)$的重要性,也解释了为什么 Bochner Normal 失败,因为正态分布在捕捉复杂的分布信号能力有限。另一方面，Bochner Non-para实际上是Mercer方法的特例，$k = 1$且没有截距。Bochner的方法源于随机特征抽样，而 Mercer 的方法基于 functional basis 展开。在实践中，由于Mercer方法不依赖于分布学习和抽样，我们可能期望它能提供更稳定的性能。然而，随着贝叶斯深度学习和概率计算的发展，我们也可以期望Bochner Inv CDF使用合适的分布学习模型进行适当的工作，这将留给未来的工作。

## 8. 结论

我们提出了一套 time embedding methods for functional time representation learning, 并证明了它们在连续时间事件序列预测中与自注意力一起使用时的有效性。所提出的方法有充分的理论依据，它们不仅揭示了时间模式，而且还捕捉了时间-事件的相互作用。通过实际数据集的实验，验证了所提出的时间嵌入方法的有效性，发现采用 Mercer时间嵌入和 non-parametric inverse CDF变换的Bochner时间嵌入具有较好的性能。我们指出所提出的方法可推广到一般的时间表征学习，并在未来的工作中探讨如何将所提出的方法应用于其他的情境，例如时间图表征学习和强化学习。

## 引用

1. D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
2. Y.Bengio,A.Courville,andP.Vincent.Representationlearning:Areviewandnewperspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.
3. L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S. Chua. Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5659–5667, 2017.
4. J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech recognition. In Advances in neural information processing systems, pages 577–585, 2015.
5. L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.
6. N. Du, H. Dai, R. Trivedi, U. Upadhyay, M. Gomez-Rodriguez, and L. Song. Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1555–1564. ACM, 2016.
7. F. M. Harper and J. A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst., 5(4):19:1–19:19, Dec. 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL http://doi.acm.org/10.1145/2827872.
8. B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.
9. D. Jackson. The theory of approximation, volume 11. American Mathematical Soc., 1930.
10. W.-C. Kang and J. McAuley. Self-attentive sequential recommendation. In 2018 IEEE Interna-
tional Conference on Data Mining (ICDM), pages 197–206. IEEE, 2018.
11. D.P.KingmaandM.Welling.Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114,2013.
12. Y.Li,N.Du,andS.Bengio.Time-dependentrepresentationforneuraleventsequenceprediction. arXiv preprint arXiv:1708.00065, 2017.
13. L. H. Loomis. Introduction to abstract harmonic analysis. Courier Corporation, 2013.
14. H. Mei and J. M. Eisner. The neural hawkes process: A neurally self-modulating multivariate point process. In Advances in Neural Information Processing Systems, pages 6754–6764, 2017.
15. J.Mercer.Xvi.functionsofpositiveandnegativetype,andtheirconnectionthetheoryofintegral equations. Philosophical transactions of the royal society of London. Series A, containing papers of a mathematical or physical character, 209(441-458):415–446, 1909.
16. G.Papamakarios,T.Pavlakou,andI.Murray.Maskedautoregressiveflowfordensityestimation. In Advances in Neural Information Processing Systems, pages 2338–2347, 2017.
17. A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177–1184, 2008.
18. D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.
19. J.TangandK.Wang.Personalizedtop-nsequentialrecommendationviaconvolutionalsequence embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 565–573. ACM, 2018.
20. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
21. H. Widom. Asymptotic behavior of the eigenvalues of certain integral equations. ii. Archive for Rational Mechanics and Analysis, 17(3):215–229, 1964.
22. S. Xiao, J. Yan, M. Farajtabar, L. Song, X. Yang, and H. Zha. Joint modeling of event sequence and time series with attentional twin recurrent neural networks. arXiv preprint arXiv:1703.08524, 2017.
23. S. Xiao, J. Yan, X. Yang, H. Zha, and S. M. Chu. Modeling the intensity function of point process via recurrent neural networks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
24. D. Xu, C. Ruan, E. Korpeoglu, S. Kumar, and K. Achan. Context-aware dual representation learning for complementary products recommendation. arXiv preprint arXiv:1904.12574v2, 2019.
25. K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015.
26. Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec. Seismic: A self-exciting point process model for predicting tweet popularity. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1513–1522. ACM, 2015.
27. Y. Zhu, H. Li, Y. Liao, B. Wang, Z. Guan, H. Liu, and D. Cai. What to do next: Modeling user behaviors by time-lstm. In IJCAI, pages 3602–3608, 2017.

## 附录（去参考论文看吧）

---
**参考**：
1. 论文：Da Xu, Chuanwei Ruan, Sushant Kumar, Evren Korpeoglu, Kannan Achan. [Self-attention with Functional Time Representation Learning](https://arxiv.org/abs/1911.12864)

