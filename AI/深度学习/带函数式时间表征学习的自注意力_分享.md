---
typora-root-url: ../../
---

# Self-attention with Functional Time Representation Learning

# 带函数式时间表征学习的自注意力_分享

## 个人总结：

传统自注意力在考虑序列问题时，利用位置编码，但位置编码只能解决离散问题，对于连续时间无法解决。而且用位置编码的注意力机制也不能识别出时间上的远近信息。

该文就是为了解决连续时间的编码问题，将连续性的时间跨度信息编码到高维空间，同时保证在高维空间中，依然具有时间的平移不变性，即时间的起点可以不同，但表达的规律是相同的。同时将这个编码与自注意力机制结合使用。因为传统自注意力机制采用的是Q和K的内积计算权重。所以本文所采用的解决方案是构造核函数，根据核函数及一些定理要求设计出了几种时间映射函数，将时间跨度信息映射到高维空间。将时间编码和事件编码拼接进行后续自注意力。

根据 Bochner 定理，设计出3种映射函数
1. reparameterization trick 
2. parametric inverse CDF 
3. non-parametric inverse CDF 

根据 Mercer 定理,设计出1种映射函数

具体映射及参数可见内容中表1

对于Stack Overflow和Walmart.com数据集，Mercer 时间嵌入实现了最好的性能，而对于MovieLens数据集，Bochner Non-para优于其余方法。

## 论文内容

## 论文想法
Let the kernel be $\mathcal{K}:T\times T \rightarrow \mathbb{R}$ where $\mathcal{K}(t_1,t_2) := \langle \Phi(t_1),\Phi(t_2)\rangle$  and$\mathcal{K}(t_1,t_2) = \psi(t_1-t_2),\forall t_1,t_2 \in T$ for some $\psi:[-t_{\max},t_{\max}] \rightarrow \mathbb{R}$. Here the feature map $\Phi$ captures how kernel function embeds the original data into a higher dimensional space。 Notice that the kernel function $\mathcal{K}$ is positive semidefinite (PSD) since we have expressed it with a Gram matrix. Without loss of generality we assume that $\Phi$ is continuous, which indicates that $\mathcal{K}$ is translation-invariant, PSD and also continuous.


So the task of learning temporal patterns is converted to a kernel learning problem with $\Phi$ as feature map. By relating time embedding to kernel function learning, we hope to identify $\Phi$ with some functional forms which are compatible with current deep learning frameworks, such that computation via bask-propagation is still feasible. Classic functional analysis theories provides key insights for identifying candidate functional forms of $\Phi$. We first state Bochner’s Theorem and Mercer’s Theorem and briefly discuss their implications.

## Bochner 定理

**Theorem 1**（Bochner's Theorem）A continuous, translation-invariant kernel  $\mathcal{K}(x,y)=\psi(x-y)$ on $\mathbb{R}^d$ is positive definite if and only if there exists a non-negative measure（测度） on $\mathbb{R}$ such that $\psi$ is the Fourier transform of the measure.

The implication of Bochner’s Theorem is that when scaled properly we can express $\mathcal{K}$ with:

$$
\tag{2}
\mathcal{K}(t_1,t_2) = \psi(t_1,t_2) = \int_{\mathbb{R}} e^{iw(t_1-t_2)}p(w)dw = E_w[\xi_w(t_1)\xi_w(t_2)^*]
$$

where $\xi_w(t)=e^{iwt}$. Since the kernel $\mathcal{K}$  and the probability measure $p(ω)$ are real, we extract the real part of (2) and obtain:

$$
\tag{3}
\mathcal{K}(t_1,t_2) = E_w[cos(w(t_1-t_2))] = E_w[cos(wt_1)cos(wt_2)+sin(wt_1)sin(wt_2)]
$$

With this alternate expression of kernel function $K$, the expectation term can be approximated by Monte Carlo integral [17]. Suppose we have $d$ samples $w_1,\cdots,w_d$ drawn from $p(ω)$, an estimate of our kernel $\mathcal{K}(t_1,t_2)$ can be constructed by $\frac{1}{d}\sum_{i=1}^d cos(w_it_1)cos(w_it_2)+sin(w_it_1)sin(w_it_2)$. As a consequence, Bochner’s Theorem motivates the finite dimensional feature map to $\mathbb{R}^d$ via:

$$
t\rightarrow \Phi_d^\mathcal{B} (t) :=\sqrt{\frac{1}{d}} [cos(w_1t),sin(w_1t),\cdots,cos(w_dt),sin(w_dt)]
$$

such that $\mathcal{K}(t_1,t_2)\approx \lim_{d \rightarrow \infty}\langle \Phi_d^\mathcal{B}(t_1),\Phi_d^\mathcal{B}(t_2)\rangle$

So far we have obtained a specific functional form for $\Phi$, which is essentially a random projection onto the high-dimensional vector space of i.i.d random variables with density given by $p(w)$, where each coordinate is then transformed by trigonometric functions. However, it is not clear how to sample from the unknown distribution of $w$. Otherwise we would already have $\mathcal{K}$ according to the Fourier transformation in (2). Mercer’s Theorem, on the other hand, motivates a deterministic approach.

## Mercer 定理

**Theorem 2**(Mercer's Theorem).Consider the function class  $L^2(\mathcal{X},\mathbb{P})$ where $\mathcal{X}$ is compact. Suppose that the kernel function $K$ is continuous with positive semidefinite and satisfy the condition $\int_{\mathcal{X}\times \mathcal{X}} \mathcal{K}^2(x,z)d\mathbb{P}(x)d\mathbb{P}(y) \le \infty$, then there exist a sequence of eigenfunctions $(\phi_i)_{i=1}^\infty$ that form an orthonormal basis of $L^2(\mathcal{X},\mathbb{P})$, and an associated set of non-negative eigenvalues $(c_i)_{i=1}^\infty$ such that:

$$
\mathcal{K}(x,z)=\sum_{i=1}^\infty c_i\phi_i(x)\phi_i(z)
$$

where the convergence of the infinite series holds absolutely and uniformly.

Mercer’s Theorem provides intuition on how to embed instances from our functional domain $T$ into the infinite sequence space $\mathcal{L }^2(\mathbb{N})$. To be specific, the mapping can be defined via $t\rightarrow\Phi^\mathcal{M}(t):=[\sqrt{c_1}\phi_1(t),\sqrt{c_2}\phi_2(t),\cdots]$, and Mercer’s Theorem guarantees the convergence of $\langle\Phi^\mathcal{M}(t_1),\Phi^\mathcal{M}(t_2)\rangle \rightarrow \mathcal{K}(t_1,t_2)$


The two theorems have provided critical insight behind the functional forms of feature map $\Phi$. However, they are still not applicable. For the feature map motivated by Bochner’s Theorem, let alone the infeasibility of sampling from unknown p(ω), the use of Monte Carlo estimation brings other uncertainties, i,e how many samples are needed for a decent approximation. As for the feature map from Mercer’s Theorem, first of all, it is infinite dimensional. Secondly, it does not possess specific functional forms without making additional assumptions. The solutions to the above challenges are discussed in the next two sections.

## Bochner Time Embedding

Reparameterization trick provides ideas on sampling from distributions by using auxiliary variable $\epsilon$ which has known independent marginal distribution $p(\epsilon)$.

### reparameterization trick 

![表1](/assets/images/深度学习/带函数式时间表征学习的自注意力/tab1.png)

表1：提出的特征映射$\Phi=[\cdots,\phi_{2i}(t),\phi_{2i+1}(t),\cdots]$的函数形式，动机源于 Bochner 定理和 Mercer 定理,以及自由参数的解释和$w$的解释。

For ’location-scale’ family distribution such as Gaussian distribution, suppose $w\sim N(\mu,\sigma)$, then with the auxiliary random variable $\epsilon \sim N(0,1)$, $w$ can be reparametrized as $\mu+\sigma\epsilon$. Now samples of $w$ are transformed from samples of $\epsilon$, and the free distribution parameters $\mu$ and $\sigma$ can be optimized as part of the whole learning model.With Gaussian distribution,the feature map $\Phi_d^\mathcal{B}$ suggestedby Bochner’s Theorem can be effectively parameterized by $\mu$ and $\sigma$, which are also the inputs to the functions $w_i(\mu,\sigma)$ that transforms the $i$th sample from the auxiliary distribution to a sample of target distribution (Table 1). A potential concern here is that the ’location-scale’ family may not be rich enough to capture the complexity of temporal patterns under Fourier transformation. Indeed, the
Fourier transform of a Gaussian function in the form of $f(x)\equiv e^{-ax^2}$ is another Gaussian function. An alternate approach is to use inverse cumulative distribution function CDF transformation.

### inverse CDF

Let $F^{-1}$ be the inverse CDF of some probability distribution (if exists), then for $\epsilon$ sampled from uniform distribution, we can always use $F^{-1}(\epsilon)$ to generate samples of the desired distribution. This suggests parameterizing the inverse CDF function as $F^{-1}\equiv g_\theta(\cdot)$ with some functional approximators such as neural networks or flow-based CDF estimation methods including normalizing flow [18] and RealNVP [5] (see the Appendix for more discussions). As a matter of fact, if the samples are first drawn (following either transformation method) and held fixed during training, we can consider using non-parametric transformations. For$\{w_i\}_{i=1}^d$ sampled from auxiliary distribution, let $\widetilde{w}_i=F^{-1}(w_i),i=1,2,\cdot,d$, for some non-parametric inverse CDF $F^{-1}$ . Since $w_i$ are fixed, learning $F^{-1}$  amounts to directly optimize the transformed samples $\{\widetilde{w}\}_{i=1}^d$ as free parameters.

![表2](/assets/images/深度学习/带函数式时间表征学习的自注意力/tab2.png)

表2：勾勒出的Bochner和Mercer时间嵌入（$\Phi_d^{\mathcal{B}}(t)$和$\Phi_{w,d}^{\mathcal{M}}(t)$）的视觉插图入,$d=3$的特定的$t=t_i$。在右侧面板中，正弦和余弦波的尺度随着频率的增大而减小，这是傅立叶级数中常见的现象。

In short, the Bochner’s time feature maps can be realized with reparametrization trick or parametric/nonparametric inverse CDF transformation. We refer to them as Bochner time encoding. In Table 1, we conclude the functional forms for Bochner time encoding and provides explanations of the free parameters as well as the meanings of $w$. A sketched visual illustration is provided in the left panel of Table 2. 

## Mercer Time Embedding

Mercer's Theorem 解决了把时间跨度嵌入到序列空间的挑战,然而,$\Phi$的函数形式未知，空间又是无限维的。解决第一个问题,我们需要在$\mathcal{K}$的周期性质上做一个假设来满足 Proposition 1,即描述了functional mapping $\Phi(\cdot)$的一个相当简单的公式。

Proposition 1. For kernel function $\mathcal{K}$ that is continuous, PSD and translation-invariant with $\mathcal{K}=\psi(t_1-t_2)$, suppose $\psi$ is a even periodic function with frequency $w$, i.e $\psi(t)=\psi(-t)$ and $\psi(t+\frac{2k}{w})=\psi(t)$ for all $t\in [-\frac{1}{w},\frac{1}{w}]$ and integers $k\in \mathbb{Z}$ the eigenfunctions of $\mathcal{K}$ are given by the Fourier basis.


Proposition 1. 内核函数$\mathcal{K}$是连续的, PSD和平移不变 with $\mathcal{K}=\psi(t_1-t_2)$,假设$\psi$是频率为$w$的周期偶函数,即$\psi(t)=\psi(-t)$且$\psi(t+\frac{2k}{w})=\psi(t)$对所有$t\in [-\frac{1}{w},\frac{1}{w}]$和整数$k\in \mathbb{Z}$, k的特征函数 given by the Fourier basis.


注意在我们的设置中，核$\mathcal{K}$不一定是周期性的。不过我们可以假定时间模式可以从一组有限的周期核$\mathcal{K}_w:T\times T \rightarrow \mathbb{R}, w \in \{w_1,\cdots,w_k\}$ 中 detected ,其中每个$\mathcal{K}_w$是连续的,平移不变的和PSD核进一步赋予一些频率$w$。换句话说，我们将未知的核函数$\mathcal{K}$投影到一组具有与$\mathcal{K}$相同属性的周期核上。

根据 Proposition 1,我们立即看到每个周期核$\mathcal{K}_{wi}$ 特征函数 stated in Mercer定理被给出：$\phi_{2j}(t)=1,\phi_{2j}(t)=cos(\frac{j\pi t}{w_i}),\phi_{2j+1}(t)=sin(\frac{j\pi t}{w_i})$,其中 $j=1,2,\cdots$, with $c_i,i=1,2,\cdots$给出相应的傅里叶系数。因此对每个$\mathcal{K}_w$我们有无限维的 Mercer 的 feature map:

$$
t \rightarrow \Phi_w^{\mathcal{M}}(t) = [\sqrt{c_1},\cdots,\sqrt{c_{2j}}cos(\frac{j\pi t}{w}),\sqrt{c_{2j+1}}sin(\frac{j\pi t}{w}),\cdots]
$$

where we omit(省略) the dependency（依赖） of all $c_j$ on $w$ for notation(符号) simplicity(简单性).

通过傅里叶级数表达$\mathcal{K}_w$的一个重要优点是,他们经常有漂亮的 truncation properties(截断特性),它允许我们使用没有失去太多信息的 truncated feature map。结果表明，在温和的条件下，傅里叶系数$c_j$指数衰减到零[21]，经典的近似理论保证了截断傅里叶级数[9]的一致收敛边界(讨论见附录)。因此,我们建议使用 truncated feature map $\Phi_{w,d}^\mathcal{M}(t)$,从而完成 Mercer 的时间嵌入:

$$
t \rightarrow \Phi_d^{\mathcal{M}} = [\Phi_{w_1,d}^{\mathcal{M}}(t),\cdots,\Phi_{w_k,d}^{\mathcal{M}}(t)]^\top
$$

因此，Mercer’s feature map embeds the periodic(周期) kernel function into the high-dimensional space spanned by truncated Fourier basis under certain frequency。对于未知的傅里叶系数$c_j$,很明显,学习核函数$\mathcal{K}_w$形式上相当于学习相应的系数。为了避免不必要的并发症，我们将$c_j$作为自由参数。

最后但并非最不重要,我们指出的频率集$\{w_1,\cdots,w_k\}$,指定每个周期核函数应该能够覆盖广泛的带宽为了捕捉各种信号,实现好的近似。它们可以作为自由参数进行固定或联合优化。在我们的实验中他们导致类似的 performances 如果正确地初始化,比如使用几何序列:$w_i = w_{\max} - (w_{\max}-w_{\min})^{i/k},i=1,\cdots,k$覆盖$[w_{\min},w_{\max}]$ with a focus on high-frequency regions。在表2的右面板中提供了草图的视觉说明。