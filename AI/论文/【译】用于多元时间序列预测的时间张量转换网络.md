# 【译】用于多元时间序列预测的时间张量转换网络

**摘要**: 多元时间序列预测具有广泛的应用领域，被认为是一项非常具有挑战性的任务，特别是当变量之间存在相关性并表现出复杂的时间模式时，如季节性和趋势。现有的许多方法都存在很强的统计假设、高维数的数值问题、手工特征工程和可伸缩性。在这项工作中，我们提出了一个新的深度学习架构，称为时间张量转换网络(Temporal Tensor Transformation Network)，它通过提出的时间切片叠加转换把原来的多元时间序列转换成一个更高阶的张量。这就产生了一个新的多元时间序列的表征，它使得卷积核能够从一个相对较大的时间区域提取复杂的非线性特征以及可变的相互作用信号。实验结果表明，时间张量转换网络在基于窗口的各种任务预测方面的性能优于现有的几种方法。通过广泛的敏感性分析，所提出的体系结构也展示了稳健的预测性能。

索引词——多元时间序列，预测，卷积，深度学习，张量转换

## I. 介绍

多元时间序列分析在金融市场预测、天气预报、能源消费预测等领域得到了广泛的应用。它被用来模拟和解释动力系统中一组时间序列变量之间的潜在时间模式。基于统计建模和深度神经网络的多元时间序列预测方法已被提出。

经典的统计模型假设时间序列是平稳的。即，数据点的汇总统计信息是一致的。预处理过程通常需要从原始序列中去除趋势、季节性和其他时间相关的结构，以使数据平稳。此外，这些模型还假定了潜在线性回归问题的独立性条件，即，模型中的随机误差不随时间相关。自相关函数和部分自相关函数通常用于确定变量的适当顺序。构建有统计意义的预测模型需要执行各种预处理、转换和特征工程，这些工作耗时且难以扩展。另一方面，基于深度学习的方法，如递归神经网络，通过使用有状态模型，展示了时间序列数据建模的最新性能。最近，卷积神经网络(CNN)也被应用于预测多元时间序列。具体来说，CNN被用作无状态模型，直接从原始时间序列中提取特征并生成预测，或者被用作RNN体系结构中特征提取步骤的一部分。

现有的利用CNN进行多元时间序列预测的工作将时间序列作为图像。例如，变量（“变量”和“特征”这两个术语在本文中可以互换使用）的数量等于图像的宽度，而时间步长的数量等于图像的长度。卷积是在时间变量的层面上进行的。在这种情况下，卷积运算的一个关键基础结构前提是假设数据的局部空间相关的拓扑，而不是所有输入都被联合建模的稠密层。也就是说，在卷积层中，神经元接收来自前一层的受限子区域(即接受域)的输入，而稠密层中的神经元接收来自前一层的整个输入。因此，当CNN内核在时间变量上进行卷积时，它只能在给定的时间窗口内观察到一组狭窄的变量交互作用。另外，当我们把时间序列中的每个特征看作一个独立的通道时，卷积运算也会遇到类似的问题，即当过滤器对数据进行卷积时，内核只能观察到一个很小的时间步长的局部窗口。在这两个场景中，接受域的有限视图呈现的是时间序列的局部焦点。与图像处理不同的是，不同区域的对象可能非常不同，时间序列数据往往相对“同质”。未来值的预测更多地依赖于历史时间窗口内的全局模式，而不是局部模式。本文提出了一种新的多元时间序列预测神经网络结构，该结构具有一类新的转换函数，即时间切片堆叠转换（temporal-slicing stack transformation）。这些操作将原始的输入数据结构转换成一个高阶张量，在这个张量中，时间序列中的各个特征被从一个一维时间序列重新排列成一个二维矩阵。这种转变扩展了接受视野。因此，卷积是在一个更大的时间区域内进行的，这可能有助于捕获与时间相关的特征，如趋势和季节性，以及在更长的范围内的变量交互。


![图1](/assets/images/深度学习/用于多元时间序列预测的时间张量转换网络/fig1.png)

图1： 时间切片堆叠转换过程的可视化

![图2](/assets/images/深度学习/用于多元时间序列预测的时间张量转换网络/fig2.png)

图2：比较自相关图和时间张量转换结果

图1演示了时间切片堆叠转换，其中使用一个窗口在2D时间序列数据上滑动并提取时间序列切片的集合。然后这些切片相应地叠在一起，形成一个三维张量。具体来说，这三个维度是“特征”（features）、“时间”(time)和“堆叠”(stack)。对于多元时间序列，该转换将二维时间序列转换为三维张量。对于单变量时间序列，转换将呈现一个2D矩阵。由此产生的结构产生一个紧急模式(emergent pattern)，其中空间特征提取器(如CNN)可以显式地对复杂的非线性自相关类特征建模。为了说明这一点，我们以单变量时间序列的转换为例。图2显示了来自四个不同数据集的单变量时间序列的转换结果。并给出了相应的自相关图。这种转换显示了高度自相关的时间序列的强模式。与原始的一维时间序列相比，二维矩阵具有更丰富的信息，因此可以实现更丰富的特征提取。

本文的以下章节组织如下。我们在第二节中重点介绍了各种用于时间序列分析的统计和深度学习方法。时间张量转换、时间切片堆叠转换和所提出的模型结构的定义在第三节中进行了描述。实验设置、数据集、度量和结果见第四节。第五节，我们使用合成数据集进行基于实验的敏感性分析，并控制输入和输出维度。最后，我们在第六部分总结并讨论未来的发展方向。

## II. 相关工作

在本节中，我们回顾了多元时间序列预测的前期工作。我们首先介绍了一些经典的统计方法，然后介绍了一些最先进的深度学习方法的时间序列建模。

时间序列预测在单变量和多变量数据的各种统计建模方法中得到了很好的研究。对于单变量时间序列建模，通常使用Box-Jenkins建模流程[1]应用自回归综合移动平均(ARIMA)模型。ARIMA模型的其他变体被用来对数据的时间模式建模，例如季节性(SARMA)和相关系数周期性(PARMA)[2]。此外，ARIMA和神经网络相结合的方法也被发展来建模线性和非线性的动力学[3]。对于多元时间序列预测，基于向量的方法，如向量自回归(VAR)方法，扩展了用于单变量时间序列[1][4]建模的自回归(AR)模型。VAR模型的变体也被提出，如VARMAX模型[5]和VARX模型[6]，其中模型包含了原VAR模型的属性，共同学习给定变量之间的相互作用。此外，作为一种非参数统计模型，高斯过程[7]被用来预测连续变量的分布，而不是上述参数方法。当时间序列中的变量数量较大时，统计模型往往具有很高的计算复杂度和面临的数值问题。

随着深度学习的出现，提出了一种新的用于多元时间序列预测的神经网络结构。Borovykh等人[8]开发了基于WaveNet架构[9]的多元时间序列模型，该模型最初是为语音音频信号处理而设计的。他们通过简化和优化其核心算法，使用膨胀卷积（dialated convolution）来捕获具有噪声信号的长期多元时间数据，从而增强了原有的体系结构。Lai等人[10]提出了另一种多元时间序列建模框架LSTNet，该框架将CNN和RNN相结合，从时间序列中提取出具有层次结构的短期和长期时间依赖关系。除了模型依赖之外，LSTNet还将模型的自回归组件视为CNN和LSTM组件之间的残差连接。Qin等人[11]开发了一种用于多变量时间序列建模的双阶段基于注意力的神经网络架构，该架构利用两组RNN作为基于编码器和解码器的架构。该方法通过基于注意力的网络结构的各个阶段，兼顾特征和时间维度，自适应地选择相关的驱动序列。

## III. 模型架构
在这一部分，我们提出了时间张量变换网络结构。首先，我们介绍了本文使用的关键符号以及多元时间序列预测的形式问题定义。然后介绍了时间张量变换运算。最后，我们提出了神经网络结构.

A. 符号

我们用$x$表示一维向量，用$X$表示二维矩阵，用$\mathcal{X}$表示三维张量。相对于其相应索引的标量用小写字母表示，后面跟着下标的字母三联体。例如，一个三维张量$\mathcal{X}$索引第$(i,j,k)$位置的标量值为$x_{i,j,k}$。当它们出现在上下文中时，将引入并定义更多的符号和变量。

B. 问题定义

我们现在正式定义多元时间序列预测问题。给定一个完整的多元时间序列数据集$X=\{x_1,x_2,\cdots,x_n\}$,其中$x\in \mathbb{R}^{m\times n}$, $m$为特征数，$n$为多元序列中时间步长的总数，我们的目标是预测一个未来的值序列，直到一个定义的水平窗口。具体地说，给定时间步长$T$之前的时间序列的一个子集，我们的模型，记为函数$F(\cdot)$，将接受一个输入序列$X_T = \{x_1,x_2,\cdots,x_T\} \in \mathbb{R}^{m\times T}$,输出序列$\hat{Y}_{T+h} \in \mathbb{R}^{m\times h}$，其中$h$为水平窗口。因此，模型可以表示为映射函数$F(X_T)=\hat{Y}_{T+h}$.

C. 时间张量转换（Temporal Tensor Transformation）

建议的转换通过向原始数据输入添加一个高阶维度来扩展原始数据。例如，如果输入序列数据值为一维向量(即，单变量时间序列）转换后的数据结构为二维矩阵结构。同样，如果输入是二维矩阵(即，多元时间序列），得到的结构将是一个三维张量。

我们把时间张量转换定义为一个映射函数$TT:X\rightarrow \tilde{\mathcal{X}}$其中$X\in \mathbb{R}^{m\times T}$为输入的多元时间序列，由此变换得到一个三维张量$\tilde{\mathcal{X}}\in \mathbb{R}^{m\times \omega\times o}$。其中,$\omega$为切片窗口大小(即，时间窗口内的步数)，$o$为最终转换张量的切片数或栈高。可以根据超参数计算$o$的值。我们特定的时间张量转换被称为时间切片堆叠转换（Temporal-Slicing Stack operation）。在介绍时间切片堆叠操作之前，我们将首先介绍切片过程中涉及的超参数。

窗口大小，用$\omega$表示，定义了变换函数的整体切片窗口，并决定了输出张量的一个主要维度。切片窗口的长度由时间序列中特征数决定，即$m$。因此，得到的切片窗口尺寸为$m\times \omega$。切片窗口的步长参数，用$s$表示，表示切片窗口沿时间维向前推进的时间步长。由于时间序列中相邻值之间的信息丢失，步幅越大，总体模式分辨率越低。膨胀(dilation)参数$d$类似于Yu等人[12]提出的用于 dilated CNN的参数，它允许我们在不牺牲有限的内存空间的情况下对更大的接受窗口大小进行切片。padding 值$p$类似于CNN固有地增加原始数据结构的维数。它支持平移不变的转换，并保留数据的维度大小。具体来说，在我们的上下文中，padding是指沿时间维向输入时间序列矩阵的两端对称地附加一组大小为$m$的一维向量的过程。然而，我们假设附加到时间序列数据上的这些值可能存在潜在的问题，因为存在用噪声或分布外的值污染原始序列数据的风险。因此，我们需要仔细地选择填充值，例如，使用相同的相邻值或预定时间窗口内的向量的局部平均值。

根据上面定义的超参数，我们可以确定地得到如下的切片数$o$(没太看懂，和卷积输出计算不一样):

$$
o = \lfloor \frac{T+2p-2d(\omega-1)-1}{s}+1\rfloor
$$

我们总结了算法1中的时间切片堆叠转换。为简单起见，我们在这个公式中不考虑填充和膨胀(即$p=0$和$d=1$)



![alg1](/assets/images/深度学习/用于多元时间序列预测的时间张量转换网络/alg1.png)

算法1：时间切片堆叠转换

D. 神经网络结构

在这一节中，我们描述了基于时间张量转换的深度学习架构，它被称为 TSSNet。神经网络架构基于一个相当简单的网络结构，如图3所示。

![图3](/assets/images/深度学习/用于多元时间序列预测的时间张量转换网络/fig3.png)

图3：时间张量转换网络架构

1. 时间切片堆叠转换:给定初始输入的多元时间序列$X$，首先进行算法1所述的时间张量变换。如前所述，时间切片堆叠转换被定义为$TT:X\rightarrow \tilde{\mathcal{X}}$,其中输入数据是一个二维矩阵$X \in \mathbb{R}^{m\times T}$,输出是一个三维张量$\tilde{\mathcal{X}} \in \mathbb{R}^{m\times \omega \times o}$

2. 卷积神经网络:将输入的时间序列数据转换为$\tilde{\mathcal{X}}$后，利用CNN沿着$\omega \times o$平面从张量中提取特征。也就是说，我们将每个特征视为一个单独的通道。因此，通道总数等于特征$m$的个数。我们还将CNN卷积核的一个维数设为堆栈高$o$，因此卷积核的维数为$o\times k$，其中$k$为卷积核的宽度。假设我们有$l$个卷积核，第$i$个核学习以下的权值集合:

$$
h_i = W_i * \tilde{\mathcal{X}}+b_i, i\in \{1,2,\cdots,l\}
$$

其中$*$为卷积操作，$W_i$和$b_i$分别是权重和偏置参数。请注意，在这个过程中，我们没有应用任何类型的激活函数，因为模型对任何类型的操作都很敏感，这些操作可以折叠输入值的范围(例如，在RELU中负值将变为零)。我们根据经验将卷积核的数量设置为m。卷积运算的输出特征图随后被输入到一个最大池化操作中，作用于相同的$\omega\times o$平面。我们以连续的方式重复这两个操作两次。

3. 稠密层:从CNN层提取特征后，将对应的特征图作为单个向量进行展平。然后，它随后被送入一个全连接的隐藏层，$fc_1$:

$$
fc_1 = W_1\times h + b_1
$$

其中$h$为之前的CNN特征图中拉平的1D特征图, $W_1$和$b_1$分别为权重和偏置参数。作为一种启发，在这个稠密层中使用的隐藏权重参数的维数通常大于输出层维数。最后将后续的隐向量$fc_1$输入到输出层，在输出层我们学习了以下参数集:

$$
\hat{y} = W_2 \times fc_1 + b_2
$$

其中$W_2$和$b_2$为权重和偏置参数。最后的输出是一个1D向量，$\hat{y}\in \mathbb{R}^{m\times h}$也可以重构为$m\times h$的二维矩阵。

E. 目标函数

为了训练提出的神经网络结构，我们最小化平方误差损失函数:

$$
\mathop{\min}_{\Theta} \sum_{t\in T_{train}} ||Y_t-\hat{Y}_t||_F^2
$$

代价最小化w.r.t.参数$\Theta$,和$||\cdot||_F^2$表示 Frobenius norm。

F. 优化方法
为了优化代价函数，我们使用规范的方法来优化标准神经网络。具体来说，我们可以应用一些常用的基于梯度的方法，如随机梯度下降法(SGD)或Adam算法[13]。

## IV. 实验

我们将在本节中评估所提出的神经网路架构的效能。我们首先介绍几个用于基准比较的基线模型。然后介绍了评价指标和数据集，最后给出了实验结果。

A. 基线模型
比较我们提出的模型的性能和鲁棒性，我们以以下方法为基准:
* 向量自回归(VAR)
* 长短期记忆(LSTM)
* Gated Recurrent Unit(GRU)
* 1D 卷积神经网络（CNN）
* LSTNet[10]
  
除了基于Python StatsModels包[15]的向量自回归(VAR)模型外，所有实现都是在PyTorch[14]中开发的。我们的评估是在有状态(即VAR和RNN变体)、无状态(即CNN变体)以及两者的混合(即LSTNet)的模型之间执行的。

对于递归神经网络的变体，我们为长短时记忆(LSTM)和门控递归单元(GRU)实现了一种多对一的单层结构，并在最后附加了两个全连接的层。同样地，1D卷积神经网络(CNN)架构利用单个1D CNN和1个最大池化及两个全连接层。在该结构中，我们将二维输入时间序列视为单通道图像，并在时间轴上进行卷积。我们的LSTNet架构基于Lai等人的代码包[10]。对于所有基于神经网络的模型，它们都生成一个批量的多步预测值，而不是单个时间步长输出。对于向量自回归，预测是基于迭代方式生成的，其中将当前时间步长的预测追加到之前的输入，以预测下一个时间步的值。

B. 评价指标

为了比较所有的方法，我们使用以下两个评估指标。我们的模型的目标是最小化均方根误差(Root Mean Squared Error,RMSE)，同时联合最大化经验相关系数(empirical correlation coefficient,CORR)。

**Root Mean Square Error (RMSE):**(感觉也不对，根号内没取平均？)

$$
RMSE = \frac{1}{n} \sum_{i=1}^n \sqrt{\sum_{j=1}^{m}\sum_{t=1}^h (\mathcal{Y}_{ijt}-\hat{\mathcal{Y}}_{ijt})^2}
$$

其中$\mathcal{Y},\hat{\mathcal{Y}} \in \mathbb{R}^{n\times m \times h}$,这里的$n$是被计算样本的总数。在这个公式中，我们对预测值的总$h$层的所有$m$个不同特征的误差进行了汇总。

**Empirical Correlation Coefficient (CORR):**


---
**参考**：
1. 论文：Yuya Jeremy Ong, Mu Qiao, Divyesh Jadav [Temporal Tensor Transformation Network for Multivariate Time Series Prediction](https://arxiv.org/abs/2001.01051)
