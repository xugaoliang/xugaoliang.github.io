# 【译】时间对比网络：从视频中自监督学习


**摘要**:我们提出了一种完全从多角度记录的未标记视频中学习表征和机器人行为的自监督方法，并研究了这种表征如何用于两种机器人模仿场景:模仿人类视频中的物体交互和模仿人类姿态。模仿人类行为需要一个视图不变的表征，它捕获末端执行器(手或机器人手爪)与环境、对象属性和身体姿势之间的关系。我们使用度量学习损失来训练我们的表征，其中相同观测的多个同时视角被吸引到嵌入空间中，同时被时间邻居排斥，这些时间邻居通常在视觉上相似，但功能上不同。换句话说，这个模型同时学习识别不同图像之间的共同点，以及相似图像之间的不同点。这个信号使我们的模型发现属性，这些属性不会随视角而改变，但会随时间而改变，而忽略了讨厌的变量，如遮挡、运动模糊、光照和背景。我们证明了这种表征可以被机器人直接模仿人类的姿势，而不需要一个明确的对应关系，并且它可以在强化学习算法中用作奖励函数。虽然表征是从一组未标记的与任务相关的视频中学习的，但机器人的行为，如pouring，则是通过观看一个人类的3d演示来学习的。通过跟随人类演示下学到的表征获得的奖励函数使强化学习在现实机器人系统的实际应用中更有效。视频结果，开源代码和数据集在 sermanet.github.io/imitate

## I. 介绍

虽然监督学习已经在一系列任务上取得了成功，比如对象分类等，但在机器人等交互式应用程序中出现的许多问题难以监督。例如，将浇注任务的每个方面都标记得足够详细以使机器人能够理解所有与任务相关的属性，这是不切实际的。浇注演示可以根据背景、容器和视角的不同而有所不同，在每一帧中可能有许多突出的属性，例如，一只手是否接触容器，容器的倾斜，或目标容器中当前的液体量或其粘度。理想情况下，现实世界中的机器人能够做到两件事:纯粹通过观察来学习对象交互任务的相关属性，以及理解如何将人类的姿势和对象交互映射到机器人上，以便直接通过第三人称视频观察进行模仿


![图1](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig1.png)

图1： 时间对比网络(Time-Contrastive Networks, TCN):同时从不同视点拍摄的 Anchor(锚) 和 positive（正例） 图像在嵌入空间上尽量靠近，而从同一序列不同时间拍摄的 negative (负例)图像尽量远离。模型通过同时回答以下问题来训练自己:不同外观的蓝色框架之间有什么共同之处?相似的红色和蓝色镜框有什么不同?由此产生的嵌入可以用于一般的自监督机器人，但也可以自然地处理3d人的模仿。

在这项工作中，我们采取了一个步骤，通过使用自监督和多视角表征学习来同时解决这些挑战。我们从未标记的交互场景多视角视频中获得学习信号，如图1所示。通过对多视角视频的学习，所学习的表征有效地将位姿等功能属性分离出来，同时保持视角和agent不变。然后，我们展示了机器人如何通过强化学习或直接回归学习将这种视觉表征与相应的运动指令联系起来，从而通过观察人类有效地学习新任务。

我们工作的主要贡献是一个表征学习算法,基于现有的语义相关特征(在我们的例子中,特征来自ImageNet数据集[1,2]上训练的网络)来产生一个对对象交互和姿势敏感，对不关心的视角和外观不敏感的度量嵌入。我们证明了这种表征形式可以用来创建一个奖励函数来强化机器人技能的学习，只使用原始的视频演示来监督，并直接模仿人类的姿势，没有任何明确的关联级别的对应，同样直接来自原始视频。我们的实验证明了使用真实的机器人进行浇筑任务的有效学习，在模拟中移动盘子进出碗碟架，以及对人类姿势的实时模仿。虽然在我们的实验中，我们为每个任务训练了不同的TCN嵌入，但是在未来的工作中我们将根据不同上下文中的各种演示构建嵌入，并讨论如何构建更大的多任务嵌入。

## II. 相关工作
**模仿学习**:模仿学习[3]被广泛用于从专家演示中学习机器人技能[4,5,6,7]，它可以分为两个领域:行为克隆和反强化学习(IRL)。行为克隆考虑一个监督学习问题，其中行为的例子作为状态-动作对提供[8,9]。另一方面，IRL使用专家演示来学习一个奖励函数，该函数可用于优化带有强化学习[10]的模仿策略。这两种类型的模仿学习都需要专家在与学习者相同的情境中提供示范。在机器人技术中，这可能是通过动觉(kinesthetic)演示[11]或远程操作[12]来实现的，但这两种方法都需要相当多的操作者专业知识。如果我们的目标是赋予机器人广泛的行为技能，那么能够直接从人类的第三人称视频中获得这些技能将会大大提高可扩展性。最近，一系列的工作研究了在不同的背景下观察到的一个演示的模仿问题，例如从一个不同的角度或一个具有不同化身的代理，例如 a  human[13,14,15] Liu等人提出在专家语境和学习者语境之间翻译示范，通过尽量缩短翻译示范的距离来学习模仿策略。然而，Liu等人明确排除了任何具有域转移的演示，即演示是由一个人执行，并由具有明显视觉差异的机器人模仿(例如，人手vs.机器人钳子)。与此相反，我们的TCN模型是在具有不同实施例、对象和背景的各种演示上进行训练的。这使得我们基于TCN的方法可以直接模拟人类演示，包括人类将液体倒入杯子的演示，以及模拟人类的姿势，而不需要任何显式的关节水平对齐。据我们所知，我们的工作是第一个模拟原始视频演示的方法，既可以模拟原始视频，又可以处理人与机器人之间的领域转换.

**无标签训练信号**:无标签学习的视觉表征承诺，可以从无监督的数据中进行视觉理解，因此，近年来已被广泛探索。在这一领域之前的工作已经研究了无监督学习，将其作为一种从小型标记数据集[17]、图像检索[18]和各种其他任务中进行监督学习的方法[19、20、21、22]。在本文中，我们特别关注表征学习，以实现对象、人及其环境之间的模型交互，这需要隐式建模广泛的因素，如功能关系，同时不受讨厌的变量(如视角和外观)的影响。该方法利用多视角同时记录的信号构造图像嵌入。之前的许多工作都使用了多种模式和时间或空间的一致性(连贯性)来提取嵌入和特征。例如，[23,24]利用视频中声音和视觉线索的同时出现来学习有意义的视觉特征。[20]还提出了一种通过训练网络进行跨通道输入重建的多通道自监督方法。[25,26]利用图像的空间连贯性作为自监督信号，[27]利用运动线索进行自监督分割任务。这些方法更侧重于空间关系，它们提供的非监督信号是对本文所探讨的方法的补充。

之前的许多工作都使用了时间连贯性[28,29,30,31]。其他人也使用度量学习训练视角不变性[22,32,33]。我们工作的新颖之处在于将这两个对立的方面结合起来，如 III-A 节所述。[19]使用了一个三元损失，在嵌入过程中，跟踪序列的第一帧和最后一帧会更接近，而来自其他视频的随机负帧则相距很远。我们的方法的不同之处在于，我们用暂时的邻居作为负例来对抗同时刻的另一个视角锚定的正例。这导致我们的方法发现有意义的维度，如属性或姿态，而[19]侧重于学习类内不变性。同步多视角捕获还提供了准确的对应，而跟踪不提供，并可以提供一套丰富的对应，如闭塞，模糊，照明和视角。

也有研究提出使用预测作为学习信号[34,35]。结果表征通常主要基于预测图像的真实性进行评估，这仍然是一个具有挑战性的开放问题。许多其他先前的方法已经使用了各种各样的标签和先验来学习嵌入。[36]使用标记数据集来训练一个姿态嵌入，然后从训练数据中为一个姿态检索任务找到新图像的最近邻居。我们的方法是通过ImageNet训练初始化的，但是可以发现像姿态和任务进程(例如浇注任务)这样的维度，而不需要任何特定于任务的标签。[37]探索各种类型的物理先验，如物体在重力下的轨迹，以学习在没有明确监督下的物体跟踪。我们的方法在精神上是相似的，因为它使用了时间共现（temporal co-occurrence），这是一个普遍的物理属性，但我们使用的原则是通用的和广泛适用的，不需要具体的物理规则的任务输入。


![图2](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig2.png)

图2：**多视图捕捉** 配备了智能手机的两个操作员。自由移动相机会带来丰富多样的比例、视角、遮挡、运动模糊和两个相机之间的背景对应


**镜像神经元**:人类和动物已经被实验证明，在他们的环境[38]中拥有物体和其他物体的视角不变表象，而众所周知的关于“镜像神经元”的工作已经证明，这些视角不变表象对于模仿[39]至关重要。图2中的多视角捕获设置类似于[38]使用的实验设置，而我们的机器人模仿设置(机器人模仿人类运动，但从未接收到ground truth位姿标签)考察了在一个学习系统中如何产生自监督位姿识别

## III. 利用时间对比网络进行模仿

我们模仿学习的方法是只依赖来自世界的感官输入。我们通过两个步骤实现这一点。首先，我们从被动观察中学习抽象表征。其次，我们利用这些表征来引导机器人模仿人类的行为，并学习执行新的任务。我们使用“模仿”（imitation）这个术语，而不是“演示”(demonstrations)，因为我们的模型也从对非演示行为的被动观察中学习。机器人需要对它所看到的一切有一个大致的了解，以便更好地识别一个积极的演示。我们故意坚持只使用自监督来保持该方法在现实世界中的可扩展性。在这项工作中，我们探讨了几种利用时间作为信号进行无监督表征学习的方法。下面我们还将探讨实现自监督机器人控制的不同方法。

A.训练时间对比网络

我们在图1中说明了我们的时间对比(TC)方法。该方法通过三元损失[40]实现多视图度量学习。图像$x$的嵌入由$f(x)\in \mathbb{R}^d$表示。这种损失保证了同时出现的$x_i^a$(anchor)和$x_i^p$(positive)在嵌入空间上比任何图像$x_i^n$(negative)更接近。因此，我们的目标是学习一个这样的嵌入$f$:

$$
||f(x_i^a)-f(x_i^p)||_2^2 + \alpha < ||f(x_i^a)-f(x_i^n)||_2^2 \\\\
\forall (f(x_i^a),f(x_i^p),f(x_i^an)) \in \mathcal{T}
$$

其中$\alpha$是正例和负例对之间的一个强制的间隔，$\mathcal{T}$是训练集中所有可能的三元组。核心理念是两个坐标系(锚和正例)来自相同的时间,但不同的视角(或模式)拉在一起,而时间邻居上的视觉相似帧被分开。这个信号有两个目的:学习无标签的解缠表征，同时学习视角不变性以进行模仿。交叉试图对应鼓励学习视角，规模，遮挡，运动模糊，照明和背景的不变性，因为正例和锚沿这些因素的变化，显示相同的主题。例如，除了遮挡之外，图1展示了顶部和底部序列之间的所有这些转换。除了学习一组丰富的视觉不变性外，我们还对用于模仿的第三人称到第一人称一致的视角不变性感兴趣。时间对比信号如何导致解缠表征?它通过引入时间上的邻居之间的竞争来解释视觉上随时间的变化。例如，在图1中，由于邻居在视觉上是相似的，区分它们的唯一方法是建模杯子里的液体量，或者建模手或物体的姿势和它们之间的相互作用。另一种对TCN提供的强大的训练信号的理解方式是同时识别模型上的两个约束条件:沿着图1中的视图轴，模型学习来解释看起来不同的图像之间什么是相同的,而沿着时间轴学会解释看起来相似的图像物体之间有什么不同。请注意，虽然模仿这种方法的自然能力很重要，但是用无监督的方式学习丰富的表征是更重要的贡献。我们的方法的关键因素是，多种视角为物理世界的变化奠定了基础，消除了可能的解释的歧义。我们在第四章中指出，TCN确实可以在没有监督的情况下发现不同物体或物体之间的对应关系，以及杯子和浇铸阶段的液体量等属性。这是一个有点令人惊讶的发现，因为从来没有提供过物体或物体之间的明确对应关系。我们假设不同但功能相似的对象在嵌入空间中的流形自然对齐，因为它们共享一些功能和外观。

多视图数据收集非常简单，只需两个配备智能手机的操作人员即可捕获，如图2所示。一个操作人员在执行任务时保持对感兴趣区域的固定视角，而另一个操作人员自由移动摄像机以引入上面讨论的变化。虽然多视图捕获比单视图捕获更麻烦，但我们认为，与人工标记等替代方法相比，多视图捕获便宜、简单、实用。

![图3](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig3.png)

图3：**单视角TCN**:在锚点周围的小窗口中选择正例，在相同的序列上，从远处的时间步选择负例

我们也可以考虑单视角视频训练的时间对比模型，如图3所示。在这种情况下，在锚的一定范围内随机选择正例帧。给定正例范围，再计算出一个间隔范围。在间隔范围边缘外随机选择负例，对模型进行训练。我们根据经验选择了2倍的正例范围，它本身设置为 0.2s。虽然我们在第四节中展示了多视图TCN的最佳性能，但是当没有多视图数据可用时，单视图版本仍然很有用。


B.用强化学习学习机器人行为

在这项工作中，我们考虑了一个模仿学习场景，其中的演示来自一个不同于学习代理的化身——第3人称视频观察，例如机器人模仿一个人。由于上下文的差异，直接跟踪所演示的像素值并不能提供学习模仿行为的合理方法。如前一节所述，TCN嵌入提供了一种提取图像特征的方法，这些特征不受摄像机角度和操作对象的影响，并且可以解释世界上的物理交互。我们利用这种洞察力来构建一个基于人类视频演示的TCN嵌入与机器人摄像机记录的摄像机图像之间的距离的奖励函数。如第IV-B节所示，通过反复试验优化这个奖励函数，我们能够模拟机器人演示的行为，仅利用其视觉输入和视频演示进行学习。虽然我们使用多个多视角视频来训练TCN，但是视频演示只包含一个人从随机角度执行任务的单个视频。

设$V=(v_1,\cdots,v_T)$是视频演示序列中每一帧的TCN嵌入。对于机器人任务执行过程中,其观察到的每个相机图像，我们计算TCN嵌入$W=(w1,\cdots,w_T)$。我们定义了一个奖励函数$R(v_t,w_t)$基于欧几里德距离的平方和一个 Huber-style 损失:

$$
R(v_t,w_t) = -\alpha ||w_t-v_t||_2^2-\beta\sqrt{\gamma+||w_t-v_t||_2^2}
$$

其中$\alpha$和$\beta$是权重参数(经验选择)和$\gamma$是一个小的常数。当嵌入相距更远时欧氏距离的平方($\alpha$加权)给了我们更强的梯度,从而在学习开始时导致更大的策略更新。当嵌入向量变得非常近的时候，Huber-style 损失($\beta$加权)开始占据主要作用以确保高精度任务的执行和接近训练结束时的微调。

为了学习机器人的模仿策略，我们利用强化学习优化了上述的奖励函数。特别地，为了优化机器人的轨迹，我们使用了PILQR算法[41]。该算法通过LQR将近似的基于模型的更新与拟合的时变线性动力学和无模型修正相结合。我们注意到，在我们的任务中，TCN嵌入在机器人面前提供了一个表现良好的低维(在我们的实验中是32维)视觉世界状态。通过在系统状态中包含TCN特性(即状态=关节角度+关节速度+TCN特性)，我们可以在基于模型的LQR更新过程中利用动力学的线性近似，从而显著加速训练速度。强化学习设置的细节可以在附录B中找到。

C.直接模仿人体姿势

在前一节中，我们讨论了如何将强化学习与TCNs一起使用，从而直接从人类的视频演示中学习对象交互技能。在本节中，我们将介绍使用TCNs的另一种方法:直接模仿人体姿态。虽然对象交互技能主要需要匹配演示的功能方面，但直接的姿态模仿需要学习人类和机器人姿态之间的隐式映射，因此涉及到框架之间更细粒度的关联。一旦学会，人-机器人映射，则可以通过初始化一个接近解决方案的策略来加速RL的探索阶段。


![图4](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig4.png)

图4：**姿势模仿训练信号**:时间对比、自回归和人类监督。时间对比信号让模型学习了人类或机器人个体的丰富表征。自回归允许机器人根据自身的图像来预测自己的关节。人类监督信号是从试图模仿机器人姿势的人那里收集来的。

我们通过自回归来学习直接的姿势模仿。如图4和图8所示，为自监督的人体姿态模仿。这个想法是直接预测机器人的内部状态给它一个自己的图像。类似于在镜子中看着自己，机器人可以将自己对自身形象的预测回归到自身的内部状态。我们首先通过观察人类和机器人执行随机动作来训练一个共享的TCN嵌入。然后机器人通过自回归来训练自己。因为它使用的TCN嵌入在人类和机器人之间是不变的，机器人可以在自我训练后自然地模仿人类。因此，我们得到了一个可以对人类动作进行端对端模仿的系统，尽管它从未被赋予任何人类姿态标签，也没有人对机器人的对应。我们在图4中展示了一种收集端到端模仿的人工监督的方法。然而，与时间对比信号和自回归信号相反，人工监控信号的采集是非常嘈杂和昂贵的。我们用它来作为我们在第IV-C节中的方法的基准，并表明大量廉价的监督可以有效地与少量昂贵的监督混合在一起.

## IV. 实验

我们的实验旨在研究三个问题。首先，我们检查TCN是否可以学习更能表示对象交互属性的视觉表征，例如浇注任务中的阶段。这使得我们可以比较TCN和其他自监督的表征形式。其次，我们研究如何将TCN与强化学习结合起来，在仿真和真实机器人平台上获得复杂的对象操作技能。最后，我们证明了TCN可以使机器人连续、实时地模仿人类的姿势，而不需要明确指定机器人和人类之间的任何关节级别的对应。总之，这些实验说明了TCN表征在建模姿态、对象交互以及机器人模仿者和人类演示者之间的隐式对应方面的适用性。

A. 从通用表征中发现属性

1. 液体浇铸:在这个实验中，我们通过观察一个人从不同的容器中浇铸液体到不同的杯子中，研究TCN简单地捕捉到了什么。这些视频是用两种标准的智能手机拍摄的(见图2)，一种是人类从主角度拍摄的，另一种是自由移动的第三人称视角。通过一个现成的应用程序在两部手机之间同步捕获，每个序列大约5秒长。我们将收集到的多视角序列分为3组:133个序列用于训练(总计约11分钟)，17个序列用于验证，30个序列用于测试。训练视频包含透明和不透明的杯子，但我们将测试视频限制为透明杯子，以评估模型是否了解杯子有多满。

2. 模型:在所有后续的实验中，我们使用了一个源自Inception架构[2]的类似于[42]的自定义架构。它由Inception模型直到层“Mixed_5d”(用ImageNet预训练的权值初始化)，然后是2个卷积层，一个空间softmax层[42]和一个全连接层。嵌入是一个全连接层，在我们的自定义模型上添加了32个单元。这种嵌入是训练要么与多视角TC损失，单视角TC损失，或打乱&学习损失[31]。对于TCN模型，我们使用未经修改的[40]的三元损失，其间隔值为0.2。请注意，在所有的实验中，负例总是来自于与正例相同的序列。我们还用其他度量学习损失，即 npairs [43]和 lifted structured[44]进行了实验，结果表明它们具有可比性。在接下来的实验中，我们使用Imagenet预训练的Inception模型[1,2](一个2048维的向量)的分类之前的最后一层的输出作为基线，称之为“Inception-ImageNet”。由于自定义模型是由ImageNet预训练初始化的，因此它是一个自然的比较点，允许我们控制通过ImageNet训练而不是其他方法引入的任何不变性。我们将TCN模型与根据我们的数据训练的 shuffle & learn基线进行比较，使用与论文相同的超参数(tmax 60, tmin 15，负类占比0.75)。注意，在我们的实现中，shuffle & learn基线和TCN都不能从高动态帧的偏置采样中获益。为了研究多视角和单视角之间的区别，我们比较了单视角TCN，它的正例范围为0.2秒，负例为2倍。

3. 模型选择:非监督训练中出现的模型选择问题。您应该根据验证损失来选择最佳模型吗?或者为一个给定的任务手工标记一个小的验证?我们报告两种方法的数据。在表I中，我们根据模型的最低验证损失来选择每个模型，而在表IV中，我们根据前面描述的5个属性标记的小验证集的分类得分来选择模型。与预期一样，通过验证分类得分选择的模型在分类任务中表现得更好。然而，由loss选择的模型，除了shuffle & learn之外，其表现稍差，而shuffle & learn的准确率损失更大。我们的结论是，选择基于验证损失的TCN模型是合理的，而不是使用任何标签。
4. 训练时间:在表IV中，我们观察到多视角TCN(使用三元损失)的性能优于单视角模型，同时需要的训练时间减少了15倍，并且在完全相同的数据集上进行训练。我们的结论是，利用时间的相似性（correspondences）可以大大提高训练的时间和准确性。

![表1](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/tab1.png)

表1：浇注对齐度和分类误差:所有模型的验证损失最小。分类误差考虑了与浇注有关的5类，详见表2。


![表2](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/tab2.png)

表2：详细属性分类误差，用验证损失进行模型选择。

5. 定量评估:我们在表1中提出了两个度量标准来评估模型能够捕获的内容。对齐度度量一个模型在语义上如何对齐两个视频。分类指标衡量的是一个模型能够在多大程度上分解与浇注相关的属性，这在实际的机器人浇注任务中是很有用的。本节中的所有结果都使用嵌入空间中的最近邻进行评估。给定一个视频的每一帧，每个模型必须选择另一个视频中语义最相似的帧。“随机”基线只是从第二个视频返回一个随机帧

序列对齐度量在学习模仿时特别相关和重要，特别是从第三方的角度来看。为每个浇注测试视频,一个操作员标注相对应的关键帧到以下事件:用手接触浇注容器的第一帧,液体流动的第一帧,液体流动的最后一帧,用手接触容器的最后一帧。这些关键帧建立了一个粗略的语义对齐，应该提供所有视频之间一个相对准确的分段线性对应。对于测试集中任意一对视频$(v_1,v_2)$，我们嵌入每一帧让模型进行评估。对于源视频$v_1$的每一帧，我们将其与来自$v_2$的所有帧的嵌入空间中最近的相邻帧相关联。我们评估$v_2$中最近的近邻在语义上与$v_1$中的参照帧是否一致。通过标记对齐，我们找到了参照帧与目标视频$v_2$的比例位置，并计算到该位置的帧距离，根据目标段长度归一化。

我们在测试和验证集中标记以下属性来评估表2中报告的分类任务:手是否与容器接触?(是或否);容器是否在容器的浇注距离内?(是或否);浇注容器的倾斜角是多少?(值90、45、0和-45度);液体在流动吗?(是或否);收件器是否含有液体?(是或否)。之所以要评估这些特殊属性，是因为它们对于模拟和执行浇注任务很重要。分类结果按类分布归一化。请注意，虽然这可以与引论中提到的监督分类器进行比较，但是在实际应用中，例如在机器人技术中，期望每个可能的任务都有标签是不现实的。相反，在这项工作中，我们的目标是比较现实的一般 off-the-shelf（现成的,不用定制的）模型，它可以使用不需要新的标签。

在表1中，我们发现多视角TCN模型优于所有基线。我们观察到，单视角TCN和shuffle & learn在分类指标上是平等的，而在对齐指标上则不是。我们发现，与其他基线相比，一般的现成的先启（Inception,起初）特征明显表现不佳。附录C提供了嵌入的定性示例和t-SNE可视化演示。我们鼓励读者参考补充视频以更好地掌握这些结果。

B. 学习对象交互技能


![图5](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig5.png)

图5：模拟盘架任务。左:第三人称VR演示的盘架任务。中间:训练中机器人摄像头的视角。右图:执行摆盘任务的机器人


![图6](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig6.png)

图6：真实的机器人浇注任务。左:浇注任务的第三人称人体演示。中间:训练中机器人摄像头的视角。右图:机器人执行浇注任务。

在本节中，我们使用第III-B节中描述的基于 TCN 的奖励函数，通过强化学习从第三人称演示中学习机器人模仿行为。我们在两个任务上评估了我们的方法，一个是在模拟的盘架环境下的盘子转移(图5，使用 Bullet 物理引擎[45])，另一个是真实的机器人从人类演示中浇注(图6)。

1. 任务设置:模拟的盘架环境包括两个放在桌子上的架子和装满盘子的架子。这项任务的目标是将盘子从一个架子移到另一个架子而不让它们掉下来。这需要一个复杂的运动，有多个阶段，如伸手，抓取，拿起，携带和放置。我们使用虚拟现实(VR)系统来记录人类的演示，以操纵一个自由浮动的钳子并移动盘子(图5左)。我们通过在虚拟世界中放置第一视角和第三人称摄像头来记录VR演示的视频。除了演示之外，我们还记录了一系列随机运动，以增加我们的TCN模型的泛化能力。在记录演示之后，我们将一个模拟的 7-DoF KUKA 机器人手臂放置在碗碟架环境中(图5右侧)，并在其上附加一个第一视角摄像机。然后利用机器人摄像机图像(图5中)计算TCN奖励函数。利用随机高斯噪声初始化机器人策略。

对于真实的机器人浇注任务，我们首先从多个摄像头收集多视角数据来训练TCN模型。这套训练集包括人类在智能手机摄像头上录制的倒液体的视频，以及机器人在两个机器人摄像头上录制的倒颗粒状珠子的视频。我们不仅收集了手头任务的正例演示，我们还收集了实际上不涉及倒东西的各种交互作用，如移动杯子、将杯子打翻、洒出珠子等，以覆盖机器人可能需要理解的事件范围。浇注实验分析了TCNs如何隐式地建立人与机器人对物体的操作之间的对应关系。我们用来训练TCN的数据集包括执行浇铸任务的约20分钟人类视频，以及以浇铸以外的方式操作杯子和瓶子的约20分钟人类视频，比如移动杯子、把它们打翻等等。为了使TCN能够同时表示人臂和机器人臂，并隐式地将它们对应起来，还必须向TCN提供数据，使其能够理解机器人臂的外观。为此，我们添加了一组数据，其中包括机器人手臂在类似浇筑的设置中操纵杯子的约20分钟视频。注意，这些数据本身不一定能说明成功的浇筑任务:在强化学习期间跟踪的最后一次演示包括一个人成功地浇筑一杯液体，而机器人则用橙色的珠子完成浇筑任务。然而，我们发现提供一些以机器人手臂为特色的剪辑对于TCN获得一个能够正确记录人类和机器人之间的相似性的表现形式是很重要的。使用额外的机器人数据在这里是合理的，因为在从未见过自己手臂的情况下，期望机器人做得好是不现实的。然而，随着时间的推移，学习的任务越多，所需的任务就越少。虽然TCN接受了大约1小时的与浇注相关的多视角序列的训练，但是机器人的策略只能从一个人提供的单个液体浇注视频中学习(图6左)。在这个视频中，我们训练了一个 7-DoF KUKA 机器人来完成如图6(右)所示的粒状珠子的浇注。我们从机器人摄像机图像(图6中)计算TCN嵌入，并使用随机高斯噪声初始化机器人策略。我们在手腕关节处设置了更高的初始探索，因为它对浇注运动的贡献最大(对于所有比较的算法)。


![图7](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig7.png)

2. 定量评价:与上一节相同基线评估相比，使用TCN模型进行奖励计算的浇注任务绩效如图7所示。每次倾倒后，我们测量接收容器中珠子的重量。我们在每个迭代中执行10次倾倒。图7中的结果平均每个模型运行4次(2次运行2个固定的随机种子)。在使用多视角TCN模型(mvTCN)的前几次迭代之后，机器人已经能够成功地倒出大量的珠子。经过10次迭代，策略收敛到一致成功的浇注行为。相比之下，机器人无法完成与其他模型的任务。有趣的是，我们观察到单视角模型(单视图TCN和shuffle & learn)的性能较低，尽管它们是在与mvTCN完全相同的多视角数据上训练的。

在使用不同的人体演示时，我们观察到图12中的相同模式。这表明在这个任务中，利用多视角通信（correspondences）是必要的，以便从第3人称的角度正确地建模对象交互。结果表明，mvTCN确实为机器人理解浇筑任务提供了合适的指导。事实上，由于PILQR[41]方法使用基于模型和非模型的更新，实验表明mvTCN不仅在浇筑成功时提供良好的指标，而且在浇筑失败时提供有用的梯度;而其他经过测试的表征不足以学习此任务。本实验说明了自监督表征学习和视觉演示的连续奖励如何缓解强化学习的样本效率问题。

3. 定性评价:正如我们在补充视频中所展示的，盘架和倾倒策略都收敛于稳健的模仿行为。在盘架任务中，机器人能够逐步学习所有的任务组件，包括手臂的运动和夹板的开闭。它首先学会伸手去拿盘子，然后抓住并把它捡起来，最后把它带到另一个盘架上并把它放在那里。该任务的学习只需要10次迭代，每次迭代有10次 roll-outs 。这说明TCN奖励函数的密度和平滑程度足以有效地引导机器人执行复杂的模仿策略。

在浇注任务中，机器人通过随机旋转和移动装满珠子的杯子开始高斯探索。在早期的迭代中，机器人首先学会移动和旋转杯子到接收容器，错过目标杯子，并洒出大量的珠子。经过几次迭代后，机器人学会了更精确，最终它能够在最后一次迭代中一致地倒入大多数珠子。这表明，我们的方法可以有效地学习非线性动态对象转换的任务，如颗粒介质和液体的运动，如果使用传统的状态估计技术来执行，则将是一个困难的任务。

C. 人体姿态模仿的自我回归

![图8](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig8.png)

图8：自监督人体姿势模仿的TCN:结构、训练和模仿。嵌入的训练是无监督的，具有时间对比损失，而关节解码器可以在自监督、人工监督或两者同时进行训练。输出关节可以直接由机器人规划师进行模拟。人类的姿势从来没有明确地表现出来。

在前一节中，我们展示了我们可以使用TCN来构建一个带有强化学习的学习对象操作的奖励函数。在本节中，我们还将研究如何使用TCN来实时地从人类直接映射到机器人，如图8所示:除了理解对象交互之外，我们还可以使用TCN来构建一个位置敏感的嵌入，可以是无监督的，也可以是在最少监督的情况下。多视图TCN特别适合这项任务，因为除了需要视角和机器人/人的不变性外，对应问题定义不清，而且很难监督。除了在TCN嵌入上添加一个关节解码器并使用自回归信号对其进行训练外，该方法没有根本的区别。在本节中，我们将与图4所示的 human-to-robot 模仿的相对应的机器人关节向量作为ground truth。将人体图像输入到仿真系统中，得到的关节向量与ground truth关节向量进行比较。


![表3](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/tab3.png)

表3：监控信号不同组合的模仿误差。报告的误差是预测和 groundtruth 之间的距离。注意:完美的模仿是不可能的。

通过比较不同的监控信号组合，我们在表3中可以看出，使用所有信号的训练效果最好。我们观察到，添加时间对比信号总是能显著提高性能。总的来说，我们得出的结论是，相对大量的廉价弱监督数据和少量昂贵的人工监督数据可以有效地平衡我们的问题。有趣的是，我们发现自监督模型(TC+self)比人监督模型表现更好。然而，需要注意的是，定量评估在这里并没有提供足够的信息:由于任务是高度主观的，不同的人类主体模仿机器人的方式也不同，因此，在现有数据上匹配关节角度是非常困难的。我们邀请读者看附带的视频模仿的例子,并观察之间有密切的联系,人类和机器人运动,包括蹲等微妙的元素构成:当人类蹲下来,通过移动关节机器人降低躯干的脊柱。在视频中，我们观察到一个复杂的人-机器人映射被发现，完全没有人类监督。在这种情况下，当通信定义不清时，就需要考虑中间人类姿态检测器。在图14中，我们可视化了位姿模仿的TCN嵌入，并证明了人与机器人之间的位姿在集群中是一致的，而对视角和背景是不变的。更多分析见附录D。


## V. 结论

本文介绍了一种基于多视角视频的自监督表征学习方法。通过从其他视角对同时发生的帧锚定一个时间对比信号来学习表征，从而得到一个消除时间变化(例如，突出事件)歧义的表征，同时为视角和其他讨厌的变量提供不变性。我们证明了该表征可以用于在机器人对象操作的强化学习系统中提供一个奖励函数，并提供人类和机器人姿态之间的映射，使之能够直接从原始视频中模仿姿态。在这两种情况下，TCN都能使机器人从人类执行各种任务的原始视频中模仿，这就解释了人类和机器人身体之间的领域转换。虽然训练过程需要一个多视角视频的数据集，但是一旦TCN被训练完成，只需要一个单独的原始视频演示来模仿。限制和未来的工作将在附录A中讨论。

鸣谢：我们感谢 Mohi Khansari, Yunfei Bai,Erwin Coumans, Jonathan Tompson, James Davidson 和 Vincent Vanhoucke 有帮助的反馈和 Ashwin Kakarla 帮忙标注评估数据。我们感谢每一个为这个项目提供模仿的人:Phing Lee, Alexander Toshev, Anna Goldie, Deanna Chen, Deirdre Quillen, Dieterich Lawson, Eric Langlois, Ethan Holly, Irwan Bello, Jasmine Collins, Jeff Dean, Ken Oslund, Laura Downs, Leslie Phillips, Luke Metz, Mike Schuster, Ryan Dahl, Sam Schoenholz 和 Yifei Feng。

## 参考
[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.
[2] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.
[3] B. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5):469–483, 2009.
[4] J.A. Ijspeert, J. Nakanishi, and S. Schaal. Movement imitation with nonlinear dynamical systems in humanoid robots. In ICRA, 2002.
[5] N.D. Ratliff, J.A. Bagnell, and S.S. Srinivasa. Imitation learning for locomotion and manipulation. In Humanoids, 2007. ISBN 978-1-4244-1861-9.
[6] K. Mulling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking movements in robot table tennis. In AAAI Fall Symposium: Robots Learning Interactively from Human Teachers, volume FS-12-07, 2012.
[7] Y. Duan, M. Andrychowicz, B. Stadie, J. Ho, J. Schneider, I. Sutskever, P. Abbeel, and W. Zaremba. One-shot imitation learning. arXiv preprint arXiv:1703.07326, 2017.
[8] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1):88–97, 1991.
[9] S. Ross, G.J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, volume 15, pages 627–635, 2011.
[10] P. Abbeel and A. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, 2004.
[11] S. Calinon, F. Guenter, and A. Billard. On learning, representing and generalizing a task in a humanoid robot. IEEE Trans. on Systems, Man and Cybernetics, Part B, 37(2):286– 298, 2007.
[12] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal. Learning and generalization of motor skills by learning from demonstration. In ICRA, pages 763–768, 2009.
[13] B.C. Stadie, P. Abbeel, and I. Sutskever. Third-person imitation learning. CoRR, abs/1703.01703, 2017.
[14] A. Dragan and S. Srinivasa. Online customization of teleoperation interfaces. In RO-MAN, pages 919–924, 2012.
[15] P. Sermanet, K. Xu, and S. Levine. Unsupervised perceptual rewards for imitation learning. CoRR, abs/1612.06699, 2016.
[16] Y. Liu, A. Gupta, P. Abbeel, and S. Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. CoRR, abs/1707.03374, 2017.
[17] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially learned inference. In ICLR, 2017.
[18] M. Paulin, M. Douze, Z. Harchaoui, J. Mairal, F. Perronin, and C. Schmid. Local convolutional features with unsupervised training for image retrieval. In ICCV, pages 91–99, Dec 2015.
[19] X. Wang and A. Gupta. Unsupervised learning of visual representations using videos. CoRR, abs/1505.00687, 2015. 
[20] R. Zhang, P. Isola, and A.A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. CoRR, abs/1611.09842, 2016.
[21] P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008.
[22] V. Kumar, G. Carneiro, and I. D. Reid. Learning local image descriptors with deep siamese and triplet convolutional networks by minimizing global loss functions. In CVPR, 2016.
[23] A. Owens, P. Isola, J.H. McDermott, A. Torralba, E.H. Adelson, and W.T. Freeman. Visually indicated sounds. CoRR, abs/1512.08512, 2015.
[24] Y. Aytar, C. Vondrick, and A. Torralba. Soundnet: Learning sound representations from unlabeled video. CoRR, abs/1610.09001, 2016.
[25] C. Doersch, A. Gupta, and A.A. Efros. Unsupervised visual representation learning by context prediction. CoRR, abs/1505.05192, 2015.
[26] S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. In CVPR, pages 4353–4361, 2015.
[27] D. Pathak, R.B. Girshick, P. Dollar, T. Darrell, and B. Hari- ´ haran. Learning features by watching objects move. CoRR, abs/1612.06370, 2016.
[28] L. Wiskott and T.J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Comput., 14(4): 715–770, April 2002. ISSN 0899-7667.
[29] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Unsupervised learning of spatiotemporally coherent metrics. In ICCV, 2015.
[30] B. Fernando, H. Bilen, E. Gavves, and S. Gould. Selfsupervised video representation learning with odd-one-out etworks. CoRR, abs/1611.06646, 2016.
[31] I. Misra, C.L. Zitnick, and Martial Hebert. Unsupervised earning using sequential verification for action recognition. oRR, abs/1603.08561, 2016.
[32] K. Moo Yi, E. Trulls, V. Lepetit, and P. Fua. LIFT: learned nvariant feature transform. CoRR, abs/1603.09114, 2016.
[33] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, nd F. Moreno-Noguer. Discriminative learning of deep onvolutional feature point descriptors. In ICCV, 2015.
[34] W.F. Whitney, M. Chang, T.D. Kulkarni, and J.B. Tenenbaum. nderstanding visual concepts with continuation learning. oRR, abs/1602.06822, 2016.
[35] M. Mathieu, C. Couprie, and Y. LeCun. Deep multiscale video prediction beyond mean square error. CoRR, bs/1511.05440, 2015.
[36] G. Mori, C. Pantofaru, N. Kothari, T. Leung, G. Toderici, . Toshev, and W. Yang. Pose embeddings: A deep architecture for learning to match human poses. CoRR, bs/1507.00302, 2015.
[37] R. Stewart and S. Ermon. Label-free supervision of neural networks with physics and domain knowledge. CoRR, bs/1609.05566, 2016.
[38] V. Caggiano, L. Fogassi, G. Rizzolatti, J.K. Pomper, P. Thier, .A. Giese, and A. Casile. View-based encoding of actions in irror neurons of area f5 in macaque premotor cortex. Current iology, 21(2):144–148, 2011.
[39] G. Rizzolatti and L. Craighero. The mirror-neuron system. nnual Review of Neuroscience, 27:169–192, 2004.
[40] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A nified embedding for face recognition and clustering. CoRR, bs/1503.03832, 2015.
[41] Y. Chebotar, K. Hausman, M. Zhang, G. Sukhatme, S. Schaal, nd S. Levine. Combining model-based and model-free updates for trajectory-centric reinforcement learning. In ICML, 017.
[42] C. Finn, X. Yu Tan, Y. Duan, Y. Darrell, S. Levine, and . Abbeel. Learning visual feature spaces for robotic manipulation with deep spatial autoencoders. CoRR, abs/1509.06113, 015.
[43] K. Sohn. Improved deep metric learning with multi-class -pair loss objective. In Advances in Neural Information rocessing Systems, pages 1857–1865, 2016.
[44] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep etric learning via lifted structured feature embedding. CoRR, bs/1511.06452, 2015.
[45] E. Coumans and Y. Bai. pybullet, a python module for physics simulation in robotics, games and machine learning. http://pybullet.org/, 2016–2017
[46] S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In NIPS, 2014.
[47] Y. Tassa, T. Erez, and E. Todorov. Synthesis and stabilization of complex behaviors. In IROS, 2012.
[48] E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforcement learning. JMLR, 11, 2010.
[49] Y. Chebotar, M. Kalakrishnan, A. Yahya, A. Li, S. Schaal, and S. Levine. Path integral guided policy search. In ICRA, 2017.

## 附录

A. 未来工作

我们的方法的一个局限性是，表征需要多视点视频来进行训练，而这并不像标准视频那样广泛可用(例如从互联网上)。我们确实分析了我们的方法的一个单视角变体，并且发现它也比基线的imagenet训练的特征实现了改进，但是多视角TCN实现了更好的结果。然而，随着机器人越来越智能，自动录制多视角视频，例如使用立体声摄像机，似乎是一个有前途的方向。我们的方法也可以被看作是一种更一般的多模态嵌入方法的具体实例，在这种方法中，多个感觉模态中的时间联合发生（co-occuring）的事件被嵌入到同一个空间中。从这个角度来看，探索更广泛的模式，如音频和触觉传感，将是未来工作的一个令人兴奋的途径。我们实验的另一个限制是，我们为每个任务(浇注、姿势模仿等)训练一个单独的TCN。对于给定的任务(如浇筑)，TCN会对包含失败浇筑、移动杯子等片段的视频进行训练，而嵌入仅用于学习单个任务，即浇筑。原则上，TCN学到的嵌入应该是与任务无关的，尽管在这种情况下可能需要相当大的数据集。未来工作中一个有趣的方向是研究如何将多个任务嵌入到同一个空间中，本质上是为模仿对象交互行为创建一个通用的表征。在这篇论文中，我们探索了使用时间作为监督信号的学习表征，在未来，模型应该同时从一组不同但互补的信号中学习。

B. 强化学习细节
--------------
假设$p(u_t|x_t)$是一个机器人策略，它定义了任务执行的每个时间步长$t$时以系统状态$x_t$为条件的动作的概率分布。我们雇佣策略搜索优化策略参数$θ$。让$\tau=(x_1,u_1,\cdots,x_T,u_T)$是状态和行为的轨迹。给定一个代价函数$c(x_t;u_t)$,我们定义轨迹成本$c(\tau)= PT t = 1 c (xt;ut)。根据策略的预期成本对策略进行优化
-----------

C. 对象交互分析

在这里，我们用t-SNE可视化从ImageNet-Inception和多视图TCN模型得到的嵌入，使用groundtruth标签着色。每种颜色都是前面定义的5个属性值的唯一组合，也就是说，如果每种颜色都被很好地分隔开，那么模型就可以唯一地识别出5个属性的所有可能组合。事实上，我们在图11中观察到TCN有一定程度的分色，但基线没有

D. 姿态模拟分析

1. 姿态模仿数据:人体训练数据由人体主体和服装对区分的序列组成。每个序列大约4分钟。在无标签的TC监督中，我们收集了大约30对人类(约2小时)，其中人类模仿一个机器人，但没有记录关节标签，以及50个随机运动的机器人序列(约3小时，很便宜的收集)。为了便于人工监督，我们收集了10双人体/服装(大约40分钟，非常昂贵的收集)，同时也记录了关节标签。每个记录的序列是由3个固定在特定角度($0^\circ$，$60^\circ$和$120^\circ$)和距离的智能手机相机捕捉。每个验证和测试集由6对训练中没有见过的人/衣服组成(大约24分钟，非常昂贵的收集)。

距离误差按每个关节的全值范围归一化，产生一个百分比误差。注意，由于模仿任务是主观的，不同的人对映射的理解也不同，因此人的监督信号是相当嘈杂的。事实上，由于人体和抓取机器人之间的生理差异，完美的模仿是不可能的。因此，对我们来说，最好的比较度量标准是观察从人类观测中预测出来的关节角度是否与人类试图模仿的实际关节角度相符

---
**参考**：
1. 论文：Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine [Time-Contrastive Networks: Self-Supervised Learning from Video](https://arxiv.org/abs/1704.06888)
