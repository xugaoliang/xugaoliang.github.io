---
typora-root-url: ../../
---

# Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks

* 作者：Guokun Lai, Wei-Cheng Chang, Yiming Yang, Hanxiao Liu
* 论文：《Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks》
* 地址：https://arxiv.org/abs/1703.07015

---

## 个人总结

本文使用深度神经网络的方法解决多元时间序列预测的问题。总和运用了CNN,RNN,Attn及传统自回归预测的优势。

用CNN捕捉短期局部模式及各元之间的模式，用RNN捕捉长期模式，并从传统方法输入中汲取灵感设计了递归跳跃RNN，用以捕捉超长期模式及明显的周期模式，对于无明显周期模式的时间序列则采用Attn来捕捉关系，用传统AR来捕捉局部尺度。

模型的总体思路是：分为DNN及AR两部分，分别捕捉重复的非线性部分及局部尺度的线性部分。

一个新颖的借鉴点是在深度模型中加入AR部分解决尺度变化问题导致DNN对尺度变化不敏感的问题。

---

## 框架

![图2](/assets/images/深度学习/用深度神经网络建模长短期时间模式/fig2.png)

### Convolutional Component （卷积）

LSTNet的第一层是一个没有池化的卷积网络，它的目标是提取时间维中的短期模式以及变量之间的局部依赖关系。卷积层由多个宽$w$高$n$的 filters (高度设置为相同数量的变量)组成。第$k$个 filter 扫过输入矩阵$X$并产生

$$
h_k = RELU(W_k * X + b_k)
$$

其中$*$表示卷积操作和输出$h_k$是一个向量，而$RELU$函数是$RELU(x) = \max(0,x)$。我们通过在输入矩阵$X$的左边填充0使每个$h_k$的长度为$T$。卷积层的输出矩阵大小为$d_c \times T$，其中$d_c$为 filters 数量。

### Recurrent Component（递归）

卷积层的输出同时输入到递归组件和递归跳越组件(在3.4小节中进行描述)。递归组件是一个带有门控递归单元（GRU)[6]的递归层，使用$RELU$函数作为隐藏的更新激活函数。$t$时刻递归单位的隐状态计算为:

$$
r_t = \sigma(x_tW_{xr}+h_{t-1}W_{hr}+b_r) \\\\
u_t = \sigma(x_tW_{xu}+h_{t-1}W_{hu}+b_u) \\\\
c_t = RELU(x_tW_{xc}+r_t \odot(h_{t-1}W_{hc})+b_c) \\\\
h_t = (1-u_t)\odot h_{t-1}+u_t\odot c_t
$$

其中$\odot$是按元素乘，$\sigma$是 sigmoid 函数，$x_t$是这一层在时刻$t$的输入。这一层的输出是每个时间戳的隐状态，虽然研究人员倾向于使用 tanh 函数作为隐藏更新激活函数，但我们从经验上发现 RELU 的性能更可靠，通过它梯度更容易反向传播。

### Recurrent-skip Component （递归跳跃）

带有GRU[6]和LSTM[15]单元的递归层经过精心设计，以记住历史信息，从而了解相对长期的依赖关系。然而，由于梯度消失，GRU和LSTM在实际应用中往往不能捕捉到非常长期的相关性。我们建议通过一种新的递归跳跃组件来缓解这个问题，该组件利用了真实集合中的周期性模式。例如，每天的用电量和交通使用量都呈现出明显的规律。如果我们要预测今天t点的用电量，季节预测模型中的一个经典技巧是除了最近的记录外还利用历史日t点的记录。由于一个周期(24小时)的长度非常长，以及随后的优化问题，这种类型的依赖关系很难被现成的循环单元捕获。受此技巧的启发，我们开发了一个具有时间跳跃连接的递归结构，以扩展信息流的时间跨度，从而简化优化过程。具体地，在当前的隐藏单元与相邻周期的同一相位的隐藏单元之间添加跳转链接。更新过程可以表述为：

$$
r_t = \sigma(x_tW_{xr}+h_{t-p}W_{hr}+b_r) \\\\
u_t = \sigma(x_tW_{xu}+h_{t-p}W_{hu}+b_u) \\\\
c_t = RELU(x_tW_{xc}+r_t \odot(h_{t-p}W_{hc})+b_c) \\\\
h_t = (1-u_t)\odot h_{t-p}+u_t\odot c_t
$$

其中，该层的输入为卷积层的输出，$p$为跳过的隐藏单元数。对于具有明确周期模式的数据集(例如，每小时电力消耗和交通使用数据集的$p = 24$)，可以很容易地确定$p$的值，否则必须进行调优。在我们的实验中，我们根据经验发现，即使是后一种情况，调优后的$p$也可以显著地提高模型的性能。此外，可以很容易地扩展LSTNet以包含跳跃长度$p$的变体。

我们使用一个稠密层来合并递归和递归跳跃组件的输出。稠密层的输入包括$t$时刻递归组件的隐藏状态，记为$h_t^R$, $t-p+1$时刻到$t$时刻的递归跳跃组件的$p$个隐藏状态，记为$h_{t-p+1}^S,h_{t-p+2}^S,\cdots,h_{t}^S$。密层的输出计算为:

$$
h_t^D = W^Rh_t^R + \sum_{i=0}^{p-1}W_i^Sh_{t-i}^S+b
$$

其中$h_t^D$是图2中上半部分神经网络在$t$时刻的预测结果。

### Temporal Attention Layer （时间注意力）

然而，递归跳跃层需要一个预定义的超参数$p$，这对于非季节性的时间序列预测是不利的，或者它的周期长度是随时间变化的。为了解决这个问题，我们考虑了另一种方法，即注意力机制[1]，它学习输入矩阵每个窗口位置的隐藏表征的加权组合。具体来说,在时刻$t$的关注力权重$\alpha_t \in \mathbb{R}^q$计算为：

$$
\alpha_t = AttnScore(H_t^R,h_{t-1}^R)
$$

其中$H_t^R = [ h_{t-q}^R,\cdots,h_{t-1}^R ]$是一个矩阵，它巧妙地将RNN的隐藏表征按列堆叠起来，而AttnScore是一些相似函数，比如点积、余弦或由一个简单的多层感知器参数化。

时间注意力层最终的输出是加权上下文向量$c_t=H_t\alpha_t$和最后窗口隐藏表征$h_{t-1}^R$的拼接，连上线性投影运算：

$$
h_t^D = W[c_t;h_{t-1}^R]+b
$$

### Autoregressive Component （自回归）


由于卷积和递归组件的非线性特性，神经网络模型的一个主要缺点是输出的比例对输入的比例不敏感。遗憾的是，在具体的真实数据集中，输入信号的尺度会以非周期性的方式不断变化，这大大降低了神经网络模型的预测精度。在第4.6节中给出了一个失败的具体例子。为了解决这一缺陷，我们将LSTNet的最终预测分解为一个线性部分(主要关注局部尺度问题)和一个包含重复模式的非线性部分，这在本质上与公路网[29]类似。在LSTNet体系结构中，我们采用经典的自回归(AR)模型作为线性分量。$h_t^L \in \mathbb{R}^n$表示AR分量的预测结果, AR模型的系数为$W^{ar}\in \mathbb{R}^{q^{ar}}$ 和 $b^{ar}\in \mathbb{R}$，其中，$q^{ar}$是覆盖输入矩阵的输入窗口的大小。注意，在我们的模型中，所有维都共享相同的一组线性参数。AR模型表示如下:

$$
\tag{5}
h_{t,i}^L = \sum_{k=0}^{q^{ar}-1} W_k^{ar} y_{t-k,i}+b^{ar}
$$

将神经网络部分的输出与AR部分的输出进行积分，得到LSTNet的最终预测结果:

$$
\hat{Y}_t = h_t^D + h_t^L
$$

其中$\hat{Y}_t$为模型在时间戳$t$处的最终预测。

### 数据及结论

![表1](/assets/images/深度学习/用深度神经网络建模长短期时间模式/tab1.png)

![图3](/assets/images/深度学习/用深度神经网络建模长短期时间模式/fig3.png)

![表2](/assets/images/深度学习/用深度神经网络建模长短期时间模式/tab2.png)

LSTNet-skip和LSTNet-Attn，在具有周期模式的数据集上，特别是在大 horizons 的设置上，持续提高了最优结果。此外，LSTNet在 horizon 为24时，在太阳能、交通和电力数据集上的RSE度量分别比强神经基线RNN-GRU高9.2%、11.7%和22.2%，证明了该框架设计对于复杂重复模式的有效性。更重要的是，当周期模式$q$在应用程序中不清楚时，用户可以考虑使用LSTNet-attn替代LSTNet-skip，因为前者仍然比基线有相当大的改进。但是提议的LSTNet在汇率数据集上比AR和LRidge略差。因为在汇率数据集中没有重复模式。当前的结果为LSTNet模型在对数据中出现的长期和短期依赖模式建模方面的成功提供了经验证据。另外，LSTNet在反映基线中与较优的AR和LRidge表现相当。


### 消融研究结论

* 使用LST-Skip或LST-attn可获得每个数据集上的最佳结果。
* 从整个模型中删除AR组件(在LSTw/oAR中)导致大多数数据集的性能显著下降，显示了AR组件的关键作用。
* 移除Skip和CNN组件 (LSTw/oCNN 或 LSTw/oskip)导致一些数据集的性能大幅下降，但不是所有数据集。LSTNet的所有组件一起导致了我们的方法在所有数据集上的健壮性能。

结论是，我们的架构设计在所有的实验环境中都是最健壮的，特别是在大 horizons 上。

![图6](/assets/images/深度学习/用深度神经网络建模长短期时间模式/fig6.png)

图6：LSTw/oAR(a)和LST-Skip(b)的预测时间序列(红色) vs $horizon=24$的电力数据集上的真实数据(蓝色)

至于为什么AR组件会有如此重要的作用，我们的解释是AR通常对数据的规模变化是健壮的。为了从经验上验证这种直觉，我们在图6中绘制了1到5000小时的电力消耗数据集中时间序列信号的一维(一个变量)，其中蓝色曲线是真实数据，红色曲线是系统预测信号。我们可以看到真正的消耗在第1000个小时左右突然增加，LSTNet-Skip成功地捕捉到了这种突然的变化，但是LSTw/oAR没有做出适当的反应。

### 4.7 长期和短期模式的混合

![图7](/assets/images/深度学习/用深度神经网络建模长短期时间模式/fig7.png)


为了说明LSTNet在对时间序列数据中短期和长期重复模式的混合建模方面的成功，图7比较了LSTNet和VAR在 Traffic 数据集中特定时间序列(输出变量之一)上的性能。如4.3节所述，交通数据呈现出两种重复模式，即日重复模式和周重复模式。从图7中我们可以看到，周五和周六的交通占用率的真实模式(用蓝色表示)非常不同，而周日和周一则完全不同。图7是VAR模型的预测结果((a)部分)和LSTNet ((b)部分)的交通流量监测传感器,他们根据RMSE结果验证集选择超参数。图中显示的VAR模型只能处理短期模式。VAR模型的预测结果模式只依赖于预测前一天。我们可以清楚地看到，其在周六(第2、9个峰值)和周一(第4、11个峰值)的结果与ground truth不同，其中周一(工作日)的ground truth有两个峰值，周六(周末)一个峰值。相反，我们提出的LSTNet模型在工作日和周末有两种模式。这个例子证明了LSTNet模型同时记忆短期和长期重复模式的能力，这是传统预测模型所不具备的，它在真实世界时间序列信号的预测任务中是至关重要的。