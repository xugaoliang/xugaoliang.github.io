---
typora-root-url: ../../../
---

# 【译】用多类N对损失目标提升深度度量学习

## 2. 初级：距离度量学习

$$ 
\mathcal{L}_{\mathrm{count}}^m (x_i,x_j;f) = 1\{y_i=y_j\} ||f_i-f_j||_2^2 + 1\{y_i \ne y_j\} \max(0,m-||f_i-f_j||_2)^2
$$

其中$m$是表示不同类之间的距离大于$m$,三元损失与对比损失有相似的精神，但是是由三元组成，分别为query,正例（相对于query)，负例。

$$
\mathcal{L}_{\mathrm{tri}}^m(x,x^+,x^-;f) = \max(0,||f-f^+||_2^2-||f-f^-||_2^2+m)
$$

与对比损失相比，三元损失只需要正例之间的相同点和负例之间的不同点，负例之间要大于一个间距$m$。这两种损失函数都存在收敛速度慢的问题，并且通常需要昂贵的数据抽样方法来提供特殊的对或一个三元组来加速训练。

## 3. 用多个负例进行深度度量学习

![图1](/assets/images/计算机视觉/用多类N对损失目标提升深度度量学习/fig1.png)

图1：深度度量学习三元损失（左）和 (N+1)元损失（右)。对深度网络的嵌入向量$f$进行训练，满足各损失的约束条件。三元损耗在一次推动一个负例的同时，拉动正例。另一方面，(N+1)元损失会同时推动N-1个负例，这是基于它们与输入实例的相似性。

![图2](/assets/images/计算机视觉/用多类N对损失目标提升深度度量学习/fig2.png)

图2：三元损失，（N+1)元损失,和多类N对损失的训练批次构造。假设每一对都属于不同的类，（c)中的N对批次构造利用所有$2\times N$的嵌入向量，用$\{f_i\}_{i=1}^N$作为 queries 构建$N$个不同的(N+1)元组，然后我们将这些$N$个不同的元组聚集在一起，形成 N-pair-mc损失。对于包含$N$个不同 queries 的批次，三元损失需要经过 $3N$次才能计算所需的嵌入向量。（N+1)元损失需要$(N+1)N$次，N-pair-mc只需要$2N$次。

三元损失背后的基本原理如下:对于一个输入(query)实例，我们同时希望缩短其嵌入向量与正例之间的距离，扩大负例之间的距离。然而，在一次更新中，三元损失只比较一个实例与一个负例，而忽略其余的负例。因此，仅保证了实例的嵌入向量远离被选择的负例的类，但未必远离其他负类。因此，我们最终只能从有限的负类的选择中区分一个例子，但仍然与许多其他类保持小的距离。在实践中，希望在足够多的随机采样三元组进行循环后，最终的距离度量能够得到正确的平衡;但是单个更新仍然是不稳定的，收敛会很慢。具体来说，在训练接近结束时，大多数随机选择的负例不再产生非零的三元损失误差。

改善基本的三元损失的一个明显方法是选择一个违反三元约束的反例。然而，对于深度度量学习来说，使用大量的输出类，硬负例挖掘的代价可能很高。我们寻求一种替代方法:一个损失函数，它为每个更新调用多个负例，如图1所示。在这种情况下，一个输入示例将与来自多个类的负例进行比较，同时需要将它与所有这些类区分开来。理想情况下，我们希望loss函数能够同时包含所有类的示例。但由于基于神经网络的嵌入存在记忆瓶颈，使得大规模深度度量学习难以实现。在这个思考过程的激励下，我们提出了一个新颖的、计算上可行的损失函数，如图2所示，它通过同时推出N个例子来近似我们的理想损失。

### 3.1 学习从多个负例中进行识别

我们将提出的方法形式化，并对其进行优化以使其从多个负例中识别出一个正例。考虑一个$(N+1)$个元祖的训练例子${x,x^+,x_1,\cdots,x_{N-1}}$:$x^+$是$x$的正例，$\{x_i\}_{i=1}^{N-1}$是负例，$(N+1)$元损失定义如下：

$$
\mathcal{L}(x,x^+,\{x_i\}_{i=1}^{N-1};f)=\log(1+\sum_{i=1}^{N-1}\exp(f^\top f_i-f^\top f^+))
$$

其中$f(\cdot;\theta)$是一个嵌入内核，用深层神经网络定义。回想一下，我们希望元组损失中负例涉及所有类，但在输出类$L$数量很大的情况下，这是不切实际的;即使我们将每个类的负例数量限制为一个，执行标准优化(如随机梯度下降法，SGD)仍然是非常困难的，因为它的批处理大小和$L$一样大。

当$N=2$时，对应的$(2+1)$元损失与三元损失非常相似，因为每对输入都只有一个正例和一个负例：

$$ 
\mathcal{L}_{(2+1)-\mathrm{tuplet}}(\{x,x^+,x_i;f\}) = \log(1+\exp(f^\top f_i-f^\top f^+)) 
$$

$$ 
\mathcal{L}_{\mathrm{triplet}}(\{x,x^+,x_i;f\}) = \max(0,f^\top f_i-f^\top f^+) 
$$

的确，在温和的假设下，我们可以证明如果嵌入$f$最小化$\mathcal{L}_{(2+1)-\mathrm{tuplet}}$，只有当他最小化 $\mathcal{L}_{\mathrm{triplet}}$，即，两个损失函数是等价的。当$N>2$时，我们进一步讨论$(N+1)-\mathrm{tuplet}$损失优于三元损失，我们将$(N+1)$元损失和三元损失在各分项上比较。其中$(L+1)$元损失每个负类加上一个实例损失可以以下形式

$$
\log(1+\sum_{i=1}^{L-1}\exp(f^\top f_i - f^\top f^+)) = -\log \frac{\exp(f^\top f^+)}{\exp(f^\top f^+)+\sum_{i=1}^{L-1}\exp(f^\top f_i)}
$$

上式与多类别logistic损失（即 softmax 损失）是相似的，当我们把$f$视作特征向量时。$f^+$和$f_i$视作权重向量，右边的分母为似然函数$P(y=y^+)$的 partition 函数。我们观察到对应于$(N+1)$元的 partition 函数近似于$(L+1)$元，且值更大的$N$，近似的更精确。因此,$(N+1)$元的损失比三元损失能更好的近似于$(L+1)$元损失。

### 3.2 有效深度度量学习的N对损失

假设我们直接将$(N+1)$元损失应用到深度度量学习框架中。当SGD的批大小为$M$，有$M\times (N+1)$个实例需要通过$f$一次更新。由于每个批处理需要评估的实例数量以二次增长到M和N，因此，在非常深的卷积网络中扩展训练再次变得不切实际。

现在，我们介绍一个有效的批构造以避免过多的计算负担。记$\{(x_1,x_1^\top),\cdots,(x_N,x_N^\top)\}$为$N$对来自$N$个不同类别的实例，即，$y_i\ne y_j,\forall i\ne j$。我们构建$N$个元组定义为$\{S_i\}_{i=1}^N$，来自于$N$对，其中$S_i=\{x_i,x_1^+,x_2^+,\cdots,x_N^+\}$,这里，$x_i$是$S_i$的 query,$x_i^+$是正例，$x_j^+,j\ne i$是负例。图2(c)说明了这个批构造过程。对应的$(N+1)$元损失，我们称之为多类$N$对损失（N-pair-mc),可以表示为：

$$
\mathcal{L}_{N-pair-mc}(\{(x_i,x_i^+)\}_{i=1}^N;f) = \frac{1}{N} \sum_{i=1}^N \log(1+\sum_{j\ne i} \exp(f_i^\top f_j^+ - f_i^\top f_i^+))
$$

我们的N对损失的数学公式与现有的其他方法有着相似的精神，例如邻域成分分析(NCA)和带抬升结构的三元损失。然而，在大规模数据集及类别数量上使用CNNs作为嵌入核，我们的批次构建的目的是实现$(N+1)$元损失的最大潜力。因此，所提出的 N-pair-mc 损失是一个由$(N+1)$元损失作为构建块损失函数和N对结构作为高可扩展性训练的关键组成的新框架。在后面的4.4节中，我们实证地展示了我们的N-pair-mc损失框架相对于其他小批量施工方法的优势。

最后，我们注意到元批次构建并不是针对$(N+1)$元的损失。我们称这组损失函数为 N-pair 损失。例如，当集成到标准三元损失时，我们得到以下 1-vs-1 N-pair 损失(N-pair-ovo):

$$
\mathcal{L}_{N-pair-ovo}(\{(x_i,x_i^+)\}_{i=1}^N;f)=\frac{1}{N}\sum_{i=1}^N \sum_{j\ne i} \log(1+\exp(f_i^\top f_j^+ - f_i^\top f_i^+))
$$

#### 3.2.1 硬负类挖掘

硬负例数据挖掘被认为是许多基于三元的距离度量学习算法必不可少的组成部分，以提高收敛速度或最终的判别性能。当输出类的数量不是太大时，N-pair损失可能是不必要的，因为大多数负类的例子已经被联合考虑了。当
我们对具有大输出类的数据集进行训练，N-pair 损失可以从精心选择的冒名顶替者示例中受益。对于来自大量类的多个实例的深度嵌入向量的计算是非常困难的。此外，对于 N-pair 损失，理论上需要N个相互为负的类，这大大增加了负搜索的难度的困难。为了克服这种困难，我们提出了负“类”挖掘，与之相对的是负“实例”挖掘，它贪婪地选择了相对有效的负类。更具体地说，n对损失的负类挖掘可以执行如下:

1. 评价嵌入向量:随机选择大量的输出类C;对于每个类，随机传递几个(一两个)示例来提取它们的嵌入向量。
2. 选择负面类:从步骤1的C类中随机选择一个类。接下来，贪婪地添加一个违反三元组约束最多的新类。当出现平局时，我们随机选择一个打成平手的类。
3. Finalize N-pair:从步骤2中选择的每个类中抽取两个例子。

#### 3.2.2 嵌入向量的L2范数正则化

$f^\top f^+$的数值不仅受$f^+$方向的影响，也受它的 norm 的影响，即使分类决策仅仅由方向决定。归一化可以是避免这种情况的一种解决方案，但是他对于我们的损失公式来说太严格了，因为它限制了$|f^\top f^+|$的值小于1，使优化变得困难。相反，我们调整嵌入向量的$L^2$范数要小。

---
**参考**：
1. 论文：Kihyuk Sohn [Improved Deep Metric Learning with
Multi-class N-pair Loss Objective](http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf)



