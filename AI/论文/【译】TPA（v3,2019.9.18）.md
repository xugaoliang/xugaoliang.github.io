# 【译】用于多元时间序列预测的时间模式注意力（v3,2019.9.18）

* 作者：Shun-Yao Shih, Fan-Keng Sun, Hung-yi Lee
* 论文：《Temporal Pattern Attention for Multivariate Time Series Forecasting》
* 地址：https://arxiv.org/abs/1809.04206v3

---

## 个人总结

---

**摘要**: 多元时间序列数据的预测，如用电量预测、太阳能发电预测、复调钢琴曲预测等，有许多有价值的应用。然而，时间步和序列之间复杂的非线性相互依赖关系使这一任务变得复杂。为了获得准确的预测，对时间序列数据的长期依赖性建模至关重要，而这可以通过具有注意力机制的递归神经网络(RNNs)来实现。典型的注意力机制是在每一个时间步上检查信息，并选择相关信息来帮助生成输出;但是，它无法捕获跨多个时间步长的时间模式。在本文中，我们建议使用一组滤波器来提取时间不变的时间模式，类似于将时间序列数据转换成它的“频域”。在此基础上，我们提出了一种新的注意力机制来提取相关的时间序列，并利用其频域信息进行多变量预测。我们将提出的模型应用于几个真实的任务中，并在几乎所有的情况下实现最先进的性能。我们的源代码可以在 https://github.com/gantheory/TPA-LSTM 找到。

![图1](/assets/images/深度学习/TPA/fig1.png)

图1：原油、汽油和木材的历史价格。为了简单起见，省略了单位，并对刻度进行了标准化。

## 1. 介绍

在日常生活中，时间序列数据无处不在。我们观察由离散时间步长传感器产生的演化变量，并将它们组织成时间序列数据。例如，家庭用电量、道路占用率、货币汇率、太阳能发电量，甚至是音符都可以被视为时间序列数据。在大多数情况下，收集的数据往往是多元时间序列(MTS)数据，如多个客户的用电量，由当地电力公司跟踪。不同系列之间可能存在复杂的动态相互依赖关系，这些相互依赖关系很重要，但很难捕获和分析。
分析师通常寻求根据历史数据预测未来。不同系列之间的相关性建模得越好，预测结果就越准确。如图1所示，原油价格对汽油价格的影响较大，但对木材价格的影响较小。因此，考虑到汽油是由原油生产的，而木材不是，我们可以利用原油价格来预测汽油的价格。

在机器学习中，我们希望模型能够自动地从数据中学习这些依赖关系。机器学习已被应用于时间结构分析的分类和预测[G。Zhang and Hu(1998)， Zhang(2003)， Lai et al.(2018)Lai, Chang, Yang, and Liu, Qin et al.(2017)Qin, Song, Cheng, Cheng, Jiang, and Cottrell]。在分类中，机器学会给一个时间序列分配一个标签，例如通过读取医疗传感器的值来评估病人的分类。在预测中，机器根据过去的观测数据预测未来的时间序列。例如，可以根据历史测量来预测未来几天、几周或几个月的降水。我们越想预测未来，就越困难。

在使用深度学习进行 MTS 预测时，经常使用递推神经网络(RNNs) [David E. Rumelhart and Williams(1986)， j.w werbos (1990)， Elman(1990)]。然而，在时间序列分析中使用RNNs的一个缺点是它们在管理长期依赖关系方面的弱点，例如每天记录的序列中的年度模式[Kyunghyun Cho和Bengio(2014)]。注意力机制[Luong et al。(2015)Luong,Pham,Manning,Bahdanau et al。(2015) Bahdanau,Cho,和Bengio],最初用于encoder-decoder [Sutskever et al。(2014) Sutskever, Vinyals,和Le)网络,一定程度上缓解这一问题,从而促进RNN的有效性(Lai et al .(2018),Chang、Yang 和 Liu]。

在这篇论文中，我们提出了时间模式注意力，一个新的注意机制的 MTS 预测，其中我们使用术语“temporal pattern”指的是跨越多个时间步的任何时间不变模式。典型的注意力机制识别出与预测相关的时间步，并从这些时间步中提取信息，这对 MTS 预测具有明显的局限性。考虑图1中的示例。要预测汽油的价值，机器必须学会关注“原油”而忽略“木材”。在我们的时间模式注意中，机器学习选择相关的时间序列，而不是像典型的注意力机制那样选择相关的时间步长。

此外，时间序列数据往往包含明显的周期性时间特征，这对预测至关重要。然而，跨越多个时间步长的周期模式很难被典型的注意机制识别，因为它通常只关注几个时间步。在时间模式注意力方面，我们引入了卷积神经网络(convolutional neural network, CNN) [LeCun and Bengio(1995)， a . Krizhevsky and Hinton(2012)]来从每个单独的变量中提取时间模式信息。
本文的主要贡献总结如下:

* 我们引入了一个新的注意力概念，在这个概念中，我们选择相关的变量，而不是相关的时间步。该方法简单、通用，适用于RNN。
* 我们使用玩具例子来验证我们的注意力机制，使模型能够提取时间模式，并集中在不同的时间序列的不同时间步。
* 通过对真实数据的实验结果证明，从周期性和部分线性到非周期性和非线性任务，我们表明，提出的注意力机制实现了跨多个数据集的 state-of-the-art 结果。
* 在我们的注意力机制中学习的 CNN filters 显示出有趣和可解释的行为。

本文的其余部分组织如下。在第2部分我们回顾了相关的工作，在第3部分我们描述了背景知识。然后，在第4部分，我们描述了所提出的注意机制。接下来，我们将在第5节的玩具示例、第6节的MTS和复调音乐数据集上展示和分析我们的注意力机制。最后，我们在第7节结束。

## 2. 相关工作

最著名的线性单变量时间序列预测模型是自回归综合移动平均(ARIMA) [G。E. Box和Ljung(2015)]，其中包含其他自回归时间序列模型，包括自回归(AR)、移动平均(MA)和自回归移动平均(ARMA)。此外，线性支持向量回归(linear support vector regression, SVR) [Cao和Tay(2003)， Kim(2003)]将预测问题视为具有时变参数的典型回归问题。然而，这些模型大多局限于线性单变量时间序列，不能很好地适用于MTS。为了预测 MTS 数据，向量自回归（VAR)被提了出来，他是基于AR模型的一种 generalization （普遍化，一般化）。VAR可能是 MTS 预测中最著名的模型。然而，基于AR和基于VAR的模型都没有捕捉到非线性。针对这一目的，人们在非线性模型上进行了大量的研究，如基于核方法的时间序列预测[Chen et al.(2008)Chen, Wang, and Harris]，集成[Bouchachia和Bouchachia(2008)]，高斯过程[Frigola和Rasmussen(2014)]或 regime switching(状态转换)[Tong and Lim(2009)]。然而，这些方法适用于预先确定的非线性，并且可能无法识别不同 MTS 的非线性的不同形式。

近年来，深度神经网络因其具有捕获非线性相互依赖关系的能力而受到广泛关注。长短期记忆(LSTM) [Hochreiter and Schmidhuber(1997)]是递归神经网络的一种变体，在几个NLP任务中显示出良好的结果，也被用于MTS预测。这方面的工作是从使用朴素RNN开始的。Connor和Martin(1991)]，将ARIMA和多层感知器相结合的混合模型进行了改进[G。Zhang和Hu(1998)， Zhang(2003)，Jain和Kumar(2007)]，然后最近进展到RNN的动态Boltzmann机[Dasgupta 和 Osogami(2017)]。虽然这些模型可以应用于 MTS，但主要针对的是单变量或双变量时间序列。

据我们所知，LSTNet [Lai et al.(2018)Lai, Chang, Yang, and Liu]是第一个专门用于MTS预测的模型，时间序列多达数百个。LSTNet使用CNNs捕获短期模式，而LSTM或GRU用于记忆相对长期的模式。然而，在实践中，由于训练的不稳定性和梯度消失问题，LSTM和GRU不能记住非常长期的相互依赖关系。为了解决这个问题，LSTNet添加了一个递归跳跃层或一个典型的注意力机制。整体模型的一部分是传统的自回归，这有助于减轻神经网络的尺度不敏感性。尽管如此，与我们提出的注意力机制相比，LSTNet有三个主要缺点:(1)为了匹配数据的周期，必须手动调整递归-跳跃层的跳跃长度，而我们提出的方法是自己学习周期模式;(2) LSTNet-Skip模型是专门为具有周期性模式的 MTS 数据设计的，而我们提出的模型，如我们的实验所示，是简单的，可以适应各种数据集，甚至是非周期性数据集;(3) LSTNet-Attn模型中的注意力层与典型的注意机制一样，选择一个相关的隐状态，而我们提出的注意力机制选择相关的时间序列，该机制更适合于MTS数据。

## 3. 预赛

在这一节中，我们简要地描述了与我们提出的模型相关的两个基本模块:RNN模块和典型的注意力机制。

### 3.1 递归神经网络

给定一个信息序列$\{x_1,x_2,\cdots,x_t\}$，其中$x_i\in \mathbb{R}^n$, RNN一般定义一个递归函数$F$，对每个时间步$t$，计算$h_t \in \mathbb{R}^m$:

$$
\tag{1}
h_t = F(h_{t-1},x_t)
$$

函数$F$的实现取决于使用哪种RNN单元。

长短时记忆(Long short-term memory, LSTM) [Hochreiter and Schmidhuber(1997)]单元被广泛使用，其递归函数略有不同:

$$
\tag{2}
h_t,c_t = F(h_{t-1},c_{t-1},x_t)
$$

由下式定义:

$$
\tag{3}
i_t = sigmoid(W_{x_i}x_t + W_{h_i}h_{t-1})
$$

$$
\tag{4}
f_t = sigmoid(W_{x_f}x_t + W_{h_f}h_{t-1})
$$

$$
\tag{5}
o_t = sigmoid(W_{x_o}x_t + W_{h_o}h_{t-1})
$$

$$
\tag{6}
c_t = f_t \odot c_{t-1} + i_t \odot tanh(W_{x_g}x_t + W_{h_g}h_{t-1})
$$

$$
\tag{7}
c_t = o_t \odot tanh(c_t)
$$

其中 $i_t,f_t,o_t,c_t \in \mathbb{R}^m$，$W_{x_i},W_{x_f},W_{x_o},W_{x_g} \in \mathbb{R}^{m\times n}$，$W_{h_i},W_{h_f},W_{h_o},W_{h_g} \in \mathbb{R}^{m\times m}$，$\odot$表示按元素相乘。

### 3.2 典型的注意力机制

在典型的注意力机制[Luong et al.(2015)Luong, Pham, and Manning, Bahdanau et al.(2015)Bahdanau, Cho, and Bengio]中，给定前面的状态$H = \{h_1,h_2,\cdots,h_{t-1}\}$,上下文向量$v_t$是从前面的状态中提取出来的。$v_t$是$H$中每一列$h_i$的加权和，它表示与当前时间步相关的信息。$v_t$与当前状态$h_t$进一步集成，从而得出预测结果。

假设一个得分函数$f:\mathbb{R}^m \times \mathbb{R}^m \rightarrow \mathbb{R}$,计算其输入向量之间的相关性。形式上，我们有以下公式来计算上下文向量$v_t$:

$$
\tag{8}
\alpha_i = \frac{\exp(f(h_i,h_t))}{\sum_{j=1}^{t-1} \exp(f(h_j,h_t))}
$$

$$
\tag{9}
v_t = \sum_{i=1}^{t-1}\alpha_i h_i
$$

## 4. 时间模式注意力

![图2](/assets/images/深度学习/TPA/fig2.png)

图2：提出的注意力机制。$h_t$表示RNN在时刻$t$的隐状态.有$k$个长度为$w$的一维CNN filters，用不同颜色的矩形表示。每个 filter 对隐状态的m个特征进行卷积，得到一个$m$行$k$列的矩阵$H^C$。接下来，计分函数通过与当前隐状态$h_t$比较,计算$H^C$的每一行的权重
。然后，将权重标准化并将$H_C$的行按其相应的权值进行加权求和，得到$V_t$。最后，我们将$V_t$、$h_t$串联起来，执行矩阵乘法生成$h'_t$,用来创建最终的预测值。

虽然之前的工作主要集中在通过不同的设置来改变基于注意力的模型的网络结构来提高不同任务的性能，但是我们认为将典型的注意机制应用于 RNN 进行 MTS 预测存在一个严重的缺陷。典型的注意力机制选择与当前时间步长相关的信息，上下文向量$v_t$为以前RNN隐状态列向量的加权和，$H = \{h_1,h_2,\cdots,h_{t−1}\}$。这种设计适合于每个时间步骤都包含一条信息的任务，例如，一个NLP任务，其中每个时间步骤对应一个单词。如果在每一个时间步中有多个变量，它就不能忽略在预测效用方面有噪声的变量。此外，由于典型的注意力机制将信息平均在多个时间步长上，因此无法检测出对预测有用的时间模式。

图2显示了所提议的模型的概述。在该方法中，给定之前的RNN隐藏状态$H\in \mathbb{R}^{m\times(t-1)}$，所提出的注意力机制基本上只关注其行向量。行上的注意力权重选择那些有助于预测的变量。由于上下文向量$v_t$现在是包含跨多个时间步长的信息的行向量的加权和，所以它捕获时间信息。

### 4.1 问题公式化

在MTS预测中，给定一个MTS, $X = \{x_1,x_2,\cdots, x_{t−1}\}$，其中$x_i\in \mathbb{R}^n$ 表示第$i$时刻的观测值，任务是预测$x_{t−1+\Delta}$值,其中$\Delta$对于不同的任务是一个固定的 horizon。我们表示相应的预测为$y_{t-1+\Delta}$,和真实值为$\hat{y}_{t-1+\Delta}=x_{t-1+\Delta}$。此外，对于每个任务，我们只使用$\{x_{t-w},x_{t-w+1},\cdots,x_{t-1}\}$来预测$x_{t-1+\Delta}$，其中$w$为窗口大小。这是一种常见的做法[Lai et al.(2018)Lai, Chang, Yang, and Liu, Qin et al.(2017)Qin, Song, Cheng, Cheng, Jiang, and Cottrell]，因为假设窗口前面没有有用的信息，因此输入是固定的。

### 4.2 使用CNN进行时间模式检测

CNN的成功很大程度上归功于它捕捉各种重要信号模式的能力;因此，我们使用CNN对$H$的行向量应用CNN过滤器来增强模型的学习能力，具体来说，我们有$k$个过滤器$C_i \in \mathbb{R}^{1\times T}$，其中$T$是我们所关注的最大长度。若未指定，则假设$T = w$，卷积运算得$H^C \in \mathbb{R}^{n\times k}$，其中$H_{i,j}^C$表示第$i$行向量和第$j$个滤波器的卷积值。在形式上，这个操作由

$$
\tag{10}
H_{i,j}^C = \sum_{l=1}^w H_{i,(t-w-1+l)} \times C_{j,T-w+l}
$$

### 4.3 提出的注意力机制

我们用$H^C$的行向量的加权和计算$v_t$。下面是定义得分函数$f:\mathbb{R}^k \times \mathbb{R}^m \rightarrow \mathbb{R}$来评价相关性:

$$
\tag{11}
f(H_i^C,h_t) = (H_i^C)^\top W_\alpha h_t
$$

其中$H_i^C$是$H^C$的第$i$行，$W_\alpha \in \mathbb{R}^{k\times m}$,注意力权重$\alpha_i$由下式得到：

$$
\tag{12}
\alpha_i = sigmoid(f(H_i^C,h_t))
$$

请注意，我们使用 sigmoid 激活函数而不是 softmax，因为我们希望有多个变量对预测有用。

完成这个过程,$H^C$的行向量加权$\alpha_i$获取上下文向量$v_t\in \mathbb{R}^k$

$$
\tag{13}
v_t = \sum_{i=1}^n \alpha_i H_i^C
$$

然后对$v_t$和$h_t$进行积分，得到最终的预测结果

$$
\tag{14}
h'_t = W_h h_t + W_v v_t
$$

$$
\tag{15}
y_{t-1+\Delta} = W_{h'}h'_t
$$

其中$h_t,h'_t\in \mathbb{R}^m$,$W_h\in \mathbb{R}^{m\times m}$,$W_v \in \mathbb{R}^{m\times k}$ 和 $W_{h'} \in \mathbb{R}^{n\times m}$和$y_{t-1+\Delta} \in \mathbb{R}^n$

## 5. 在玩具实例上对提出的注意力的分析

![图3](/assets/images/深度学习/TPA/fig3.png)

图3：没有相互依赖关系的第一种玩具示例的可视化(左)和有相互依赖关系的第二种玩具示例(右),$D = 6$，意味着每个例子中有6个时间序列。

为了阐述传统注意力机制的失效和相互依赖的影响，我们研究了不同注意力机制对两类人工构建玩具的表现。

在第一类玩具的例子,第$i$个时间序列的第$t$个时间步的定义为$sin(\frac{2\pi it}{64})$,也就是说,每个时间序列是一个不同周期的正弦波。注意，在第一类中任何两个时间序列都是相互独立的，因此不存在相互依赖性。

第二类的玩具通过混合时间序列来给第一种类型增加相互依赖性，因此第$i$个时间序列的第$t$个时间步:

$$
\tag{16}
\sin(\frac{2\pi it}{64}) + \frac{1}{D}\sum_{j=1,j\ne i}^D sin(\frac{2\pi jt}{64})
$$

其中$D$是时间序列的数目，这两种玩具实例都在图3中可视化，其中$D=6$

以下分析中的所有模型均训练窗口尺寸$w = 64$, horizon $\Delta=1$，参数数量相似。在这个设置中，每个玩具示例包含64个样本。每一个样本中的每个时间序列包含从t = 0到63的 Eq.16 的值，我们可以移动一个时间步长得到第二个样本的值，从t = 1到64。对于最后一个样本，我们使用t = 63到126之间的值作为相应的输入序列。注意，t = 64到127之间的值等于t = 0到63之间的值。我们针对$D =\{1,6,11,\cdots,56\}$和记录训练的平均绝对值损失。没有验证和测试数据，因为本节的目的是证明我们的注意力比典型的注意力更适合MTS数据，而不是我们的注意力的泛化能力。结果如图4所示。

![图4](/assets/images/深度学习/TPA/fig4.png)

图4：没有相互依赖关系的第一种玩具示例，在$log10$的标准偏差范围的平均绝对损失（左）。和有相互依赖关系的第二种玩具示例（右）。都在10次运行中。基线表示所有预测都为0的损失。

### 5.1 传统注意力机制的失败

直观地，对于第一个玩具例子，模型可以通过记忆恰好出现在前一个周期的值来准确地预测下一个值。然而，我们知道不同的时间序列有不同的周期，这意味着有一个好的预测，模型应该能够为不同的序列回看不同的时间步长。从这一点来看，很明显，传统注意力机制的失败来自于只提取前一个时间步，而忽略其他时间步中的信息。另一方面，我们的注意力机制对CNN滤波器从RNN隐状态的行向量中提取的特征进行关注，使得模型能够跨多个时间步长选择相关信息。

上述解释是验证了在图4中左边的图,我们观察到当$D\gg 1$时,具有 Luong 注意力的 LSTM 的性能与其他相比很差。注意，所有的模型都有相似的参数量，这意味着与有Luong注意力的LSTM相比，没有注意力的LSTM具有更大的隐藏大小。因此,当$D\gg 1$时，没有注意力的 LSTM 优于 有 Luong 注意力的 LSTM,因为更大的隐藏大小有助于模型做出预测,而 Luong 注意力几乎是无用的。相反，我们的注意力是有用的，所以平均而言，有我们注意力的 LSTM 比没有注意力的 LSTM 要好，尽管它的隐藏大小更小。同样，将 CNN 从我们的注意力中移除，与表4中的“Sigmoid-W/o CNN”单元相同的模型并不影响性能，这意味着我们的按特征的注意力是必不可少的。

### 5.2 相互依赖关系的影响

当MTS数据中存在相互依赖关系时，最好利用这种相互依赖关系进一步提高预测精度。图4中的右侧图显示，有Luong注意力的LSTM和没有注意力的LSTM都不会从增加的相互依赖中受益，因为损失值保持不变。另一方面，当存在相互依赖时，LSTM的损失较低，这表明我们的注意力成功地利用了相互依赖来促进MTS预测。同样，将CNN从我们的注意力中移开并不会影响这种情况下的性能。

### 6 实验和分析

在本节中，我们将首先描述用于进行实验的数据集。接下来，我们展示了我们的实验结果和对LSTNet的预测的可视化。然后，我们讨论了消融研究。最后，我们分析了在何种意义上，CNN滤波器类似于DFT中的基。

### 6.1 数据集

为了评估该机制的有效性和泛化能力，我们使用了两种不同类型的数据集:典型的MTS数据集和复调音乐数据集。

典型MTS数据集由[Lai et al.(2018)Lai, Chang, Yang, and Liu]发表;有四个数据集:

* Solar Energy(太阳能):2006年阿拉巴马州光伏电站的太阳能发电量数据。
* Traffic(交通):加州运输部提供的数据(2015-2016年)，描述了旧金山湾区高速公路的道路占用率(0到1之间)。
* Electricity(电力): 321个客户的 kWh 用电量记录。
* Exchange Rate(汇率):八个国家(澳大利亚、英国、加拿大、中国、日本、新西兰、新加坡、瑞士)的汇率，从1990年到2016年。

这些数据集是真实世界的数据，包含线性和非线性的相互依赖关系。此外，太阳能、交通和电力数据集显示出强烈的周期性模式，表明每天或每周的人类活动。根据LSTNet的作者，所有数据集中的每个时间序列都按时间逻辑顺序分为训练集(60%)、验证集(20%)和测试集(20%)。

相比之下，下面介绍的复调音乐数据集要复杂得多，因为它们不存在明显的线性或重复模式:
* MuseData [Nicolas Boulanger-Lewandowski and Vincent(2012)]: MIDI format中收集了许多古典音乐作曲家的作品。
* LPD-5-Cleansed [Hao-Wen Dong和Yang(2018)， ra(2016)]: 21,425 multi-track piano-rolls，包含鼓、钢琴、吉他、贝斯和弦乐。

![表1](/assets/images/深度学习/TPA/tab1.png)

表1：所有数据集的统计，其中L为时间序列的长度，D为时间序列的数量，S为采样间隔，B为数据集的字节大小。MuseData和LPD-5-Cleansed都有不同长度的时间序列，因为音乐片段的长度是不同的。

为了在这些数据集上训练模型，我们认为每个播放的音符都是1和0。设置一个节拍作为一个时间步长，如表1所示。给定由16拍组成的4小节音符，任务是预测下一小节的每个音高是否被演奏。对于训练集、验证集和测试集，我们遵循原始的MuseData分割，该分割分为524个训练集、135个验证集和124个测试集。然而，LPD-5-Cleansed 在之前的工作中并没有被分割[Hao-Wen Dong和Yang(2018)， ra(2016)];因此，我们将其随机分为训练集(80%)、验证集(10%)和测试集(10%)。LPD-5-Cleansed 数据集的大小比其他数据集大得多，因此我们决定使用更小的验证和测试集。

典型MTS数据集与复调音乐数据集的主要区别在于，典型MTS数据集中的标量是连续的，而复调音乐数据集中的标量是离散的(0或1)。

### 6.2 方法比较

在典型的MTS数据集上，我们将所提出的模型与以下方法进行了比较:
* AR:标准自回归模型。
* LRidge:带l2正则化的VAR模型:MTS预测最流行的模型。
* LSVR:具有SVR目标函数的VAR模型[V.Vapnik (1997)]。
* GP: Gaussian process model [Frigola-Alcade (2015), S.Roberts and Aigrain (2011)]。
* SETAR:自激阈值自回归模型，经典单变量非线性模型[Tong and Lim(2009)]。
* LSTNet- skip:带有递归跳转层的 LSTNet。
* LSTNet- attn:带注意力层的 LSTNet。

AR、LRidge、LSVR、GP和SETAR是传统的基线方法，LSTNet-Skip和LSTNet-Attn是基于深度神经网络的最先进的方法。

但是，由于传统的基线方法和LSTNet都是非线性的，缺乏周期性，因此不适用于多音音乐数据集，因此我们使用 LSTM 和 带有Luong attention的LSTM 作为基线模型，对所提出的模型在复调音乐数据集上进行评估:

* LSTM:第3节介绍的RNN单元。
* LSTM with Luong attention: 具有注意力机制得分函数$f(h_i,h_t)=(h_i)^\top W h_t$的LSTM,其中$W\in \mathbb{R}^{m\times m}$[Luong et al.(2015)Luong, Pham, and Manning].

### 6.3 模型设置和参数设置

在所有的实验中，我们在RNN模型中使用LSTM单元，并将CNN过滤器的数量固定在32个。另外，受LSTNet的启发，我们在对典型MTS数据集进行训练和测试时，在模型中包含了一个自回归组件。

对于典型的MTS数据集，我们对可调参数进行了网格搜索，就像使用LSTNet所做的那样。具体来说,太阳能,交通,电力,窗口大小$w$的范围是$\{24,48,96,120,144,168\}$,隐藏单元的数量$m$的范围是$\{25,45,70\}$,和指数学习率衰减（rate为0.995）的步长范围为$\{200,300,500,1000\}$。在汇率方面，这三个参数分别是$\{30,60\}$、$\{6,12\}$和$\{120,200\}$。两种类型的数据归一化也被看作是网格搜索的一部分:一种按其本身的最大值对每个时间序列进行归一化，另一种按整个数据集的最大值对每个时间序列进行归一化。最后，我们使用绝对损失函数和Adam，在太阳能、交通和电力方面的学习率为$10^{−3}$，在汇率方面的学习率为$3\cdot 10^{−3}$。对于AR、LRidge、LSVR和GP，我们遵循LSTNet论文中报道的参数设置[Lai et al.(2018)Lai、Chang、Yang和Liu]。对于SETAR，我们为太阳能、交通、电力搜索了$\{24,48,96,120,144,168\}$的嵌入维数，并将汇率的嵌入维数固定为30。我们的方法和LSTNet之间的两种不同的设置是(1)我们有两种数据规范化方法可供选择，而LSTNet只使用第一种类型的数据规范化;(2)窗口大小$w$的网格搜索是不同的。

模型用于复调音乐数据集,包括基线和下面提出模型,我们使用3层RNNs,as done in [Chuan and Herremans(2018)],和通过调整LSTM单元数量来固定$5\cdot 10^6$左右的可训练的参数以比较不同模型。此外，我们使用 Adam 优化与$10^{-5}$的学习率和交叉熵损失函数。

### 6.4 评价指标

在典型的MTS数据集上，由于我们将所提出的模型与LST-Net进行了比较，因此我们采用了相同的评价指标:RAE、RSE和CORR。第一个度量是相对绝对误差(RAE)，定义为

$$
\tag{17}
RAE = \frac{\sum_{t=t_0}^{t_1}\sum_{i=1}^{n}|(y_{t,i}-\hat{y}_{t,i})|}{\sum_{t=t_0}^{t_1}\sum_{i=1}^{n}|\hat{y}_{t,i}-\overline{\hat{y}_{t_0:t_1,1:n}}|}
$$

下一个度量是根相对平方误差(RSE):

$$
\tag{18}
RSE = \frac{\sqrt{\sum_{t=t_0}^{t_1}\sum_{i=1}^{n}(y_{t,i}-\hat{y}_{t,i})^2}}{\sqrt{\sum_{t=t_0}^{t_1}\sum_{i=1}^{n}(\hat{y}_{t,i}-\overline{\hat{y}_{t_0:t_1,1:n}})^2}}
$$

最后第三个指标是经验相关系数(empirical correlation coefficient, CORR):

$$
\tag{19}
CORR = \frac{1}{n}\sum_{i=1}^{n}\frac{\sum_{t=t_0}^{t_1} (y_{t,i}-\overline{y_{t_0:t_1,i}})(\hat{y}_{t,i}-\overline{\hat{y}_{t_0:t_1,i}})}{\sqrt{\sum_{t=t_0}^{t_1}(y_{t,i}-\overline{y_{t_0:t_1,i}})^2\sum_{t=t_0}^{t_1}(\hat{y}_{t,i}-\overline{\hat{y}_{t_0:t_1,i}})^2}}
$$

其中$y,\hat{y}$定义在4.1节,$\hat{y}_t,\forall t\in [t_0,t_1]$是测试集的 ground-truth 值，$\overline{y}$为集合$y$的均值，RAE和RSE均为不考虑数据尺度，分别是平均绝对误差(MAE)和均方根误差(RMSE)的标准化版本。对于RAE和RSE，越低越好，而对于CORR，越高越好。

为了确定哪种模型在复调音乐数据集上更好，我们使用验证集损失(负对数似数)、precision、recall和F1分数作为度量，这些度量在复调音乐生成的工作中被广泛使用[Nicolas Boulanger-Lewandowski和Vincent(2012)， Chuan和Herremans(2018)]。

### 6.5 典型MTS数据集的结果

![图5](/assets/images/深度学习/TPA/fig5.png)

图5：本文提出的模型和LSTNet-Skip在交通测试集上的预测结果。所提出的模型可以清楚地在峰后的平线上和谷内产生更好的预测。

![表2](/assets/images/深度学习/TPA/tab2.png)
表2：以RAE、RSE和CORR为度量标准对典型MTS数据集的结果。最好的表现在黑体字;第二好的表现是突出的。我们在十次运行中报告我们模型的平均值和标准偏差。除了我们模型的结果外，所有的数字都参考了LSTNet [Lai et al.(2018)Lai, Chang, Yang, and Liu]的论文。

在典型的MTS的数据集,我们选择了验证集上最好的模型，在测试集上使用RAE/RSE/CORR作为度量。数值结果列表在表2中,前两个表的度量是RAE,紧随其后的是两个表的RSE度量,最后两个表使用CORR度量。这两个表都表明，在所有数据集、层次和指标上，所提出的模型几乎优于所有其他方法。此外，我们的模型能够处理各种数据集大小，从最小的534 KB汇率数据集到最大的172 MB太阳能数据集。在这些结果中，所提出的模型一致地证明了它在MTS预测中的优越性。

与现有的 LSTNet-Skip 和 LSTNet-Attn 方法相比，该模型具有更好的性能，特别是在交通和电力方面，具有最大的时间序列。此外，在汇率方面，在不存在重复模式的情况下，所提出的模型总体上仍然是最好的;LSTNet-Skip 和 LSTNet-Attn 的性能落后于传统方法，包括AR、LRidge、LSVR、GP和SETAR。在图5中，我们还可视化并比较了所提出的模型和LSTNet-Skip的预测。
综上所述，该模型在周期性和非周期性MTS数据集上都达到了最优的性能。

### 6.6  关于复调音乐数据集的结果

![图6](/assets/images/深度学习/TPA/fig6.png)

图6：在MuseData(左)和LPD-5-clean(右)不同训练epochs下的验证损失。

![表3](/assets/images/深度学习/TPA/tab3.png)

表3：复调音乐数据集上不同模型的Precision、recall和F1评分。

在本小节中，为了进一步验证所提模型对离散数据的有效性和泛化能力，我们描述了在复调音乐数据集上进行的实验;结果如图6和表3所示。我们比较了三个RNN模型:LSTM, 带Luong注意力的LSTM，带所提出的注意力机制的LSTM。图6显示了不同训练阶段的验证损失，在表3中，我们使用验证损失最低的模型来计算测试集的精度、召回率和F1分数。

从结果中，我们首先验证了我们的观点，即典型的注意机制在类似超参数和可训练权值的情况下并不适用于这样的任务，LSTM和所提出的模型优于这种注意机制。此外，与LSTM相比，该模型在整个学习过程中学习更有效，并在精度、回忆率和F1分数方面获得更好的性能。

### 6.7 CNN filter 分析

![图7](/assets/images/深度学习/TPA/fig7.png)

图7：(1)训练了3小时 horizon 的CNN过滤器的DFT的大小比较，(2)流量数据集的每个窗口。为了使图形更直观，横轴的单位是周期。

![图8](/assets/images/深度学习/TPA/fig8.png)

图8：两个不同的CNN过滤器在3小时的时间范围内对交通进行训练，检测不同周期的时间模式。

DFT是傅里叶变换(FT)的一种变体，它及时处理等间隔的信号样本。在时间序列分析领域，利用FT或DFT来揭示时间序列重要特征的研究工作非常广泛 [N.E. Huang and Liu(1998), Bloomfield(1976)]。在我们的例子中，由于MTS数据也是等距离散的，我们可以使用DFT来分析它。然而，在MTS数据中，存在多个时间序列，因此我们自然会对每个时间序列的频率分量的大小进行平均，得到一个单一的频域表示。我们称其为平均离散傅里叶变换(avg-DFT)。单频域表示揭示了MTS数据的主要频率成分。例如，在图5中假设一个显著的24小时振荡是合理的，图7所示的交通数据集的avg-DFT验证了这一点。

因为我们希望我们的CNN filter学习临时的MTS模式，所以平均CNN过滤器中的主导频率分量应该与训练MTS数据中的相似。因此，我们还将avg-DFT应用到 $k=32$ 的 CNN filter 上，这些过滤器是针对3小时视距的交通进行训练的;在图7中，我们将结果与交通数据集的每个窗口的avg-DFT一起绘制。令人印象深刻的是，这两条曲线大部分时间在相同的时间段达到峰值，这意味着学习的CNN滤波器类似于DFT中的基。在24小时、12小时、8小时和6小时期间，不仅交通数据集的大小达到峰值，CNN过滤器的大小也达到最大值。此外，在图8中，我们展示了不同的CNN过滤器的不同行为。一些擅长捕捉长期(24小时)的时间模式，而另一些人擅长识别短期(8小时)的时间模式。总的来说，我们认为所提出的CNN滤波器在DFT中起到了base的作用。正如[Rippel et al.(2015)Rippel, Snoek, and Adams]的工作中所证明的那样，这种“频域”是CNN在训练和建模中使用的一个强大代表。因此，LSTM依赖于所提出的注意机制提取的频域信息来准确预测未来。

### 6.8 消融研究

![表4](/assets/images/深度学习/TPA/tab4.png)

表4： 消融研究。能源、交通、电力的评估，以及音乐的负对数似然。我们报告了10次运行的平均值和标准偏差。在每个语料库中，粗体文本表示最佳，下划线文本表示次之。

为了验证上述改进来自于每一个额外的组合，而不是一组特定的超参数，我们对太阳能、交通、电力和音乐数据集进行了消融研究。主要有两种设置:一种是控制如何处理RNN的隐藏状态$H$，另一种是控制如何将得分函数$f$集成到建议的模型中，甚至禁用该函数。首先，在提出的方法中，我们让模型关注每个位置（$H_i^C$）上的各种滤波器的值;我们也可以考虑关注相同的过滤器在不同位置$(H^C)_i^\top$或行向量的$H(H_i^\top)$的值。这三种不同的方法分别对应表4中的列标头:“Position”、“Filter”和“Without CNN”。第二，在典型的注意机制中，通常对得分函数$f$的输出值使用softmax来提取最相关的信息，而我们使用sigmoid作为激活函数。因此，我们比较这两个不同的函数。预测的另一种可能的结构是将所有之前的隐藏状态连接起来，让模型自动了解哪些值是重要的。考虑到这两组设置，我们用这四个数据集上所有可能的结构组合来训练模型。

MuseData结果表明，sigmoid激活并注意$H_i^C$(position)的预测模型是最佳的，说明所提出的预测模型是合理有效的。无论从模型中删除哪个组件，性能都会下降。例如，使用softmax代替sigmoid将负对数似然从0.04882提高到0.04923;如果我们不使用CNN过滤器，我们会得到一个更糟糕的模型，其负对数似然为0.4979。此外，我们注意到，在表4的前三个数据集(太阳能、交通和电力)上，所提出的模型与使用softmax的模型之间没有显著的改进。考虑到我们使用sigmoid的动机(如4.3节所述)，这并不奇怪。最初，我们期望CNN过滤器找到基本的模式，并期望sigmoid函数帮助模型将这些模式组合成一个有帮助的模式。然而，由于这三个数据集具有很强的周期性，因此使用少量的基本模式就有可能实现良好的预测。然而，总的来说，所提出的模型更 general，并在不同的数据集上产生稳定和有竞争力的结果。

## 7. 结论

在本文中，我们以MTS预测为重点，提出了一种新的时间模式注意力机制，消除了典型注意机制对这类任务的限制。为了使模型不仅在同一时间步内，而且在所有以前的时间和序列中，学习多个变量之间的相互依赖关系，我们允许注意维度是按照特征的。我们在玩具例子和真实世界数据集上的实验强烈支持这一观点，并表明所提出的模型达到了最先进的结果。此外，过滤器的可视化也以一种更容易被人类理解的方式验证了我们的动机。

## 引用

* A. Krizhevsky and Hinton(2012). A Krizhevsky IS, Hinton GE (2012) Imagenet classifica- tion with deep convolutional neural networks. Advances in Neural Information Processing Systems pp 1097–1105
* Bahdanau et al.(2015)Bahdanau, Cho, and Bengio. Bahdanau D, Cho K, Bengio Y (2015) Neural machine translation by jointly learning to align and translate. ICLR 
* Bloomfield(1976). Bloomfield P (1976) Fourier Analysis of Time Series: An Introduction.John Wiley
* Bouchachia and Bouchachia(2008). Bouchachia A, Bouchachia S (2008) Ensemble learning for time series prediction. Proceedings of the 1st International Workshop on Nonlinear Dynamics and Synchronization
* Cao and Tay(2003). Cao LJ, Tay FEH (2003) Support vector machine with adaptive parameters in financial time series forecasting. IEEE Transactions on Neural Networks pp 1506–1518
* Chen et al.(2008)Chen, Wang, and Harris. Chen S, Wang XX, Harris CJ (2008) Narxbased nonlinear system identification using orthogonal least squares basis hunting. IEEE Transactions on Control Systems pp 78–84
* Chuan and Herremans(2018). Chuan CH, Herremans D (2018) Modeling temporal tonal relations in polyphonic music through deep networks with a novel image-based representation. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16679 
* Dasgupta and Osogami(2017). Dasgupta S, Osogami T (2017) Nonlinear dynamic Boltzmann machines for time-series prediction David E. Rumelhart and Williams(1986). 
* David E Rumelhart GEH, Williams RJ (1986) Learning representations by backpropagating errors. Nature pp 533–536
* Elman(1990). Elman JL (1990) Finding structure in time. Cognitive science pp 179–211 
* Frigola and Rasmussen(2014). Frigola R, Rasmussen CE (2014) Integrated pre-processing for Bayesian nonlinear system identification with Gaussian processes. IEEE Conference on Decision and Control pp 552–560
* Frigola-Alcade(2015). Frigola-Alcade R (2015) Bayesian time series learning with Gaussian processes. PhD thesis, University of Cambridge
* G. E. Box and Ljung(2015). G E Box GCR G M Jenkins, Ljung GM (2015) Time series analysis: forecasting and control. John Wiley & Sons
* G. Zhang and Hu(1998). G Zhang BEP, Hu MY (1998) Forecasting with artificial neural networks: The state of the art. International journal of forecasting pp 35–62
* Hao-Wen Dong and Yang(2018). Hao-Wen Dong LCY Wen-Yi Hsiao, Yang YH (2018) MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment
* Hochreiter and Schmidhuber(1997). Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Computation 9(8):1735–1780, DOI 10.1162/neco.1997.9.8. 1735, URL https://doi.org/10.1162/neco.1997.9.8.1735, https://doi.org/10. 1162/neco.1997.9.8.1735
* J. Connor and Martin(1991). J Connor LEA, Martin DR (1991) Recurrent networks and NARMA modeling. Advances in Neural Information Processing Systems pp 301–308
* Jain and Kumar(2007). Jain A, Kumar AM (2007) Hybrid neural network models for hy- drologic time series forecasting. Applied Soft Computing 7(2):585–592
* J.Werbos(1990). JWerbos P (1990) Backpropagation through time: what it does and how to do it. Proceedings of the IEEE pp 1550–1560
* Kim(2003). KimKJ(2003)Financial time series forecasting using support vector machines. Neurocomputing 55(1):307–319
* Kyunghyun Cho and Bengio(2014). Kyunghyun Cho DB Bart Van Merrienboer, Bengio Y (2014) On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:14091259
* Lai et al.(2018)Lai, Chang, Yang, and Liu. Lai G, Chang WC, Yang Y, Liu H (2018) Mod- eling long- and short-term temporal patterns with deep neural networks. SIGIR pp 95–104
* LeCun and Bengio(1995). LeCun Y, Bengio Y (1995) Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks
* Luong et al.(2015)Luong, Pham, and Manning. Luong T, Pham H, Manning CD (2015) Ef- fective approaches to attention-based neural machine translation. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing pp 1412–1421
* N.E. Huang and Liu(1998). NE Huang SLMWHSQZNYCT Z Shen, Liu H (1998) The em- pirical mode decomposition and Hilbert spectrum for nonlinear and nonstationary time series analysis. Proc Roy Soc London A 454:903–995
* Nicolas Boulanger-Lewandowski and Vincent(2012). Nicolas Boulanger-Lewandowski YB, Vincent P (2012) Modeling temporal dependencies in high-dimensional sequences: Ap- plication to polyphonic music generation and transcription
* Qin et al.(2017)Qin, Song, Cheng, Cheng, Jiang, and Cottrell. Qin Y, Song D, Cheng H, Cheng W, Jiang G, Cottrell GW (2017) A dual-stage attention-based recurrent neural network for time series prediction. In: IJCAI’17, pp 2627–2633, URL http://dl.acm.org/citation.cfm?id=3172077.3172254
* Raffel(2016). Raffel C (2016) Learning-based methods for comparing sequences, with ap- plications to audio-to-MIDI alignment and matching. PhD Thesis
* Rippel et al.(2015)Rippel, Snoek, and Adams. Rippel O, Snoek J, Adams RP (2015) Spec- tral representations for convolutional neural networks. NIPS pp 2449–2457
* S. Roberts and Aigrain(2011). S Roberts MESRNG M Osborne, Aigrain S (2011) Gaussian processes for time-series modelling. Phil Trans R Soc A
* Sutskever et al.(2014)Sutskever, Vinyals, and Le. Sutskever I, Vinyals O, Le QV (2014) Se- quence to sequence learning with neural networks. Advances in Neural Information Processing Systems pp 3104–3112
* Tong and Lim(2009). Tong H, Lim KS (2009) Threshold autoregression, limit cycles and cyclical data. In: Exploration Of A Nonlinear World: An Appreciation of Howell Tong’s Contributions to Statistics, World Scientific, pp 9–56
* V. Vapnik(1997). V Vapnik ASea S E Golowich (1997) Support vector method for func- tion approximation, regression estimation, and signal processing. Advances in Neural Information Processing Systems pp 281–287
* Zhang(2003). Zhang GP (2003) Time series forecasting using a hybrid ARIMA and neural network model. Neurocomputing pp 159–175

---
**参考**：
1. 论文：Shun-Yao Shih, Fan-Keng Sun, Hung-yi Lee [Temporal Pattern Attention for Multivariate Time Series Forecasting](https://arxiv.org/abs/1809.04206v3)
