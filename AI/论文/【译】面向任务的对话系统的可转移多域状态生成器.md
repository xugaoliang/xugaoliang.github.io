# 【译】面向任务的对话系统的可转移多域状态生成器

* 作者：Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, Pascale Fung
* 论文：《Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems》
* 地址：https://arxiv.org/abs/1905.08743
* 代码：https://github.com/jasonwu0731/trade-dst

## 摘要

过度依赖领域本体和缺乏跨领域的知识共享是对话状态跟踪的两个实际问题，但研究较少。现有的方法通常在推理过程中无法跟踪未知的槽值，而且在适应新领域方面也存在困难。在本文中，我们提出了一种可转移的对话状态生成器(**TRA**nsferable **D**ialogue stat**E**,TRADE)，它使用复制机制从话语中生成对话状态，从而在预测训练时从未遇到的(domain、slot、value)三元组时促进知识转移。我们的模型由一个话语编码器、一个槽门和一个状态生成器组成，这些都是跨域共享的。实证结果表明，MultiWOZ（一个人与人之间的对话数据集）的5个领域的联合目标准确率达到了48.62%。此外，通过对不可见域的 zero-shot 和 few-shot 对话状态跟踪仿真，证明了该算法的传输能力。TRADE在其中一个zero-shot域内实现了60.58%的联合目标精度，并且能够在不忘记已经训练好的域的情况下适应few-shot 情况。

## 1 介绍

![图1](/assets/images/TRADE/fig1.png)

图1：交谈中多域对话状态跟踪的一个示例。左边的实箭头是单轮映射，右边的点箭头是多轮映射。状态跟踪器需要跟踪用户为所有域中的所有槽所提到的槽值。


对话状态跟踪(Dialogue state tracking,DST)是面向任务的对话系统(如餐厅预订或机票预订)的核心组件。DST的目标是提取在对话中表达的用户目标/意图，并将它们编码为一组紧凑的对话状态，即，一组槽及其对应的值。例如，如图1所示，从对话中提取(slot, value)对，如(price, cheap)和(area, centre)。准确的DST性能对于适当的对话管理是至关重要的，在对话管理中，用户意图决定下一个系统操作 和/或 要从数据库查询的内容。

传统上，状态跟踪方法基于预先定义本体的假设，其中所有槽及其值都是已知的。预定义本体可以将DST简化为分类问题，提高性能(Henderson 等， 2014b;Mrksic 等, 2017;Zhong 等，2018)。但是，这种方法有两个主要缺点:1)很难提前获得完整的本体(Xu and Hu, 2018)。在业界，数据库通常只通过外部API公开，而外部API由其他人拥有和维护。获取访问权限以枚举每个槽的所有可能值是不可行的。2)即使存在完整的本体，可能的槽值数量也可能很大，并且是可变的。例如，餐馆名称或火车发车时间可以包含大量可能的值。因此，以往许多基于神经分类模型的工作可能无法应用于实际场景。

Budzianowski等人(2018)最近推出了一个多域对话数据集(MultiWOZ)，由于它的多域对话，这给DST带来了新的挑战。如图1所示，用户可以通过预约餐厅开始对话，然后询问附近景点的信息，最后要求预定出租车。在这种情况下，DST模型必须在每轮对话中确定对应的域、槽和值，其中包含本体中大量的组合，即， 30对(域、槽)和4500多个可能的槽值。多域设置的另一个挑战是需要执行多轮映射。单轮映射是指可以从一个轮中推断出(域、槽、值)三元组的场景，而在多轮映射中，应该从发生在不同域中的多个轮中推断出(domain、slot、value)三元组。例如，图1中 Attraction 域的(area, centre)对可以通过前面提到的 restaurant 域中的 area 信息来预测。

为了解决这些挑战，我们强调DST模型应该跨领域共享跟踪知识。不同域之间有许多槽，它们共享所有或部分值。例如，area 槽可以存在于许多领域，例如餐厅、景点和出租车。此外，餐厅域中的名称槽可以与出租车域中的出发槽共享相同的值。此外，为了使DST模型能够跟踪不可见域中的槽，跨多个域传输知识是必要的。我们期望DST模型可以通过在其他域中学习跟踪相同的槽来学习在 zero-shot 域中跟踪某些槽。

本文提出了一种可转移的对话状态生成器(TRADE)，用于多领域的面向任务的对话状态跟踪。我们的方法简单，性能提高，这是 TRADE 的主要优势。对这项工作的贡献总结如下(代码发布在：https://github.com/jasonwu0731/trade-dst):

* 为了克服多轮映射问题，TRADE 利用上下文增强的槽门和复制机制来正确跟踪对话历史中任何位置提到的槽值。
* 通过跨域共享其参数，并且不需要预定义的本体，TRADE可以在域之间共享知识来跟踪不可见的槽值，从而在多域DST上实现最先进的性能。
* 通过利用在训练期间已经看到的域，TRADE实现了 zero-shot DST。如果有一些不可见域的训练样本可用，TRADE可以适应新的 few-shot 域而不忘记以前的域。

## 2 TRADE 模型


![图2](/assets/images/TRADE/fig2.png)

图2：:所提出的 TRADE 模型的架构，包括(a)一个话语编码器、(b)一个状态生成器和(c)一个槽门，所有这些都在域之间共享。状态生成器将对所有可能的$(domain、slot)$对独立解码$J$次。在第一个解码步骤中，状态生成器将以第$j$个$(domain,slot)$嵌入作为输入，生成对应的槽值和槽门。槽门预测对话是否触发第$j$对$(domain,slot)$。


图2中提出的模型由三部分组成:话语编码器、槽门和状态生成器。我们的模型不是预测每个预定义本体项的概率，而是直接生成槽值。与Johnson等人(2017)对多语言神经机器翻译的研究类似，我们共享所有的模型参数，状态生成器为了每个(domain、slot)对，用不同的句子开始标记开始。

该话语编码器将对话话语编码成一个固定长度的向量序列。为了确定是否提到了任何(domain、slot)对，状态生成器使用上下文增强的槽门。状态生成器对所有(domain、slot)对的多个输出令牌进行独立解码，以预测它们对应的值。上下文增强的槽门可以预测是否每对都是由对话通过三路分类器触发的。

让我们定义 $X=\lbrace (U_1,R_1),\cdots,(U_T,R_T) \rbrace$ 为 $T$ 轮对话中用户话语与系统响应对的集合，$B=\lbrace B_1,\cdots,B_T \rbrace$为每轮的对话状态，每个 $B_t$ 是一个元组 （domain：$D_n$，slot：$S_m$，value：$Y_j^{value}$）,其中 $D=\lbrace D_1,\cdots,D_N \rbrace$ 为 $N$ 个不同的域，$S=\lbrace S_1,\cdots,S_M \rbrace$为 $M$ 个不同的槽，假设有 $J$ 对可能的 $(domain,slot)$，$Y_j^{value}$ 是第$j$对 $(domain,slot)$  的真实词序列。

### 2.1 话语编码器

注意，话语编码器可以是任何现有的编码模型。我们使用双向门控循环单元(GRU) (Chung 等， 2014)对对话历史进行编码。语音编码器的输入记为$history$ $X_t=[U_{t-l},R_{t-l},\cdots,U_t,R_t] \in \mathbb{R}^{|X_t|\times d_{emb}}$，它是对话历史中所有单词的串联。$l$为所选对话轮数，$d_{emb}$为嵌入大小。编码的对话历史记录表示为$H_t=[h_1^{enc},\cdots,h_{|X_t|}^{enc}] \in \mathbb{R}^{|X_t|\times d_{hdd}}$，其中 $d_{hdd}$ 是隐状态的大小。正如在第1节中提到的，由于多轮映射问题，模型应该在跨越多个轮次的序列中推断状态。因此，我们使用长度为$l$的最近对话历史作为话语编码输入，而不是仅使用当前的话语。

### 2.2 状态生成器

要使用输入源中的文本生成槽值，需要一种复制机制。执行复制有三种常见的方法，即，index-based 复制(Vinyals 等， 2015)，hard-gated 复制(Gulcehre 等，2016;Madotto 等，2018;Wu 等，2019)和 soft-gated 复制(See 等，2017; McCann 等，2018)。index-based 机制不适用于DST任务，因为在话语中并不总是能找到真正槽值的确切单词。hard-gate 复制机制通常需要对门控功能进行额外的监视。因此，我们使用 soft-gated 指针生成器复制，将词汇表上的分布和对话历史上的分布合并为一个输出分布。

我们使用GRU作为状态生成器的解码器来预测每个$(domain,slot)$对的值，如图2所示。状态生成器独立地解码$J$个对。我们只提供域和槽的总和嵌入作为解码器的第一个输入。在解码第$j$个$(domain,slot)$对的步骤$k$时，生成器GRU以一个词嵌入$w_{jk}$作为输入，返回一个隐状态$h_{jk}^{\mathrm{dec}}$。状态生成器首先使用可训练嵌入$E\in \mathbb{R}^{|V|\times d_{hdd}}$将隐状态$h_{jk}^{\mathrm{dec}}$映射到词汇表空间$P_{jk}^{\mathrm{vocab}}$，其中$|V|$为词汇表大小。同时，通过编码的对话历史$H_t$,用$h_{jk}^{\mathrm{dec}}$计算历史的注意力$P_{jk}^{\mathrm{history}}$ :

$$
\tag{1}
\begin{aligned}
P_{jk}^{\mathrm{vocab}} &= Softmax(E \cdot (h_{jk}^{\mathrm{dec}})^\top) \in \mathbb{R}^{|V|} \\\\
P_{jk}^{\mathrm{history}} &= Softmax(H_t \cdot (h_{jk}^{\mathrm{dec}})^\top) \in \mathbb{R}^{|X_t|} 

\end{aligned}
$$

最后的输出分布 $P_{jk}^{\mathrm{final}}$ 是两个分布的加权和,

$$
\tag{2}
P_{jk}^{\mathrm{final}}=p_{jk}^{\mathrm{gen}} \times P_{jk}^{\mathrm{vocab}} + (1-p_{jk}^{\mathrm{gen}}) \times P_{jk}^{\mathrm{history}} \in \mathbb{R}^{|V|}
$$

