---
typora-root-url: ../../../
---

# 【译】从视觉观察中学习动作表征

**摘要**:在这项工作中，我们探索了一种新的方法，让机器人通过观察世界来自学。特别地，我们研究了学习任务不可知表象对连续控制任务的有效性。我们扩展了时间对比网络(Time-Contrastive Networks，TCN)，它通过在嵌入空间中联合嵌入多帧而不是单一帧来学习视觉观察。我们表明，通过这样做，我们现在能够更准确地编码位置和速度属性。我们测试了这种自我监督方法在强化学习环境中的有效性。我们证明，通过观察自身采取随机操作或其他代理成功执行任务而获得的表示，可以使用仅使用所学习的嵌入作为输入的近似策略优化(Proximal Policy Optimization, PPO)等算法来实现对连续控制策略的学习。与单帧基线相比，我们还在真实的 Pouring 数据集上进行了显著的改进，运动属性的相对误差降低了39.4%，静态属性的相对误差降低了11.1%。在 https://sites.google.com/view/actionablerepresentations 上的视频结果是有用的

## I. 简介
对许多视觉任务来说，监督学习是一个强大的方法，用来学习有用的视觉表征，如图像分类，目标检测，和语义分割。因此，许多最先进的计算机视觉深度学习方法都包含了有监督的替代损失预训练，但这需要有充足的训练数据可以读取;最常见的是大规模的ImageNet 或 COCO 分类数据集。

如果能够利用有监督预训练来实现基于图像观测的机器人控制，这将是非常有用的。然而，有三个问题使预训练在这一领域的应用复杂化。首先，<u>目前还不完全清楚哪些语义标签或代理损失适用于当前的机器人控制任务。特别是，场景的语义标记通常与机器人控制无关。强分类性能所需的不变性集合(如对象姿态的不变性、类内变化等)可能不适用于机器人策略。这些潜在因素对解决这一问题具有内在的必要性。</u>其次，为每一个新的机器人任务建立一个数据收集管道和收集标签是非常昂贵的。最后，大多数机器人策略在与大型数据集(如ImageNet或COCO)几乎没有重叠的域上执行;例如，他们将很少或没有覆盖机器人实验室环境。此外，<u>当一个通用型机器人发现一个新环境中有以前没有发现过的对象时，它需要适应并学习它。这种图像域的转移常常需要额外的数据收集。</u>与通过状态或任务奖励来学习不同，<u>纯粹通过观察来学习的另一个动机是利用新的学习信号。</u>例如，一个人可以学会预测世界上的事物是如何运动的，即使是在它失去控制的情况下。这并不局限于从演示中学习，也包括从世界上的非演示事件或物体中学习。

作为监督学习的另一种选择，<u>最近的自监督方法，如时间对比网络(TCN)或位置速度编码器(PVE)</u>，在构建适合于增强学习(RL)机器人应用的鲁棒视觉表征方面已经<u>显示出令人鼓舞的结果，而不需要昂贵的监督标签。这些方法从现成的图像中直接利用感兴趣的域。他们通过构建代理度量学习损失来实现这一点，这些度量学习损失包含了大量的结构先验(如时序一致性、视图不变性等)，这些先验已被证明能够有效且简洁地编码策略学习所需的潜在状态因子。</u>在模拟环境中，PVE使学习连续控制策略成为可能，而TCN被证明有助于模仿学习，即使用机器人手臂将液体倒入容器中。然而，在TCN的情况下，多帧状态不进行编码，因为嵌入是基于单个帧的。同样，PVE提出了一组必须针对每个环境进行调优的先验。此外，自监督技术还没有达到直接根据真实状态(而不是图像观察)训练的策略的性能。

然而，尽管最近的自监督技术有局限性，这些方法学习的表征具有许多我们希望在这项工作中加以利用的理想属性:<u>1。嵌入可以用来描述在观察过程中访问的不同状态(包括运动属性)，而不需要显式的状态标签。2. 嵌入应该对视点的变化具有鲁棒性，这样就可以在大量现成的训练数据(如YouTube视频)存在的情况下进行第三人称演示学习。3.嵌入应该能够适应新环境中的在线适应，而不需要额外的标签。</u>

在这项工作中，我们提出了一个变种的TCN，我们称之为多帧时间对比网络(multi-frame Time-Contrastive Network，mfTCN)，<u>它编码的短时的潜在状态(如速度，角速度，加速度等)。</u>我们通过实验证明，我们的嵌入可以有效地从真实世界和模拟数据中编码多帧潜在状态，<u>简单的线性回归和分类器可以从 mfTCN 嵌入中获得精确的状态估计。</u>最后，我们证明了该演示可用作DeepMind控制套件中模拟机器人控制任务的PPO训练策略的基础，并且基于 mfTCN 的策略奖励性能与从精确状态表征训练得到的性能相匹配。


本文的贡献有:
* 引入了 TCN 的多帧变体，更适合静态和运动属性分类。
* 显示RL策略可以使用 mfTCN 从像素学习，同时性能优于基于像素的从头开始学习或使用 PVE 学习
* 我们还表明，与使用基于真(本体感受)状态的策略相比，我们所学习的策略是有竞争力的。因此，我们认为我们的表征是可操作的，因为它们不仅可以编码本体感受状态下的静态和运动信息，还可以直接用于连续的控制任务。


![图1](/assets/images/计算机视觉/从视觉观察中学习动作表征/fig1.png)

图1：给定同一事件的两个视点，我们从视频中采样剪辑，并通过多帧时间对比网络(mfTCN)获得每个视图的嵌入。为了训练深度网络，我们考虑同一时间不同视点的剪辑是相似的，不同时间的剪辑是不同的

## II. 相关工作

**连续控制环境** 近年来，强化学习(RL)算法的性能有了显著的提高，端到端的来自像素的策略在包括Atari游戏和连续控制在内的一系列测试中取得了成功。Tassa等人的引入了一组引人注目的基准测试环境，称为控制套件，我们在这项工作中使用了它。它们基于Todorov等人最初在Mujoco环境中引入的任务。类似任务的另一个流行基准是OpenAI的健身房。

**利用先验学习状态表征** 无论是在无监督还是半监督的情况下，学习有用的状态表征都是机器人控制中一个长期研究的领域。Scholz等人表明，在输入状态中加入基于物理的先验可以提高基于模型的RL的性能。Jonschkowski等人使用类似的基于物理的机器人先验(robotic-priors)来学习与物理任务的动态一致的状态表征。这个框架在[4]中进行了扩展，以包含额外的先验术语来捕获多帧动态，其中状态表征仅从视觉观察中学习。与我们的工作类似，状态表征是从模拟环境中由随机代理操作产生的观察中学习来的。Lesort等人在他们的工作中除了[4]中引入的先验外，还引入了参考点先验。与此同时，[14]还演示了在连续控制任务中表示运动的重要性。它们通过使用网络预测的光流作为策略的额外输入来实现。

**使用图像的自监督学习** 近年来，在使用无监督和半监督的预训练方法来学习对机器人任务有用的视觉表征和强化学习方面取得了多项成功。Watter等人研究了一个局部线性潜空间，该潜空间允许他们使用最优控制算法来跟踪嵌入空间中的轨迹。van Hoof等人使用变分自编码器来稳定基于视觉和触觉数据流的强化学习系统。Finn等人成功地学习了在低维空间中对输入图像进行编码的模型。对模型进行训练，重建输入图像。所学习的嵌入与机器人的真实状态一起作为输入提供。这种联合表征使机器人能够预先完成一些复杂的任务，如如舀米和挂钩。如果没有视觉输入，这些任务是不可能完成的。Munk等人提出了一种方法，在使用强化学习训练参与者-批评者模型之前，使用“预测先验”将他们的输入映射到一个有用的隐藏状态。Agrawal等人介绍了一个有趣的框架，在这个框架中，一个代理首先通过观察它所采取的随机动作的效果来学习一个世界模型。这种学习表征可以用于执行需要多步骤决策的任务。Pinto等人表明，通过在现实世界中对机器人执行预先定义的动作，可以学习良好的视觉表征。随着学习表征的改进，这也将使机器人能够执行更复杂的任务。最近关于模仿学习的研究，Yu等人甚至主张通过元学习来适应不同的演示，直接从像素中学习策略。

**使用视频的自监督学习** 研究人员也成功地通过使用视频作为输入来学习有用的表征。Sermanet 等人利用时间作为监督信号来学习视频中出现的结构，从而学习一个健壮的任务无关的视觉表征。Pathak等人还训练了一个可视化分类器，帮助代理识别完成任务需要访问的中间状态。该分类器采用基于时间一致性的自监督训练目标进行训练。Finn等人介绍了一个现实世界中机器人与物体交互的数据集。他们通过预测视频中未来的帧来学习简洁的视觉表达。Babaeizadeh等人通过以随机方式预测未来帧扩展了上述工作。Dosovitskiy等人表明，他们可以通过预测未来的状态变化来学习在环境中采取行动。

**用辅助损失来改进表征** 虽然上述方法突出了在强化学习之前使用学习的健壮表征的优势，但最近有一些工作是在RL目标中添加辅助任务，以学习执行更好的模型。多任务学习也被证明有助于学习更健壮的内部表征，从而最终提高性能。虽然在这项工作中，我们只考虑我们在学习控制策略之前学习任务无关表征的情况，我们可以调整我们的方法到一个多任务设置中，在那里策略和表征学习共同改进。

**时间对比网络（Time-Contrastive Networks,TCN）** 在这项工作中，我们扩展了由Sermanet等人引入的TCNs，使用时间作为监督信号。使用TCNs可以区分代理在倾向于执行任务时可能遇到的各种状态。这些表征已被证明对模仿学习任务是有用的。在传统的控制系统中，输入是各个对象或agent部件的位置和速度。因为，我们将使用学到的表征来训练代理学习控制策略，嵌入的一个理想特性是同时对位置和速度进行编码。在原始的TCN中，该模型能够编码世界的状态和各种对象/代理部件的位置。然而，TCN很难对物体/代理部件的运动线索或速度进行编码，因为它只依赖于单个帧。在这项工作中，我们将TCNs扩展到多帧设置，在每个时间步长中嵌入多帧，我们期望学习一个同时编码静态和运动属性的表征。

## III. 方法

视频有可能成为一个丰富的数据来源，为机器人增加许多倍它的环境知识。然而，<u>仅根据代理收到的奖励直接从像素学习控制策略是很困难的。这就是健壮的视觉表征可以派上用场的地方。</u>从视觉观察中学习似乎是人类获得运动技能的关键因素。<u>通常情况下，人们通过执行任务的演示来相互传递技能。我们也看到医生使用视觉**反馈**远程操作手术器械的例子，而不是直接获得器械的位置和速度。我们展望未来，机器人将能够利用视觉反馈和其他传感器的输入。</u>良好的视觉表征也可能是通过虚拟现实演示将技能从机器人转移到人类的关键。从持续学习的角度来看，我们希望学习即使在没有提供演示的情况下也能得到更新的可视化表示。agent通过观察环境的变化来学习。为此，<u>我们提出了一种与任务无关的方法来从观察中学习这些表征，从而允许我们从像素中学习控制策略。本质上，我们将表征学习阶段与特定于任务的控制策略学习阶段分开。</u>我们在下面给出这两个部分的详细信息。

**A. 从观察中学习表征**

![图2](/assets/images/计算机视觉/从视觉观察中学习动作表征/fig2.png)
图2： 在上面的图中，我们展示了如何对一批样本进行训练。如顶部一行所示，我们同时从两个视图中采样剪辑，其中每个剪辑由3个帧组成，每个帧之间的步长为3。我们确保剪辑不重叠，
至少$\alpha$时间步骤之间的差距。只有两个样本,我们可以看到自监督信号的强度力量模型在相似帧寻找差异,同时在不同的视点寻找相似之处。

在此阶段，代理首先以任务无关的方式学习表征。agent可以从各种各样的观察中学习:从对其环境的被动观察中，从其他agent的演示中，以及从观察自身在世界中的行为中。这种任务无关的学习是通用的，因为它允许模型从世界和其他代理(即，即使它没有访问真正的状态或甚至奖励)学习，也允许模型从自身学习。<u>在这项工作中，我们将自己限制在多视图设置中，对于每个观察，我们有两个同步的视图(模型也可以用更多的视图进行训练)。</u>

在形式上，我们提供了多个视频作为输入，agent可以从中观察和学习世界的良好表征。每个视频$v$收集自多个同步视点$v_1,v_2,\cdots,v_k$.在[3]（TCN）中，我们使用时间作为监督信号。在图2中，我们展示了如何根据给定的视频创建训练批次。我们认为所有给定视点在$t$时刻的剪辑是相似的(它们在嵌入空间中相互吸引)。此外,这些片段也认为在任何视点下，超出$\alpha$步时间的任何剪辑不相似(不相似的剪辑在嵌入空间互相对抗)。为了鼓励网络学习编码上述直觉的表示法，我们学习一个由基本网络产生的嵌入得到的度量空间。与[3]（TCN）不同的是，我们不是在每一步嵌入一个帧，而是在每一步嵌入多个帧。为此，我们引入了两个超参数:嵌入在一起的帧数(用$n$表示)和它们之间的步长（用$s$表示）.在每个时间步$t$,我们嵌入当前帧和前面$n−1$帧（用步长$s$选择）。这意味着在每一步网络都有一个固定的回望窗口$(n-1)\times s+1$帧。

<u>嵌入多帧的动机是为了让网络不仅能推断出物体的状态，还能利用场景中的动作线索。</u>正如第4节和可预见的，TCN难以编码运动，因为它一次嵌入一个帧。在多帧版本中，我们通过联合嵌入多帧来缓解这个问题，使得对运动线索和物体速度的编码更加容易。

![图3](/assets/images/计算机视觉/从视觉观察中学习动作表征/fig3.png)

图3： 用于从给定的帧序列中提取mfTCN嵌入的体系结构。将三维卷积层加入用来聚合时间中
跨帧信息和编码mfTCN嵌入的运动线索。

我们在图3中展示了 mfTCN 中使用的深层网络的架构。该网络以帧序列的剪辑作为输入，并将其共同嵌入到低维空间中。我们描述了网络的不同部分如下:

* 1) 基网络:我们使用卷积神经网络(CNN)作为基网络，对原始像素提取低维表征。我们将在这些表征的基础上学习 mfTCN 嵌入。基网络将$t$时刻的输入帧$I_t$编码为隐特征$h_t$:

$$
h_t = CNN(I_t)
$$

* 2) 时间聚合:从帧中提取不同时间步长的特征来聚合时间信息的方法有多种。我们可以进行时间平均，使用时间卷积或使用递归神经网络。在我们的模型中，我们选择使用3D时间卷积来执行时间聚合，但是也可以使用其他的架构。

$$
\phi_t = Conv3D(H_t,h_{t-s},\cdots,h_{t-(n-1)\times s})
$$

* 3) 降维:经过时间加权步骤，最终得到一个具有4维(时间、高度、宽度、通道)的卷积特征图。为了学习整个剪辑的紧凑表示，我们通过先执行空间平均，然后使用全连接层来降低维数。

$$
mfTCN_t = FC(\phi_t)
$$

* 4) 时间对比学习:我们使用[35]中引入的$n$对损失来训练我们的网络。出于解释的目的，我们借用了来自于三元损失[36]的术语“anchor，positive 和 negative”，因为类似的概念用于$n$对损失。一般来说，一个 anchor-positive 是一对样本，我们希望在嵌入空间中比 anchor-negative 彼此更接近。损失的目的是学习一种度量嵌入，这种嵌入将同一类别的数据样本聚在一起，使它们在学习嵌入空间中离不同类别的样本更远。为了能够在时间对比设置中使用$n$对损失，我们从多个视图中同时采样非重叠剪辑。我们在每个时间步获得k个实例(等于视点的数量)。这些实例被认为是积极的。如果我们已经采样了$n$步,那么我们得到$k\times (n−1)$个负例和每个时间步里$k$个正例(参见图2)。换句话说,剪辑采样来自不同视点相同时间的是正例，剪辑采样自不同时间的是负例，不管视点如何。该启发式算法为训练模型提供了必要的监控信号。在图2中，我们可以看到，仅使用来自两个视点的两个样本，我们就能够生成6个监督标签:2对相互靠近的嵌入，4对相互远离的嵌入。这个数字与我们在给定批次中采样的时间步数相结合而增长的。这种丰富的监控信号与通过嵌入多个帧共同提供的上下文相结合，允许我们从视觉观察中学习良好的表征。

* 5) TCN和mfTCN的架构差异: 在最初的TCN[3]和mfTCN之间有一些架构上的差异。首先，在mfTCN中有一个3D卷积层，它将不同时间步长的卷积特征图作为输入。其次，我们在mfTCN中没有原来的TCN用来降低维度的空间softmax[37]层。


**B. 学习控制策略**
为了测试我们的嵌入的实用性和鲁棒性，我们决定在这些学习表征的基础上执行连续的控制任务。连续控制算法通常采用真实状态(关节角度、位置等)作为输入。我们希望我们的学习表征可以完全替代特定任务所需的真实状态。为此，我们选择PPO[5]作为on-policy优化算法，该算法允许我们学习连续的控制策略。

## IV. 实验

* A.对侧手翻速度和位置的回归
我们的目标是证明mfTCN嵌入能够编码物体和代理部分的速度和位置。度量我们的模型能够在多大程度上捕获这些信息是很重要的，因为如果我们希望能够使用这些嵌入来执行连续的控制任务，那么它们需要对这些实体的精确信息进行编码。为了定量测量，我们从Deepmind控制套件[6]中收集了关于Cartpole环境的多视点视频。我们在他们的环境中使用默认的多摄像头设置。我们从收集的视频学习了一个多帧TCN嵌入，其中的代理是采取随机行动。随机操作遵循与[4]相同的分布。我们评估这些嵌入是否能够编码场景中不同物体的位置和速度。为了做到这一点，我们遵循[4]的实验设置，其中他们使用模拟器提供的真实状态，在嵌入的基础上训练回归器。与位置速度编码器一样，我们还将$t$和$t-1$时刻的嵌入的差异拼接起来。我们训练3个完全连接的256维的层，顶部用Adam优化器，学习率0.001，学习嵌入。然后从验证集中显示经过训练的回归量，并预测代理的真实状态。我们使用160000和20000帧的训练/测试分割来完成这项任务。实验结果如表1所示。我们观察到多帧嵌入能够更好地对真实位置和速度进行编码。此外，当我们增加嵌入学习的大小时，均方误差(MSE)减小。有趣的是,我们没有明确训练的嵌入编码位置或速度但显而易见的模型需要编码他们及其他属性来区分不同时间步上片段的不同,同时仍然知道多个视点同时应该编码相同的状态。

* B.从自我观察中学习TCN嵌入的策略
在这个实验中，我们考虑这样一个场景:一个代理能够在它的环境中观察自己。我们考虑了Deepmind的控制套件的Cartpole域中的切换任务。该任务是摆动杆向上施加的力量，在推车的基础上，然后平衡它没有偏离太多的基地中心。物理模型与[38]中给出的模型相似。为了收集数据来训练mfTCN，代理使用随机初始状态执行随机操作。我们使用默认的相机设置。注意，在这个设置中，第二个摄像机与代理一起移动。
我们使用最近策略优化(PPO)[5]算法来学习一个基于mfTCN嵌入结果的策略。通常情况下，真实的状态(如对象的位置、关节的位置等)是作为输入来学习策略的。我们用习得的低维表征代替真实状态作为PPO的输入。我们报告了在测试时 100 episodes/rollouts 的奖励的平均值和标准偏差。在[6]之后，我们每1000步为每个episode  rollout。

---
**参考**：
1. 论文：Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet [Learning Actionable Representations from Visual Observations](https://arxiv.org/abs/1808.00928)

[3]: P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine, “Time-contrastive networks: Self-supervised learning from video,” Proceedings of International Conference in Robotics and Automation (ICRA 2018).

[4]: R. Jonschkowski, R. Hafner, J. Scholz, and M. Riedmiller, “Pves: Position-velocity encoders for unsupervised learning of structured state representations,” arXiv preprint arXiv:1705.09805, 2017.

[14]: A. Amiranashvili, A. Dosovitskiy, V. Koltun, and T. Brox, “Motion perception in reinforcement learning with dynamic objects,” in Proceedings of The 2nd Conference on Robot Learning, ser. Proceedings of Machine Learning Research, A. Billard, A. Dragan, J. Peters, and J. Morimoto, Eds., vol. 87. PMLR, 29–31 Oct 2018, pp. 156–168. [Online]. Available: http://proceedings.mlr.press/v87/amiranashvili18a.html

[35] K. Sohn, “Improved deep metric learning with multi-class n-pair loss
objective,” in Advances in Neural Information Processing Systems,
2016, pp. 1857–1865.