Tom Michael Mitchell教授，机器学习定义：如果一个程序可以在任务T上，随着经验E的增加，效果P也可以随之增加，则称这个程序可以从经验中学习



如何从实体中提取特征，对于很多传统机器学习算法的性能有巨大影响。（例子：查询书籍，书名在excel中，和非结构化的图片中）

同样的数据使用不同的表达方式会极大地影响解决问题的难度。（例子：笛卡尔坐标系不可线性分，极坐标系可线性分）



人工的方式无法很好地抽取实体中的特征，那么是否有自动的方式呢？答案是肯定的。深度学习解决的核心问题之一就是自动地将简单的特征组合成更加复杂的特征，并使用这些组合特征解决问题。深度学习是机器学习的一个分支，它除了可以学习特征和任务之间的关联之外，还能自动从简单特征中提取更加复杂的特征。

深度学习算法可以从数据中学习更加复杂的特征表达，使得最后一步权重学习变得更加简单且有效。



小白鼠视觉神经连接到听觉中枢，一段时间之后小鼠可以习得使用听觉中枢看世界。这说明虽然哺乳动物大脑分为了很多区域，但这些区域的学习机制却是相似的。

因为深度学习的通用性，深度学习的研究者往往可以跨越多个研究方向，甚至同时活跃于所有的研究方向。

现代深度学习的发展并不拘泥于模拟人脑神经元和人脑的工作机理。模拟人类大脑也不再是深度学习研究的主导方向。我们不应该认为深度学习是在试图模拟人类大脑。



历史：

最早的神经网络：

1943年，Warren McCulloch 教授和Walter Pitts教授，论文 《a logical calculus of the ideas immanent in nervous activity》中提出MCCulloch-Pitts Neuron的计算结构（通过n个权重w1,w2,…wn来计算n个输入的加权和，然后用这个加权和经过一个阈值函数得到一个0或1的输出）

1958年，Frank Rosenblatt教授提出感知机模型（perceptron),根据样例数据来学习特征权重的模型。

1969年，Marvin Minsky教授和Seymour Papert教授，《Perceptrons:An Introduction to Computational Geometry》中证明感知机只能解决线性可分问题，明确指出感知机无法解决异或问题，并指出在当时计算能力下，实现多层神经网络是不可能的事情。导致神经网络第一次重大低潮期

20世纪80年代末，第二波神经网络研究因分布式知识表达和神经网络反向传播算法的提出而兴起。分布式的知识表达的核心思想是现实世界中的知识和概念应该通过多个神经元（neuron)来表达,而模型中的每一个神经元也应该参与表达多个概念。n种颜色m种型号汽车，一种方法n*m个神经元表达，另一种n+m个神经元表达。神经网络从宽度方向走向了深度的方向。为之后的深度学习奠定了基础。可以很好解决类似异或等线性不可分问题。

除了解决了线性不可分问题，在20世纪80年代末，研究人员在降低训练神经网络的计算复杂度上也取得了突破性成就。David Everett Rumelhart 教授、Geoffrey Everest Hinton教授和Ronald J. Williams教授于1986年在自然杂志上发表的《Learning Representations by Back-propagating errors》文章中首先提出了反向传播算法（back propagation)，此算法大幅降低了训练神经网络所需要的时间。80年代末计算机能力比70年代突飞猛进增长，卷积神经网络，循环神经网络也得到发展，Sepp Hochreiter教授和Juergen Schmidhuber教授于1991年提出的LSTM模型可以有效地对较长的序列进行建模。

神经网络发展的同事，传统机器学习算法也有了突破性发展，并在90年代末逐步超越了神经网络，成为当时机器学习领域最常用的方法。SVM手写体识别错误率讲到0.8%.神经网络达不到。两个原因：首先在当时计算资源下，要训练深层的神经网络仍然是非常困难的，其次，当时数据量比较小，无法满足训练深层神经网络的需求。

随着计算机性能的进一步提高，以及云计算、GPU的出现，到2010年左右，计算量已经不再是阻碍神经网络发展的问题。与此同时，随着互联网+的发展，获取海量数据也不再困难。这让神经网络所面临的几个最大问题得到了解决。于是神经网络的发展也迎来了新的高潮。2012年ImageNet举办的图像分类竞赛（ImageNet Large Scale Visual Recognition Challenge,ILSVRC）中，由Alex Krizhevsky教授实现的深度学习系统AlexNet赢得了冠军。自此之后，deep learning作为深层神经网络的代名词被大家所熟知。深度学习的发展也开启了一个AI的新时代。

如今，深度学习已经从最初的图像识别领域扩展到了机器学习的各个领域。



TensorFlow两个最主要的工具包——Protocol Buffer 和 Bazel















ILSVRC官网：http://image-net.org

人脸识别数据集官网：http://vis-www.cs.umass.edu/lfw

cern研讨会：https://indico.cern.ch

SVHN数据集-斯坦福大学开源数据集：http://ufldl.stanford.edu/housenumbers

TIMIT数据集官网：https://catalog.ldc.upenn.edu/ldc93s1



语料库：

WordNet:https://wordnet.princeton.edu

ConceptNet:http://conceptnet5.media.mit.edu

FrameNet:https://framenet.icsi.berkeley.edu/fndrupal

GloVe:http://nlp.stanford.edu/projects/glove

谷歌TensorFlow论文：《Large-Scale Machine Learning on Heterogeneous Distributed Systems》

Inception论文：Rethinking the InceptionArchitectureforComputerVision。 

Inception迁移学习：A Deep Convolutional Activation Feature for Generic Visual Recognition 





## 第3章：TensorFlow入门

一个计算图中，可以通过集合（collection）来管理不同类别的资源。比如通过tf.add_to_collection函数可以将资源加入一个或多个集合中，然后通过tf.get_collection获取一个集合里面的所有资源。这里的资源可以是张量、变量或者运行TensorFlow程序所需要的队列资源，等等。为了方便使用，TensorFlow也自动管理了一些最常用的集合，表3-1总结了最常用的几个自动维护的集合



 <center> **表3-1 TensorFlow中维护的集合列表** </center>

|               集合名称                |                集合内容                |           使用场景           |
| :-----------------------------------: | :------------------------------------: | :--------------------------: |
|        tf.GraphKeys.VARIABLES         |                所有变量                |     持久化TensorFlow模型     |
|   tf.GraphKeys.TRAINABLE_VARIABLES    | 可学习的变量（一般指神经网络中的参数） | 模型训练、生成模型可视化内容 |
|        tf.GraphKeys.SUMMARIES         |           日志生成相关的张量           |     TensorFlow计算可视化     |
|      tf.GraphKeys.QUEUE_RUNNERS       |         处理输入的QueueRunner          |           输入处理           |
| tf.GraphKeys.MOVING_AVERAGE_VARIABLES |       所有计算了滑动平均值的变量       |     计算变量的滑动平均值     |



 <center> **表3-2 TensorFlow随机数生成函数** </center>

|      函数名称       |                          随机数分布                          |               主要参数                |
| :-----------------: | :----------------------------------------------------------: | :-----------------------------------: |
|  tf.random_normal   |                           正态分布                           |       平均值、标准差、取值类型        |
| tf.truncated_normal | 正态分布，但如果随机出来的值偏离平均值超过2个标准差，那么这个数将会被重新随机 |       平均值、标准差、取值类型        |
|  tf.random_uniform  |                           均匀分布                           |       最小、最大取值，取值类型        |
|   tf.random_gamma   |                          Gamma分布                           | 形状参数alpha、尺度参数beta、取值类型 |





 <center> **表3-3 TensorFlow常数生成函数** </center>

|  函数名称   |             功能             |                    样例                    |
| :---------: | :--------------------------: | :----------------------------------------: |
|  tf.zeros   |        产生全0的数组         | tf.zeros([2,3],int32) -> [[0,0,0],[0,0,0]] |
|   tf.ones   |        产生全1的数组         | tf.ones([2,3],int32) -> [[1,1,1],[1,1,1]]  |
|   tf.fill   | 产生一个全部为给定数字的数组 |   tf.fill([2,3],9) -> [[9,9,9],[9,9,9]]    |
| tf.constant |     产生一个给定值的常量     |      tf.constant([1,2,3]) -> [1,2,3]       |



所有的变量都会被自动第加入到GraphKeys.VARIABLES这个集合中，通过tf.global_variables()函数可以拿到当前计算图上所有的变量。当构建机器学习模型时，比如神经网络，可以通过变量声明函数中的trainable参数来区分需要优化的参数（比如神经网络中的参数）和其他参数（比如迭代的轮数）。如果声明变量时参数trainable为True,那么这个变量将会被加入到GraphKeys.TRAINABLE_VARIABLES集合。在TensorFlow中可以通过tf.trainable_variables函数得到所有需要优化的参数。TensorFlow中提供的神经网络优化算法会将GraphKeys.TRAINABLE_VARIABLES集合中的变量作为默认的优化对象。

**变量类型不可变**



但如果每轮迭代中选取的数据都要通过常量来表示，那么 TensorFlow 的计算图将会太大。因为每生成一个常量， TensorFlow都会在计算图中增加一个节点。一般来说，一个神经网络的训练过程会需要经过几百万轮甚至几亿轮的迭代，这样计算图就会非常大，而且利用率很低。 为了避免这个问题， TensorFlow提供了 placeholder机制用于提供输入数据。placeholder 相当于定义了一个位置，这个位置中的数据在程序运行时再指定。这样在程序中就不需要生成大量常量来提供输入数据，而只需要将数据通过 placeholder 传入TensorFlow 计算图 。在 placeholder定义时，这个位置上的数据类型是需要指定的。和其他张量一样， placeholder 的类型也是不可以改变的。 placeholder 中数据的维度信息可以根据提供的数据推导得出，所以不一定要给出。 



目前 TensorFlow 支持 10 种不同的优化器，读者可以根据具体的应用选择不同 的优化算法。 比较 常用的优化方法有 三 种: tf.train.GradientDescentOptimizer、 tf.train.AdamOptimizer 和 tf.train.MomentumOptimizer。在定义了反向传播算法之后，通过运行 sess.run(train_step)就 可以对所有在 GraphKeys.TRAINABLE_VABLES 集合中 的变量进行优化，使得在当前 batch 下损失函数更小 。  



TensorFlow 中所有 运算的输入、输出都是张量。张量本身并不存储任何数据，它只是对运算结果的引用。 



## 第4章：深层神经网络

深度学习有两个非常重要的特性一一多层和非线性，这两点在对复杂问题建模时是缺 一 不可的 。  

####  4.1.1线性模型的局限性

线性模型的最大特点是任意线性模型的组合仍然还是线性模型。  

只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别，而且它们都是线性模型 。 然而线性模型能够解决的问题是有限的 ，这就是线性模型最大的局限性，也是为什么深度学习要强调非线性。 



在线性可分问题中，线性模型就能很好区分不同颜色的点 。因为线性模型就能解决线性可分问题，所以在深度学习的定义中特意强调它的目的为解决更加复杂的问题 。 所谓复杂问题，至少是无法通过直线(或者高维空间的平面)划分的 。在现实世界中，绝大部分的问题都是无法线性分割的。 



#### 4.1.2激活函数实现去线性化

如果将每一个神经元(也就是神经网络中的 节点)的输出通过 一个非线性函数，那么整个神经网络的模型也就不再是线性的了。这个非线性函数就是激活函数。 



#### 4.1.3多层网络解决异或运算

感知机可以简单地理解为单层的神经网络。感知机会先将输入进行加权和，然后再通过激活函数最后得到输出 。 这个结构就是一个没有隐藏层的神经网络。 

感知机无法模拟异或运算的功能。 

当加入隐藏层之后，异或问题就可以得到很好地解决。 

（https://playground.tensorflow.org/）一个有4个节点隐藏层的神经网络在训练 500 轮之后的效果。 除了可以看到最右边的输出节点可以很好地区分不同颜色的点 ， 更加有意思的是，隐藏层的 4 个节点中，每个节点都有一个角是黑色的。 这 4 个隐藏节点可以被认为代表了从输入特征中抽取的更高维的特征 。 比如第一个节点可以大致代表两个输入的逻辑与操作的结果( 当两个输入都为正数时该节 点输出为正数〉。从这个例子中可以看到，深层神经网络实际上有组合特征提取的功能。这 个特性对于解决不易提取特征向量的问题(比如图片识别、语音识别等)有很大帮助。这也是深度学习在这些问题上更加容易取得突破性进展的原因 。 

### 4.2 损失函数定义

#### 4.2.1经典损失函数

**分类问题 **

因为交叉煽一般会与 softmax 回归一起使用，所以 TensorFlow 对这两个功能进行了统一封装，并提供了  tf.nn.softmax_cross_entropy_with_logits 函数。比如可以直接通过以下代码来实现使用了 softmax 回归之后的交叉熵损失函数: 

```python
cross_entropy= tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y) 
```

其中 y 代表了原始神经 网络的输出结果，而 y一给出了标准答案 。 这样通过 一个命令就可以得到使用了 Softmax 回归之后的交叉摘。在只有一个正确答案的分类问题中，TensorFlow 提供了 tf.nn.sparse_softmax_cross_entropy_with_logits 函数来进一步加速计算过程。 

**回归问题**

对于回归问题，最常用的损失函数是均方误差(MSE, mean squared eηor) 

```python
mse = tf.reduce_mean(tf.square(y_ - y))
```

其中 y 代表了神经网络的输出答案， y_代表了标准答案 。 

#### 4.2.2 自定义损失函数


$$
Loss(y,y')=\sum_{i=1}^{n}{f(y_i,y'_i)},
f(x,y)=\left\{
\begin{array}{lr}
{a(x-y)}, {x>y}\\{b(y-x)}, {x{\le}y}
\end{array}
\right.
$$

```python
loss = tf.reduce_sum(tf.where(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b))
```

### 4.3 神经网络优化算法

梯度下降算法并不能保证被优化的函数达到全局最优解 。 

在训练神经网络时，参数的初始值会很大程度影响最后得到的结果 。 只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解。 

除了不一定能达到全局最优 ，梯度下降算法的另外一个问题就是计算时间太长 。 因为要在全部训练数据上最小 化损失，所以损失函数$J(θ)$是在所有训练数据上的损失和 。 这样在每一轮迭代中都需要计算在全部训练数据上的损失函数 。 

为了加速训练过程 ， 可以使用随机梯度下降的算法 (stochastic gradient descent) ，它的问题也非常明显 : 在某 一 条数据上损失函数更小并不代表在全部数据上损失函数更小， 于是使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优 。 

为了综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中一般采用这两个算法的折中——每次计算一小部分训练数据的损失函数。 这一小部分数据被称之为一个 batch。 通过矩阵运算，每次在一个 batch 上优化神经网络的参数并不会比单个数据慢太多 。 另一方面，每次使用一个 batch 可以大大减小收敛所需要的法代次数，同时可以使收敛到 的结果更加接近梯度下降的效果。 

### 4.4 神经网络进一步优化

#### 4.4.1 学习率的设置

学习率既不能过大，也不能过小 。 为了解决设定学习率的问题， TensorFlow 提供了一种更加灵活的学习率设置方法一一指数衰减法。 

tf.train.exponential_decay 函数实现了指数衰减学习率。 通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定。 它实现了以下代码的功能: 

```
decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_ steps)
```

tf.train.exponential_decay函数可以通过设置参数 staircase 选择不同的衰减方式。 staircase 的默认值为 False，这时学习率随法代轮数变化的趋势是平滑的。当 staircase 的值被设置为True 时， global_step / decay_steps 会被转化成整数。这使得学习率成为一个阶梯函数( staircase function)。 在这样的设置下 ， decay_steps 通常代表了完整的使用 一遍训练数据所需要的迭代轮数。这个迭代轮数也就是总训练样本数除以每一个 batch 中的训练样本数。 这种设置的常用场景是每完整地过完一遍训练数据，学习率就减小一次。这可以使得训练数据集中的所有数据对模型训练有相等的作用。当使用连续的指数衰减学习率时，不同的训练数据有不同的学习率，而当学习率减小时，对应的训练数据对模型训练结果的影响也就 小了。 

```python
global_step= tf.Variable(0, trainable=False)
#通过exponential_decay 函数生成学习率。
learning_rate= tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)
#使用指数农减的学习惑。在 minimize 函数 中传入 global step 将自动更新
#global_step参数，从而使得学习率也得到相应更新。
learning_step = tf.train.GradientDescentOptimizer(learning_rate)\
.minimize( ...my_loss..., global_step=global_step)
```



一般来说初始学习率、衰减系数和衰减速度都是根据经验设置的。 而且损失函数下降的速度和法代结束之后总损失的大小没有必然的联系。也就是说并不能通过前几轮损失函数下降的速度来比较不同神经网络的效果。 

#### 4.4.2 过拟合问题

为了避免过拟合问题 ， 一个非常常用的方法是正则化( regularization)。正则化的思想就是在损失函数中加入刻画模型复杂程度的指标。假设用于刻画模型在训练数据上表现的损失函数为$J(\theta)$，那么在优化时不是直接优化$J(\theta)$，而是优化$J(\theta)+\lambda R(w)$ 。其中$R(w)$刻画的是模型的复杂程度，而 λ表示模型复杂损失在总损失中的比例。注意这里θ表示的是一个神经网络中所有的参数，它包括边上的权重 w 和偏置项 b。 一般来说模型复杂度只由权重 w 决定 。常用的刻画模型复杂度的函数$R(w)$有两种， 一种是$L1$正则化，计算公式是: 
$$
R(w)=||w||_1=\sum_i|w_i|
$$
另一种是$L2$正则化，计算公式是: 
$$
R(w)=||w||_2^2=\sum_i|w_i^2|
$$
无论是哪一种正则化方式，基本的思想都是希望通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪音。但这两种正则化的方法也有很大的区别。 首先，$L1$正则 化会让参数变得更稀疏，而$L2$ 正则化不会 。 所谓参数变得更稀疏是指会有更多的参数变为 0，这样可以达到类似特征选取的功能。之所以$L2$正则化不会让参数变得稀疏的原因是当参数很小时，比如 0.001，这个参数的平方基本上就可以忽略了，于是模型不会进一步将这个参数调整为 0。其次 ，$L1$正则化的计算公式不可导，而$L2$正则化公式可导。因为在优化时需要计算损失函数的偏导数，所以对含有$L2$正则化损失函数的优化要更加简洁 。 优化带$L1$正则化的损失函数要更加复杂，而且优化方法也有很多种。在实践中，也可以将 LI 正则化和 L2 正则化同时使用 : 
$$
R(w)=\sum_i\alpha|w_i|+(1-\alpha)w_i^2
$$

```python
w = tf.Variable(tf.random_normal([2, 1], stddev=1,seed=1))
y = tf.matmul(x, w)
loss= tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda)(w)
```

```python
weights= tf.constant([[l.O, -2.0], [-3.0, 4.0]]) 
with tf.Session() as sess:
	#输出为(|1|+|-2|+|-3|+|4|)*0.5=5。其中 0.5 为正则化项的权重。
	print sess.run(tf.contrib.layers.l1_regularizer(.5)(weights))
    #输出为(1^2+ (-2)^2 + (-3)^2 + 4^2 ) /2 * 0.5=7.5
    #TensorFlow 会将L2正则化损失值除以 2 使得求导得到的结果更加简洁 。
	print sess.run(tf.contrib.layers.l2_regularizer(.5) (weights))
```



在简单的神经网络中，这样的方式就可以很好地计算带正则化的损失函数了 。但当神经网络的参数增多之后，这样的方式首先可能导致损失函数 loss 的定义很长， 可读性差且容易出错。但更主要的是，当网络结构复杂之后定义网络结构的部分和计算损失函数的部分可能不在同 一个函数中，这样通过变量这种方式计算损失函数就不方便了。为了解决这个问题，可以使用 TensorFlow 中提供的集合 (collection)。 

```python
var = tf.Variable(tf.random_normal(shape), dtype = tf.float32)
tf.add_to_collection('losses', tf.contrib.layers.12_regularizer(lambda)(var))
...
mse_loss= tf.reduce_mean(tf.square(y_ - y))
tf.add_to_collection('losses', mse_loss)

loss = tf.add_n(tf.get_collection('losses'))

```





#### 4.4.3 滑动平均模型

另外 一个可以便模型在测试数据上更健壮( robust)的方法一一滑动平均模型。在采用随机梯度下降算法训练神经网络时， 使用滑动平均模型在很多应用中都可以在 一 定程度提高最终模型在测试数据上的表现。



在 TensorFlow 中提供了 tf.train.ExponentialMovingAverage 来实现滑动平均模型。在初始化 ExponentialMovingAverage 时，需要提供一个衰减率( decay)。 这个衰减率将用于控制 模型更新的速度。 ExponentialMovingAverage 对每一个变量会维护一个影子变量( shadow variable)，这个影子变量的初始值就是相应变量 的初始值，而每次运行变量更新时，影子 变量的值会更新为 : 

shadow_ variable = decay * shadow_ variable+ (1- decay) * variable

其中 shadow_variable 为影子变量， variable 为待更新的变量， decay 为衰减率。从公式 中可以看到 ， decay 决定了模型更新的速度 ， decay 越大模型越趋于稳定。在实际应用中， decay 一般会设成非常接近 1 的数(比如 0.999 或 0.9999)。为了使得模型在训练前期可以更新得更快， ExponentialMovingAverage还提供了 num_updates参数来动态设置 decay 的大小。如果在 ExponentialMovingAverage初始化时提供了 num_updates参数，那么每次使用的 衰减率将是 : 
$$
min\left\{decay,\frac{1+num\_updates}{10+num\_updates}\right\}
$$


# 第五章：MNIST数字识别问题

#### 5.2.1 TensorFlow 训练神经网络 

在神经网络的结构上，深度学习一方面需要使用激活函数实现神经网络模型的去线性化，另一方面需要使用 一个或多个隐 藏层使得神经网络的结构更深，以解决复杂问题 。 在训练神经网络时，使用带指数衰减的学习率设置、使用正则化来避免过度拟合，以及使用滑动平均模型来使得最终模型更加健壮。 

#### 5.2.2 使用验证数据 集判断模型效果 

在这个程序的开始设置 了初始学习率、学习率衰减率 、 隐藏层节点数量、迭代轮数等 7 种不同的参数 。那么如何设置这些参数的取值呢?在大部分情况下，配置神经网络的这些参数都是需要通过实验来调整的。 虽然一个神经网络模型的效果最终是通过测试数据来评判的，但是我们不能直接通过模型在测试数据上的效果来选择参数(指超参数)。使用测试数据来选取参数可能会导致神经网络模型过度拟合测试数据，从而失去对未知数据的预判能力。因为 一个神经网络模型的最终目标是对未知数据提供判断，所以为了估计模型在未知数据上的效果，需要保证测试数据在训练过程中是不可见的。只有这样才能保证通过测试数据评估出来的效果和在真实应用场景下模型对未知数据预判的效果是接近的。于是，为了评测神经网络模型在不同参数下的效果，一般会从训练数据中抽取一部分作为验证数据。使用验证数据就可以评判不同参数取值下模型的表现。除了使用验证数据集，还可以采用交叉验证( cross validation)的方式来验证模型效果。但因为神经网络训练时间本身就比较长，采用 cross validation会花费大量时间 。 所以在海量数据的情况下， 一般会更多地采用验证数据集的形式来评测模型的效果 。 

这两条曲线（测试集和验证集在不同训练轮次下的正确率）不会完全重 合，但是这两条曲线的趋势基本一样，而且他们的相关系数( correlation coefficient)大于 0.9999。 这意味着在 MNIST 问题上，完全可以通过模型在验证数据上的表现来判断一个模型的优劣 。 

不同问题的数据分布不 一样 ，如果验证数据分布不能很好地代表测试数据分布，那么模型在这两个数据集上的表现就有可能不 一样。所以，验证数据的选取方法是非常重要的， 一般来说选取的验证数据分布越接近测试数据分布 ， 模型在验证数据上的表现越可以体现 模型在测试数据上的表现 。 但通过本节中介绍的实验 ， 至少可以说明通过神经网络在验证数据上的效果来选取模型的参数是一个可行的方案。 

#### 5.2.3 不同模型效果比较 

设计神经网络时的 5 种优化方法  

神经网络结构的设计上

1.使用激活函数

2.使用多层隐藏层。 

在神经网络优化时，可以

3.使用指数衰减的学习率

4.加入正则化的损失函数

5.使用滑动平均模型 

**调整神经网络的结构对最终的正确率有非常大的影响** 。 没有隐藏层或者没有激活函数肘，模型的正确率只有大约 92.6%， 这个数字要远远小于使 用了隐藏层和激活函数时可以达到的大约 98.4%的正确率。这说明神经网络的结构对最终 模型的效果有本质性的影响 。 

使用滑动平均模型、指数衰减的学习率和使用正则化带来 的正确率的提升并不是特别明显 。 其中使用了所有优化算法的模型和不使用滑动平均的模型以及不使用指数衰减的学习率的模型都可以达到大约 98.4%的正确率。 这是因为滑动平均模型和指数衰减的学习率在一定程度上都是限制神经网络中参数更新的速度，然而在 MNIST 数据上，因为模型收敛的速度很快，所以这两种优化对最终模型的影响不大 。 这是否能说明这些优化方法作用不大呢?答案是否定的。当问题更加复杂时，迭代不会这么快接近收敛，这时滑动平均模型和指数衰减的学习率可以发挥更大的作用。比如在 CIFAR-10 图像分类数 据集上 ， 使用滑动平均模型可以将错误率降低 11%，而使用指数衰减的学习率可以将错误 率降低 7%。 

相 比滑动平均模型和指数衰减学习率 ，使用加入正则化的损 失函数给模型效果带来的 提升要相对显著。使用了正 则化损失函数的神 经网络模型可以降低大约 6%的错误率(从 1.69%降低到 1.59%)。 只优化交叉熵的模型在训练数据上的交叉熵损失要比优化总损失的模型更小。然而在测试数据上，优化总损失的模型却要好于只优化交叉娟的模型(灰色实线)。这个原因就是第 4 章中介绍的过拟合问题。 只优化交熵的模型可以更好地拟合训练数据(交叉熵损失更小)，但是却不能很好地挖掘数据中潜在的规律来判断未知的测试数据，所以在测试数据上的正确率低。

随着迭代的进行，正则化损失是在不断加大的。因为 MNIST 问题相对比较简单， 迭代后期的梯度很小，所以正则化损失的增长也不快。如果问题更加复杂，选代后期的梯度更大，就会发现总损失(交叉熵损失加上正则化损失)会呈现出一个U字型。优化总损失的模型，这个模型的正则化损失部分也可以随着迭代的进行越来越 小，从而使得整体的损失呈现一个逐步递减的趋势。  

总的来说，通过 MNJST 数据集有效地验证了**激活函数、隐藏层可以给模型的效果带来质的飞跃**。 由于 MNIST 问题本身相对简单，**滑动平均模型、指数衰减的学习率和正则化损失**对最终正确率的提升效果不明显。但通过进一步分析实验的结果 ，可以得出这些优化方法确实可以解决第 4 章中提到的神经网络优化过程中的问题。**当需要解决的问题和使用到的神经网络模型更加复杂时，这些优化方法将更有可能对训练效果产生更大的影响。 **

### 5.3 变量管理

当神经 网络的结构更加复杂、参数更多时，就需要一个更好的方式来传递和管理神经网络中的参数了 。 TensorFlow 提供了通过变量名称来创建或者获取一个变量的机制。 通过这个机制， 在不同的函数中可以直接通过变量的名字来使用变量，而不需要将变盘通过参数的形式到处传递 。 TensorFlow 中通过变量名称获取变量的机制主要是通过 tf.get variable 和 tf.variable_scope 函数实现的 。 

当神经网络结构更加复杂、参数更多时，使用这种变量管理方式将大大提高程序的可读性。 



### 6.2 卷积神经网络简介 

什么全连接神经网络无法很好地 处理图像数据。 

使用全连接神经网络处理图像的最大问题在于全连接层的参数太多。参数增多除了导致计算速度减慢，还很容易导致过拟合问题。所以需要一个更合理的神经网络结构来有效地减少神经网络中参数个数。卷积神经网络就可以达到这个目的 。

一个卷积神经网络主要由以下5种结构组成 : 

1. 输入层 

2. 卷积层

3. 池化层 

4. 全连接层 

5. Softmax层 


### 6.3 卷积神经网络常用结构 

#### 6.3.1 卷积层 

卷积层中常用的过滤器尺寸有 3×3 或 5×5。 

#### 6.3.2 池化层  

池化层可 以非常有效地缩小矩阵的尺寸，从而减少最后全连接层中的参数。使用池化层既可以加快计算速度也有防止过拟合问题的作用。 

在实际应用中使用得最多的池化层过滤器尺为[1,2,2,1]或者[1,3,3,1]。

### 6.4 经典卷积网络模型 

#### 6.4.1 LeNet-5 模型 

LeNet-5 模型每一层的结构。 



|层| 输入    | 填充 | filter | step | 输出                   | 参数个数            |
| --- | ------- | ---- | ------ | ---- | ---------------------- | ------------------- |
| 1.卷积 | 32x32x1 | 否   | 5x5x1x6 | 1    | 28x28x6 | 5x5x1x6+6=156 |
| 2.池化 | 28x28x6 | 否 | 2x2 | 2 | 14x14x6 |  |
| 3.卷积 | 14x14x6 | 否 | 5x5x6x16 | 1 | 10x10x16 | 5x5x6x16+16 |
| 4.池化 | 10x10x16 | 否 | 2x2 | 2 | 5x5x16 |  |
| 5.全连接 | 5x5x16 | 否 | 5x5x16x120 | 0 | 120 | 5x5x16x120+120 |
| 6.全连接 | 120 |  |  |  | 84 | 120x84+84 |
| 7.全连接 | 84 |  |  |  | 10 | 84x10+10 |



原始的 LeNet-5 模型中使用的过滤器和 6.3.2 节介绍的有些细微差别， 

LeNet-5模型论文中最后一层输出层的结构和全连接层有区别，但我们这用全连接层近似的表示。 



在 MNIST 测试数据上，上面给出的卷积神经网络可以 达到大约 99.4%的正确率。相比第 5 章中最高的 98.4%的正确率，**卷积神经网络可以巨幅提高神经网络在 MNIST 数据集上的正确率。** 

**如何设计卷积神经网络架构**

下面的正则化公式总结了一些经典的用于图片分类问题的卷积神经网络架构： 

**输入层→（卷积层+→池化层？）+→全连接层+**

“+”表示一层或多层，“？”表示有或者没有 
除了LeNet-5模型，2012年ImageNet ILSVRC图像分类挑战的第一名AlexNet模型、2013年ILSVRC第一名ZF Net模型已经2014年第二名VGGNet模型的架构都满足上面的正则表达式。

**如何设置卷积层或池化层配置**

- 过滤器的尺寸：1或3或5，有些网络中有过7甚至11
- 过滤器的深度：逐层递增。每经过一次池化层之后，卷积层深度*2
- 卷积层的步长：一般为1，有些也会使用2甚至3
- 池化层：最多的是max_pooling，过滤器边长一般为2或者3，步长一般为2或3

#### 6.4.2 Inception-v3 模型  

在 LeNet-5 模型中，不同卷积层通过串联的方式连接在一起，而 Inception-v3 模型中的 Inception 结构是将不同的卷积层通过并联的方式结合在一起 

一个卷积层可以使用边长为 1、 3 或者 5 的过滤器，那么如何在这些边长中选呢? Inception 模块给出了 一个方案，那就是同时使用所有不同尺寸的过滤器，然后再将得到的矩阵拼接起来。 

dropout一般只在全连接层而不是卷积层或者池化层使用。 

只有全连接层的权重需要加入正则化 



Inception-v3 模型论文：Rethinking the InceptionArchitectureforComputerVision 



#### 6.5.1 迁移学习介绍 

为了解决标注数据和训练时间的问题，可以使用本节将要介绍的迁移学习 。 

根据论文 DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition中的结论，可以保留训练好的 Inception-v3 模型中所有卷积层的参数，只是替换最后一层全连接层。 在最后这一层全连接层之前的网络层称之为瓶颈层(bottleneck)。 

在训练好的 Inception-v3模型中，因为将瓶颈层的输出再通过一个单层的全连接层神经网络可以很好地区分 1000种类别的图像，所以有理由认为瓶颈层输出的节点向量可以被作为任何图像的 一个更加精简且表达能力更强的特征向量。 

一般来说，在数据量足够的情况下，迁移学习的效果不如完全重新训练。但是迁移学习所需要的训练时间和训练样本数要远远小于训练完整的模型。  











## 8 循环神经网络 

### 8.1 循环神经网络简介 

循环神经网络( recurτent neural network，时刑 )源自于 1982 年由 Saratha Sathasivam 提出的霍普菲尔德网络。 

霍普菲尔德网络：Sathasivam S. Logic Learning in Hopjield Networks [1]. Modem Applied Science, 2009 



循环神经网络的主要用途是处理和预测序列数据。 

卷积神经网络在不同的空间位置共享参数，循环神经网络是在不同时间位置共享参数， 从而能够使用有限的参数处理任意长度的序列。

理论上循环神经网络可以支持任意长度的序列，然而在实际训练过程中，如果序列过长， 一方面会导致优化时出现梯度消散和梯度爆炸的问题，另一方面，展开后的前馈神经网络会占用过大的内存，所以实际中一般会规定一个最大长度，当序列长度超过规定长度之后会对序列进行截断。 



在最近的研究中，也有的模型将循环神经网络应用于空间位置，或是将卷积神经网络应用于时间序列 。 前者的例子可参见 DeepMind 提出的 PixelRNN模型 ; 后者的例子可以 参见 DeepMind提出的 WaveNet模型和 Facebook提出的 ConvS2S模型。 

### 8.2 长短时记忆网络 CLSTM)结构 

长期依赖( long-term dependencies)问题。 

在复杂语言场景中 ，有用信息的间隔有大有小、长短不 一 ，循环神经网络的性能也会受到限制。 长短时记忆网络( long short-term memory， LSTM)的设计就是为了解决这个问题。 

所谓“门”的结构就是一个使用sigmoid 神经网络和一个按位做乘法的操作，这两个操作合在一起就是一个“门”的结构。 

### 8.3 循环神经网络的变种 

#### 8.3.1 双向循环神经网络和深层循环神经网络 

双向 循环神经网络是由两个独立的循环神经网络叠加在一起组成的 。 输出由这两个循环神经网 络的输出拼接而成 

双向神经网络的介绍可以参考 Mike Schuster和 Kuldip K. Paliwal 发表的论文 Bidirectionalrecurrent neural networks。

深层循环神经网络( Deep RNN 〉是循环神经网络的另外 一种变种。为了增强模型的表达能力，可以在网络中设置多个循环层，将每层循环网络的输出传给下一层进行处理。 

#### 8.3.2 循环神经网络的 dropout 

通过 dropout，可以让卷积神经网 络更加健壮(robust)。类似的，在循环神经网络中使用 dropout也有同样的功能。而且，类似卷积神经网络只在最后的全连接层中使用 dropout，循环神经网络一般只在不同层循环体结构之间使用 dropout，而不在同一层的循环体结构之间使用。 也就是说从时刻 t-1 传递 到时刻 t时，循环神经网络不会进行状态的 dropout;而在同一个时刻 t 中，不同层循环体之间会使用 dropout。 



循环神经网络是在序列的时间轴上重复使用循环模块所 得到的网络结构。在训练时，循环神经网络展开成前馈神经网络，而在测试时则循环地将 上一步的输出作为下一步的输入来获取新的输出。



#### 9.2.1 PTB 数据集的顶处理

在实际工程中，通常使用 TFRecords 格式来提高读写效率。虽然预处理原则上可以放在 TensorFlow 的 Dataset 框架中与读取文本同时进行，但在工程实践上，保存处理好的数据有几个重要的优点:第 一 ，在调试模型的过程中，可以保证不同模型采取的预处理步骤相同 : 第二 ，减小文件体积，节省磁盘读取的时间；第三，方便对预处理步骤本身进行 debug，例如在模型训练效果不理想时，只需检查最终的数据文件就可以知道是不是预处理过程出了问题。



有研究2指出，如果共享词向量层和 Softmax层的参数，不仅能大幅减少参数数量，还能提高最终模型效果。